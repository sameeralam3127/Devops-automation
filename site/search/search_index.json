{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to DevOps Automation","text":"<p>Welcome to DevOps Automation \u2013 your resource for everything DevOps and Site Reliability Engineering (SRE). Whether you are experienced or just starting, this site will help you explore tools, practical tips, and connect with a professional community.</p> <p>Practical advice to help you deliver and manage code effectively.</p> <p>Discover the latest tools and approaches to keep your systems reliable and secure.</p> <p>Step-by-step guides for setup, deployment, and system improvement.</p> <p>I am Sameer Alam, a DevOps engineer and SRE practitioner. My focus is simplifying complex systems and ensuring reliability. Over the years, I have gained insights into what makes systems operate efficiently.</p> <p>See my projects: My GitHub Blog Explore my earlier blog posts from 2016, when I first began documenting my learning. For questions or ideas, feel free to reach out.</p> <p>\u2013 Sameer Alam</p>"},{"location":"ansible/ansible/","title":"Introduction to Ansible","text":""},{"location":"ansible/ansible/#1-what-is-ansible","title":"1. What is Ansible?","text":"<p>Ansible is an open-source IT automation tool that allows you to automate provisioning, configuration management, application deployment, and orchestration.</p> <ul> <li>It uses simple YAML files (called playbooks) to define automation tasks.</li> <li>It is agentless, requiring only SSH access (or WinRM for Windows) to manage remote systems.</li> </ul> <p>Earlier approaches/tools used before Ansible:</p> <ul> <li>Manual scripting with Bash/Shell scripts or Python scripts.</li> <li>Configuration management tools such as Puppet and Chef, which required agents and had steeper learning curves.</li> <li>Orchestration and provisioning tools like Terraform (still widely used, but focused more on infrastructure provisioning rather than configuration management).</li> </ul> <p>Note</p> <p>Ansible excels at configuration management, whereas Terraform is stronger for infrastructure provisioning.</p>"},{"location":"ansible/ansible/#2-core-concepts","title":"2. Core Concepts","text":"<ol> <li>Inventory \u2013 A file listing the machines (hosts) you want to manage.</li> <li>Playbooks \u2013 YAML files that define sets of tasks for execution.</li> <li>Modules \u2013 Pre-built units of work (installing packages, copying files, managing services, etc.).</li> <li>Roles \u2013 A structured way to organize playbooks and files for reuse.</li> </ol>"},{"location":"ansible/ansible/#3-configuration-file","title":"3. Configuration File","text":"<p>Ansible can use a configuration file (<code>ansible.cfg</code>). It is loaded in the following order:</p> <ol> <li><code>ANSIBLE_CONFIG</code> (if set as an environment variable)</li> <li><code>ansible.cfg</code> (in the current directory)</li> <li><code>~/.ansible.cfg</code> (in the home directory)</li> <li><code>/etc/ansible/ansible.cfg</code></li> </ol> <p>Tip</p> <p>Always create a project-level <code>ansible.cfg</code> to avoid conflicts with global settings.</p>"},{"location":"ansible/ansible/#4-installing-ansible","title":"4. Installing Ansible","text":"<ul> <li>Using pip (Python package manager):</li> </ul> <pre><code>pip install ansible\n</code></pre> <ul> <li>On Ubuntu/Debian:</li> </ul> <pre><code>sudo apt update\nsudo apt install ansible\n</code></pre> <ul> <li>On macOS (Homebrew):</li> </ul> <pre><code>brew install ansible\n</code></pre> <ul> <li>On Windows (via WSL):   Install Ansible inside Ubuntu/Debian WSL using the steps above.</li> </ul> <p>Verify installation:</p> <pre><code>ansible --version\n</code></pre> <p>Warning</p> <p>Do not mix OS package installs (apt, brew) with pip installs on the same machine to avoid version conflicts.</p>"},{"location":"ansible/ansible/#5-working-with-inventory","title":"5. Working with Inventory","text":"<p>Create a file named <code>hosts.ini</code>:</p> <pre><code>[webservers]\n192.168.1.10\n192.168.1.11\n\n[databases]\ndb1.example.com\n</code></pre> <p>List hosts:</p> <pre><code>ansible all --list-hosts\n</code></pre>"},{"location":"ansible/ansible/#6-running-ad-hoc-commands","title":"6. Running Ad-Hoc Commands","text":"<ul> <li>Ping all hosts:</li> </ul> <pre><code>ansible all -m ping\nansible all -m ping -vvv\n</code></pre> <ul> <li>Run commands with password prompt:</li> </ul> <pre><code>ansible web -m command -a \"uptime\" -k\nansible web -m command -a \"yum install httpd* -y\" -k\n</code></pre> <ul> <li>Copy and fetch files:</li> </ul> <pre><code>ansible all -m copy -a \"src=/etc/passwd dest=/tmp\"\nansible all -m fetch -a \"src=/var/log/yum.log dest=/logs\"\n</code></pre> <ul> <li>File management:</li> </ul> <pre><code>ansible all -m file -a \"path=/tmp/india mode=777\"\nansible all -m file -a \"path=/tmp/india state=absent\"\n</code></pre> <ul> <li>Run shell scripts:</li> </ul> <pre><code>ansible all -m shell -a \"sh /tmp/scripts.sh\"\n</code></pre>"},{"location":"ansible/ansible/#7-writing-your-first-playbook","title":"7. Writing Your First Playbook","text":"<p><code>playbook.yml</code>:</p> <pre><code>---\n- name: Install Apache on webservers\n  hosts: webservers\n  become: yes\n  tasks:\n    - name: Ensure Apache is installed\n      apt:\n        name: apache2\n        state: present\n      when: ansible_os_family == \"Debian\"\n</code></pre> <p>Run it:</p> <pre><code>ansible-playbook -i hosts.ini playbook.yml\n</code></pre>"},{"location":"ansible/ansible/#8-gathering-facts","title":"8. Gathering Facts","text":"<p>Ansible automatically collects system information (facts):</p> <pre><code>ansible all -m setup\nansible all -m setup -a \"filter=ansible_python_version\"\n</code></pre>"},{"location":"ansible/ansible/#9-verbose-mode","title":"9. Verbose Mode","text":"<ul> <li><code>-v</code> basic verbose</li> <li><code>-vv</code> more details</li> <li><code>-vvv</code> debug level (SSH, facts, module arguments)</li> </ul>"},{"location":"ansible/ansible/#10-disabling-host-key-checking","title":"10. Disabling Host Key Checking","text":"<pre><code>export ANSIBLE_HOST_KEY_CHECKING=False\n</code></pre> <p>Or update <code>ansible.cfg</code>:</p> <pre><code>[defaults]\nhost_key_checking = False\n</code></pre>"},{"location":"ansible/ansible/#11-troubleshooting-common-issues","title":"11. Troubleshooting Common Issues","text":"<ul> <li>Python Interpreter Issue:   Some hosts may not have Python installed or use a different version. Define interpreter in inventory:</li> </ul> <pre><code>[webservers]\n192.168.1.10 ansible_python_interpreter=/usr/bin/python3\n</code></pre> <ul> <li>SSH Key Authentication (Passwordless):</li> </ul> <pre><code>ssh-keygen\nssh-copy-id user@host\n</code></pre> <ul> <li>Connection Problems:   Use <code>ansible -vvv</code> for detailed debugging.</li> </ul> <p>Note</p> <p>Always verify SSH connectivity manually before using Ansible.</p>"},{"location":"ansible/ansible/#12-advanced-usage","title":"12. Advanced Usage","text":"<ul> <li>Modules: Explore with:</li> </ul> <pre><code>ansible-doc -l\nansible-doc copy\n</code></pre> <ul> <li>Packages:</li> </ul> <pre><code>ansible all -m package -a \"name=httpd state=present\"\nansible all -m package -a \"name=samba* state=latest use=yum\"\n</code></pre>"},{"location":"ansible/ansible/#13-roles","title":"13. Roles","text":"<p>Organize reusable playbooks:</p> <ul> <li><code>tasks/</code></li> <li><code>handlers/</code></li> <li><code>templates/</code></li> <li><code>files/</code></li> <li><code>vars/</code> / <code>defaults/</code></li> </ul>"},{"location":"ansible/ansible/#14-automation-tower-awxansible-tower","title":"14. Automation Tower (AWX/Ansible Tower)","text":"<ul> <li>A web-based UI and REST API to manage playbooks.</li> <li>Provides RBAC, job scheduling, logging, and notifications.</li> <li>AWX is the open-source upstream project of Ansible Tower.</li> </ul>"},{"location":"ansible/ansible/#15-terraform-vs-ansible","title":"15. Terraform vs Ansible","text":"<ul> <li>Terraform: Best for provisioning and managing infrastructure as code (e.g., creating servers, networks, cloud resources).</li> <li>Ansible: Best for configuration management, application deployment, and post-provisioning tasks.</li> <li>They are often used together: Terraform provisions, Ansible configures.</li> </ul> <p>Tip</p> <p>Use Terraform to build infrastructure, then hand off to Ansible for configuration and deployment.</p>"},{"location":"ansible/ansible/#16-community-and-learning-resources","title":"16. Community and Learning Resources","text":"<ul> <li>Official Documentation: Ansible Docs</li> <li>Ansible Galaxy: A hub for finding and sharing community-developed roles.</li> <li>Community: Forums, GitHub repositories, and mailing lists.</li> <li> <p>Practice Ideas:</p> </li> <li> <p>Automate a web server deployment.</p> </li> <li>Configure a database.</li> <li>Deploy a multi-tier app with roles.</li> </ul> <p>Success</p> <p>Recommendation: Use virtual machines, WSL, or cloud instances to practice Ansible playbooks. Next step: Start with simple ad-hoc commands, then move to playbooks and roles for real automation.</p>"},{"location":"ansible/ansible/#troubleshooting-roles-and-advanced-details","title":"Troubleshooting, Roles, and Advanced Details","text":""},{"location":"ansible/ansible/#1-troubleshooting-and-environment-variables","title":"1. Troubleshooting and Environment Variables","text":""},{"location":"ansible/ansible/#environment-variables-vs-config-file-precedence","title":"Environment Variables vs. Config File Precedence","text":"<ul> <li>Environment variables (e.g., <code>export ANSIBLE_CONFIG=/custom/path/ansible.cfg</code>) take highest precedence.</li> <li>Config file settings (e.g., <code>ansible.cfg</code>) are overridden by environment variables.</li> <li>Order of precedence (highest to lowest):</li> <li>Command-line flags (e.g., <code>--private-key</code>)</li> <li>Environment variables (e.g., <code>ANSIBLE_PRIVATE_KEY_FILE</code>)</li> <li>Config file settings</li> <li>Default values</li> </ul>"},{"location":"ansible/ansible/#common-troubleshooting-tips","title":"Common Troubleshooting Tips","text":"<ol> <li>SSH Connection Issues:</li> </ol> <pre><code># Test SSH connectivity manually first:\nssh user@target-host\n\n# Enable verbose Ansible output:\nansible -i inventory.ini all -m ping -vvv\n</code></pre> <ol> <li>Python Interpreter Errors:</li> </ol> <pre><code># In inventory.ini, specify Python path:\n[webservers]\nweb1 ansible_host=192.168.1.10 ansible_python_interpreter=/usr/bin/python3\n</code></pre> <ol> <li>Permission Denied:    Use <code>--become</code> (<code>-b</code>) to escalate privileges:</li> </ol> <pre><code>ansible -i inventory.ini all -m package -a \"name=nginx\" --become\n</code></pre> <ol> <li>Debug Modules:    Use the <code>debug</code> module to print variables:    <pre><code>- name: Display variables\n  debug:\n    var: my_variable\n</code></pre></li> </ol>"},{"location":"ansible/ansible/#2-creating-roles-simplified","title":"2. Creating Roles Simplified","text":""},{"location":"ansible/ansible/#step-by-step-role-creation","title":"Step-by-Step Role Creation","text":"<ol> <li>Generate role structure:</li> </ol> <pre><code>ansible-galaxy role init my_role\n</code></pre> <p>Creates:</p> <pre><code>my_role/\n\u251c\u2500\u2500 defaults/    # Low-priority variables\n\u251c\u2500\u2500 tasks/       # Main tasks\n\u251c\u2500\u2500 handlers/    # Handlers\n\u251c\u2500\u2500 templates/   # Jinja2 templates\n\u251c\u2500\u2500 files/       # Static files\n\u251c\u2500\u2500 vars/        # High-priority variables\n\u2514\u2500\u2500 meta/        # Role dependencies\n</code></pre> <ol> <li>Add tasks (<code>tasks/main.yml</code>):</li> </ol> <pre><code>- name: Install Nginx\n  apt:\n    name: nginx\n    state: present\n  notify: restart nginx\n</code></pre> <ol> <li>Add handlers (<code>handlers/main.yml</code>):</li> </ol> <pre><code>- name: restart nginx\n  service:\n    name: nginx\n    state: restarted\n</code></pre> <ol> <li>Use the role in a playbook:    <pre><code>- hosts: webservers\n  roles:\n    - my_role\n</code></pre></li> </ol>"},{"location":"ansible/ansible/#3-group-variables-inventory-and-host-variables","title":"3. Group Variables, Inventory, and Host Variables","text":""},{"location":"ansible/ansible/#dynamic-inventory","title":"Dynamic Inventory","text":"<p>Use dynamic inventories for cloud providers (AWS, Azure):</p> <pre><code>ansible-inventory -i aws_ec2.yml --list\n</code></pre>"},{"location":"ansible/ansible/#group-variables","title":"Group Variables","text":"<ol> <li>Directory structure:</li> </ol> <pre><code>inventory/\n\u251c\u2500\u2500 group_vars/\n\u2502   \u251c\u2500\u2500 webservers.yml\n\u2502   \u2514\u2500\u2500 databases.yml\n\u2514\u2500\u2500 hosts.ini\n</code></pre> <ol> <li>Define group variables (<code>group_vars/webservers.yml</code>):    <pre><code>---\nhttp_port: 80\nntp_servers: [0.pool.ntp.org, 1.pool.ntp.org]\n</code></pre></li> </ol>"},{"location":"ansible/ansible/#host-variables","title":"Host Variables","text":"<ol> <li>In inventory file:</li> </ol> <pre><code>[webservers]\nweb1 ansible_host=192.168.1.10 http_port=8080\n</code></pre> <ol> <li>In <code>host_vars/</code> directory:    <pre><code>inventory/\n\u251c\u2500\u2500 host_vars/\n\u2502   \u2514\u2500\u2500 web1.yml\n</code></pre> <code>web1.yml</code>:    <pre><code>---\nhttp_port: 8080\n</code></pre></li> </ol>"},{"location":"ansible/ansible/#4-advanced-configuration-details","title":"4. Advanced Configuration Details","text":""},{"location":"ansible/ansible/#custom-config-file","title":"Custom Config File","text":"<p>Create a project-specific <code>ansible.cfg</code>:</p> <pre><code>[defaults]\ninventory = ./inventory/hosts.ini\nroles_path = ./roles\nretry_files_enabled = False\nhost_key_checking = False\n\n[privilege_escalation]\nbecome = True\nbecome_method = sudo\nbecome_user = root\n</code></pre>"},{"location":"ansible/ansible/#vault-for-secrets","title":"Vault for Secrets","text":"<p>Encrypt sensitive data:</p> <pre><code>ansible-vault create secrets.yml\nansible-playbook site.yml --ask-vault-pass\n</code></pre>"},{"location":"ansible/ansible/#5-example-full-project-structure","title":"5. Example: Full Project Structure","text":"<pre><code>my_ansible_project/\n\u251c\u2500\u2500 ansible.cfg\n\u251c\u2500\u2500 inventory/\n\u2502   \u251c\u2500\u2500 hosts.ini\n\u2502   \u251c\u2500\u2500 group_vars/\n\u2502   \u2502   \u2514\u2500\u2500 webservers.yml\n\u2502   \u2514\u2500\u2500 host_vars/\n\u2502       \u2514\u2500\u2500 web1.yml\n\u251c\u2500\u2500 roles/\n\u2502   \u2514\u2500\u2500 nginx/\n\u2502       \u251c\u2500\u2500 tasks/\n\u2502       \u251c\u2500\u2500 handlers/\n\u2502       \u2514\u2500\u2500 templates/\n\u2514\u2500\u2500 playbooks/\n    \u2514\u2500\u2500 deploy.yml\n</code></pre>"},{"location":"ansible/ansible/#6-quick-command-reference","title":"6. Quick Command Reference","text":"<pre><code># Test connectivity\nansible all -m ping\n\n# Run a playbook with custom inventory\nansible-playbook -i inventory/custom.ini playbook.yml\n\n# Dry-run (check changes)\nansible-playbook playbook.yml --check\n\n# Limit to specific hosts\nansible-playbook playbook.yml --limit webservers\n\n# Use vault\nansible-playbook playbook.yml --vault-id @prompt\n</code></pre>"},{"location":"ansible/ansible/#7-best-practices","title":"7. Best Practices","text":"<ol> <li>Use <code>ansible-lint</code> to validate playbooks.</li> <li>Version control all Ansible content.</li> <li>Use <code>--check</code> mode before applying changes.</li> <li>Document variables and roles with <code>README.md</code>.</li> </ol> <p>By following this guide, you\u2019ll streamline your Ansible workflows, avoid common pitfalls, and create reusable, maintainable automation code!</p>"},{"location":"blog/","title":"Blog","text":"<p>Welcome to the blog. Here you\u2019ll find updates, tutorials, and articles.</p>"},{"location":"blog/2018/02/04/active-directory-domain-service-ad-ds/","title":"Active Directory Domain Service (AD DS)","text":"<p>Active Directory Domain Services (AD DS) is a centralized database management system that stores information about users, computers, groups, printers, security settings, server configurations, and network infrastructure. By consolidating this information centrally, AD DS simplifies administration and enforces consistent policies across the enterprise.</p>"},{"location":"blog/2018/02/04/active-directory-domain-service-ad-ds/#directory-service-ds","title":"Directory Service (DS)","text":"<p>Directory Service, developed by the IETF, defines relationships between objects in a system. It uses the Lightweight Directory Access Protocol (LDAP), which operates on port 389, to query and manage directory information.</p>"},{"location":"blog/2018/02/04/active-directory-domain-service-ad-ds/#features-of-ad-ds","title":"Features of AD DS","text":"<ul> <li>Manages user logon, authentication, and directory searches.</li> <li>Handles communication between users and domains.</li> <li>Stores directory data (the directory store) and provides mechanisms to locate and retrieve information.</li> <li>Implements centralized control of resources (users, computers, and data).</li> <li>Runs on servers known as Domain Controllers.</li> </ul>"},{"location":"blog/2018/02/04/active-directory-domain-service-ad-ds/#components-of-ad-ds","title":"Components of AD DS","text":""},{"location":"blog/2018/02/04/active-directory-domain-service-ad-ds/#forest","title":"Forest","text":"<ul> <li>Highest-level container in Active Directory.</li> <li>Represents a grouping of one or more independent domain trees.</li> <li>Defines the logical security boundary for an enterprise.</li> <li>Shares a single database and a single global address list.</li> <li>Contains objects like the Directory Schema, Global Catalog, and Directory Structure.</li> <li>The first domain created is the Forest Root Domain.</li> <li>By default, users or administrators of one forest cannot access another forest.</li> </ul>"},{"location":"blog/2018/02/04/active-directory-domain-service-ad-ds/#domain","title":"Domain","text":"<ul> <li>Logical boundary for storing information about users, groups, computers, and printers.</li> <li>Identified by a Fully Qualified Domain Name (FQDN).</li> <li>Managed by a Domain Controller, which provides authentication and authorization.</li> <li>All domain controllers in a domain hold a synchronized copy of the domain database.</li> </ul>"},{"location":"blog/2018/02/04/active-directory-domain-service-ad-ds/#domain-tree","title":"Domain Tree","text":"<ul> <li>A collection of domains organized in a hierarchical structure.</li> <li>Shares a common schema and global catalog.</li> <li>Supports a parent\u2013child relationship (e.g., <code>enhanceofit.com</code> as the parent, <code>delhi.enhanceofit.com</code> as a child).</li> <li>Provides namespace organization and hierarchy for enterprises.</li> </ul>"},{"location":"blog/2024/04/11/securing-secrets-with-ansible-vault/","title":"Securing Secrets with Ansible Vault","text":"<p>Automation is a cornerstone of modern IT operations, but automation often involves sensitive data such as passwords, API keys, and certificates. Storing these secrets in plain text within playbooks or configuration files poses a serious security risk. Ansible Vault provides a built-in solution to encrypt and manage such information securely.</p>"},{"location":"blog/2024/04/11/securing-secrets-with-ansible-vault/#what-is-ansible-vault","title":"What Is Ansible Vault?","text":"<p>Ansible Vault is a feature that allows you to encrypt variables, files, or even entire playbooks. Encrypted content remains unreadable to anyone without the correct decryption password or key. This ensures sensitive information is protected, even if your code repository is public or compromised.</p>"},{"location":"blog/2024/04/11/securing-secrets-with-ansible-vault/#why-use-vault","title":"Why Use Vault?","text":"<ol> <li> <p>Protect Sensitive Data    Secrets like SSH keys, database credentials, and cloud access tokens can be stored securely without exposing them in playbooks.</p> </li> <li> <p>Enable Secure Collaboration    Teams can share encrypted files through version control without risking leaks of confidential information.</p> </li> <li> <p>Compliance and Auditing    Encrypting sensitive values helps organizations meet security compliance requirements by ensuring proper handling of credentials.</p> </li> <li> <p>Seamless Integration    Vault works directly with playbooks, roles, and variables, requiring minimal changes to existing automation workflows.</p> </li> </ol>"},{"location":"blog/2024/04/11/securing-secrets-with-ansible-vault/#how-vault-works","title":"How Vault Works","text":"<ul> <li> <p>Encrypt a file:   <pre><code>ansible-vault create secrets.yml\n</code></pre></p> </li> <li> <p>Edit an encrypted file:</p> </li> </ul> <pre><code>ansible-vault edit secrets.yml\n</code></pre> <ul> <li>Encrypt an existing file:</li> </ul> <pre><code>ansible-vault encrypt vars.yml\n</code></pre> <ul> <li>Run a playbook with Vault password:</li> </ul> <pre><code>ansible-playbook site.yml --ask-vault-pass\n</code></pre>"},{"location":"blog/2024/04/11/securing-secrets-with-ansible-vault/#best-practices-with-ansible-vault","title":"Best Practices with Ansible Vault","text":"<ul> <li>Store Vault passwords securely (e.g., use Ansible Tower, AWX, or a password manager).</li> <li>Separate sensitive data into dedicated encrypted files rather than mixing them into large playbooks.</li> <li>Limit access to the Vault password to only authorized personnel.</li> <li>Rotate Vault passwords periodically.</li> </ul>"},{"location":"blog/2023/03/17/why-security-hardening-is-important/","title":"Why Security Hardening Is Important","text":"<p>In today\u2019s connected world, systems are constantly exposed to potential threats. Security hardening is the process of reducing vulnerabilities by applying best practices, limiting attack surfaces, and configuring systems to operate in the most secure way possible.</p>"},{"location":"blog/2023/03/17/why-security-hardening-is-important/#what-is-security-hardening","title":"What Is Security Hardening?","text":"<p>Security hardening means strengthening an IT system, application, or network so that it resists attacks. It involves:</p> <ul> <li>Disabling unnecessary services and ports.</li> <li>Enforcing strong authentication and authorization.</li> <li>Applying timely software patches.</li> <li>Configuring secure defaults for operating systems and applications.</li> <li>Monitoring and auditing activity.</li> </ul>"},{"location":"blog/2023/03/17/why-security-hardening-is-important/#why-it-matters","title":"Why It Matters","text":"<ol> <li> <p>Reduce Attack Surface    Every open port, unused account, or default configuration is a potential entry point for attackers. Hardening eliminates these weak spots.</p> </li> <li> <p>Compliance and Standards    Many industries must meet strict regulations (e.g., ISO 27001, PCI-DSS, HIPAA). Hardening ensures systems are compliant.</p> </li> <li> <p>Mitigate Human Error    Default settings are often insecure. Hardening enforces policies that reduce the risk of mistakes leading to breaches.</p> </li> <li> <p>Protect Confidential Data    Hardening controls limit access to sensitive information and reduce the chance of data leaks.</p> </li> <li> <p>Increase Resilience    A hardened system is less likely to be compromised, reducing downtime, financial loss, and reputational damage.</p> </li> </ol>"},{"location":"blog/2023/03/17/why-security-hardening-is-important/#common-hardening-steps","title":"Common Hardening Steps","text":"<ul> <li>Enforce strong password policies and use multi-factor authentication.</li> <li>Remove or disable unused accounts and services.</li> <li>Keep systems and applications fully patched.</li> <li>Apply firewall rules and network segmentation.</li> <li>Encrypt sensitive data at rest and in transit.</li> <li>Enable system logging and audit trails.</li> </ul>"},{"location":"blog/2023/03/17/why-security-hardening-is-important/#conclusion","title":"Conclusion","text":"<p>Security hardening is not a one-time task but an ongoing process. By adopting strong configurations and continually reviewing them, organizations can drastically reduce the risk of cyberattacks and safeguard their most valuable assets.</p>"},{"location":"blog/2025/08/24/password-and-system-hardening-best-practices/","title":"Password and System Hardening Best Practices","text":"<p>Securing user authentication and account management is a critical step in system hardening. Weak passwords, poorly managed accounts, or improper access controls can all lead to system compromise. This post covers practical measures such as enforcing strong passwords, using PAM modules, managing user details with GECOS, controlling root access, and enabling SSH key authentication.</p>"},{"location":"blog/2025/08/24/password-and-system-hardening-best-practices/#enforce-strong-passwords","title":"Enforce Strong Passwords","text":"<p>Ensure users create strong passwords that meet minimum complexity requirements. You can enforce these rules via PAM (Pluggable Authentication Modules):</p> <pre><code># /etc/pam.d/common-password\npassword requisite pam_pwquality.so retry=3 minlen=12 ucredit=-1 lcredit=-1 dcredit=-1 ocredit=-1\n</code></pre> <p>This configuration requires:</p> <ul> <li>At least 12 characters</li> <li>Uppercase, lowercase, numeric, and special characters</li> <li>3 attempts before failure</li> </ul>"},{"location":"blog/2025/08/24/password-and-system-hardening-best-practices/#control-user-information-with-gecos","title":"Control User Information with GECOS","text":"<p>The GECOS field in <code>/etc/passwd</code> helps document user accounts. Regularly review accounts to ensure:</p> <ul> <li>Each entry has clear identification.</li> <li>No stale or unused accounts remain.</li> </ul> <p>Audit users with:</p> <pre><code>awk -F: '{print $1, $5}' /etc/passwd\n</code></pre>"},{"location":"blog/2025/08/24/password-and-system-hardening-best-practices/#restrict-root-access","title":"Restrict Root Access","text":"<p>Direct root login is dangerous. Instead:</p> <ol> <li>Disable root SSH login in <code>/etc/ssh/sshd_config</code>:</li> </ol> <pre><code>PermitRootLogin no\n</code></pre> <ol> <li>Add trusted administrators to the sudo group:</li> </ol> <pre><code>usermod -aG sudo alice\n</code></pre> <p>This enforces accountability since all privileged commands are logged through <code>sudo</code>.</p>"},{"location":"blog/2025/08/24/password-and-system-hardening-best-practices/#validate-active-users-early","title":"Validate Active Users Early","text":"<p>Before deploying to production, validate the user list:</p> <pre><code>cut -d: -f1 /etc/passwd\n</code></pre> <p>Remove or disable unnecessary accounts:</p> <pre><code>usermod -L olduser     # lock account\n</code></pre>"},{"location":"blog/2025/08/24/password-and-system-hardening-best-practices/#use-ssh-keys-instead-of-passwords","title":"Use SSH Keys Instead of Passwords","text":"<p>Configure public key authentication for secure remote logins:</p> <ol> <li>On client machine:</li> </ol> <pre><code>ssh-keygen -t ed25519\nssh-copy-id user@server\n</code></pre> <ol> <li>On server, update <code>/etc/ssh/sshd_config</code>:</li> </ol> <pre><code>PasswordAuthentication no\nPubkeyAuthentication yes\n</code></pre> <ol> <li>Restart SSH:</li> </ol> <pre><code>systemctl restart sshd\n</code></pre> <p>This prevents brute-force password attacks and enforces stronger authentication.</p>"},{"location":"blog/2025/08/24/password-and-system-hardening-best-practices/#example-script-for-user-hardening","title":"Example Script for User Hardening","text":"<pre><code>#!/bin/bash\n# Basic user hardening script\n\n# Disable root SSH login\nsed -i 's/^PermitRootLogin.*/PermitRootLogin no/' /etc/ssh/sshd_config\n\n# Enforce password complexity\nif ! grep -q \"pam_pwquality.so\" /etc/pam.d/common-password; then\n  echo \"password requisite pam_pwquality.so retry=3 minlen=12 ucredit=-1 lcredit=-1 dcredit=-1 ocredit=-1\" &gt;&gt; /etc/pam.d/common-password\nfi\n\n# Lock old/stale accounts\nfor user in $(awk -F: '$3 &gt;= 1000 {print $1}' /etc/passwd); do\n  if [ \"$user\" != \"admin\" ] &amp;&amp; [ \"$user\" != \"alice\" ]; then\n    usermod -L $user\n  fi\ndone\n\nsystemctl restart sshd\necho \"System hardening applied.\"\n</code></pre>"},{"location":"blog/2017/12/25/windows-10-shortcut-commands/","title":"Windows 10 Shortcut Commands","text":"<p>Windows provides many built-in shortcut commands that help administrators and users quickly access tools, settings, and features. These commands can be executed by pressing Windows + R to open the Run dialog, and then typing the appropriate command.</p> <p>Below is a comprehensive and up-to-date list of Windows 10 shortcut commands.</p>"},{"location":"blog/2017/12/25/windows-10-shortcut-commands/#common-folder-shortcuts","title":"Common Folder Shortcuts","text":"<ul> <li>Open Documents Folder \u2192 <code>documents</code></li> <li>Open Videos Folder \u2192 <code>videos</code></li> <li>Open Downloads Folder \u2192 <code>downloads</code></li> <li>Open Favorites Folder \u2192 <code>favorites</code></li> <li>Open Recent Folder \u2192 <code>recent</code></li> <li>Open Pictures Folder \u2192 <code>pictures</code></li> </ul>"},{"location":"blog/2017/12/25/windows-10-shortcut-commands/#system-information-and-utilities","title":"System Information and Utilities","text":"<ul> <li>About Windows dialog \u2192 <code>winver</code></li> <li>System Configuration \u2192 <code>msconfig</code></li> <li>System Information \u2192 <code>msinfo32</code></li> <li>Registry Editor \u2192 <code>regedit</code></li> <li>Registry Editor (32-bit) \u2192 <code>regedt32</code></li> <li>Task Manager \u2192 <code>taskmgr</code></li> <li>Command Prompt \u2192 <code>cmd</code></li> <li>Windows PowerShell \u2192 <code>powershell</code></li> <li>Windows PowerShell ISE \u2192 <code>powershell_ise</code></li> </ul>"},{"location":"blog/2017/12/25/windows-10-shortcut-commands/#administrative-tools","title":"Administrative Tools","text":"<ul> <li>Computer Management \u2192 <code>compmgmt.msc</code> or <code>compmgmtlauncher</code></li> <li>Device Manager \u2192 <code>devmgmt.msc</code> or <code>hdwwiz.cpl</code></li> <li>Disk Management \u2192 <code>diskmgmt.msc</code></li> <li>Services \u2192 <code>services.msc</code></li> <li>Event Viewer \u2192 <code>eventvwr.msc</code></li> <li>Local Group Policy Editor \u2192 <code>gpedit.msc</code></li> <li>Local Security Policy \u2192 <code>secpol.msc</code></li> <li>Local Users and Groups \u2192 <code>lusrmgr.msc</code></li> <li>Component Services \u2192 <code>comexp.msc</code> or <code>dcomcnfg</code></li> <li>Performance Monitor \u2192 <code>perfmon.msc</code></li> <li>Resource Monitor \u2192 <code>resmon</code></li> <li>Resultant Set of Policy \u2192 <code>rsop.msc</code></li> <li>Trusted Platform Module (TPM) Management \u2192 <code>tpm.msc</code></li> <li>WMI Management \u2192 <code>wmimgmt.msc</code></li> </ul>"},{"location":"blog/2017/12/25/windows-10-shortcut-commands/#control-panel-and-settings","title":"Control Panel and Settings","text":"<ul> <li>Control Panel \u2192 <code>control</code></li> <li>Programs and Features \u2192 <code>appwiz.cpl</code></li> <li>Power Options \u2192 <code>powercfg.cpl</code></li> <li>Sound \u2192 <code>mmsys.cpl</code></li> <li>Date and Time \u2192 <code>timedate.cpl</code></li> <li>Region and Language \u2192 <code>intl.cpl</code></li> <li>Display Settings \u2192 <code>desk.cpl</code></li> <li>Internet Options \u2192 <code>inetcpl.cpl</code></li> <li>Windows Firewall \u2192 <code>firewall.cpl</code></li> <li>Windows Firewall with Advanced Security \u2192 <code>wf.msc</code></li> <li>User Account Control Settings \u2192 <code>useraccountcontrolsettings</code></li> </ul>"},{"location":"blog/2017/12/25/windows-10-shortcut-commands/#system-properties","title":"System Properties","text":"<ul> <li>General \u2192 <code>sysdm.cpl</code></li> <li>Advanced Tab \u2192 <code>systempropertiesadvanced</code></li> <li>Computer Name Tab \u2192 <code>systempropertiescomputername</code></li> <li>Hardware Tab \u2192 <code>systempropertieshardware</code></li> <li>Remote Tab \u2192 <code>systempropertiesremote</code></li> <li>System Protection Tab \u2192 <code>systempropertiesprotection</code></li> <li>Performance Settings \u2192 <code>systempropertiesperformance</code></li> <li>DEP Settings \u2192 <code>systempropertiesdataexecutionprevention</code></li> </ul>"},{"location":"blog/2017/12/25/windows-10-shortcut-commands/#file-and-printer-management","title":"File and Printer Management","text":"<ul> <li>Create Shared Folder Wizard \u2192 <code>shrpubw</code></li> <li>Shared Folders \u2192 <code>fsmgmt.msc</code></li> <li>Printer Migration \u2192 <code>printbrmui</code></li> <li>Print Management \u2192 <code>printmanagement.msc</code></li> <li>Print User Interface \u2192 <code>printui</code></li> </ul>"},{"location":"blog/2017/12/25/windows-10-shortcut-commands/#security-and-certificates","title":"Security and Certificates","text":"<ul> <li>Certificates \u2192 <code>certmgr.msc</code></li> <li>Encrypting File System Wizard \u2192 <code>rekeywiz</code></li> <li>Stored User Names and Passwords \u2192 <code>credwiz</code></li> <li>SAM Lock Tool \u2192 <code>syskey</code></li> </ul>"},{"location":"blog/2017/12/25/windows-10-shortcut-commands/#troubleshooting-and-diagnostics","title":"Troubleshooting and Diagnostics","text":"<ul> <li>Disk Cleanup \u2192 <code>cleanmgr</code></li> <li>Disk Defragmenter \u2192 <code>dfrgui</code></li> <li>DirectX Diagnostic Tool \u2192 <code>dxdiag</code></li> <li>Problem Steps Recorder \u2192 <code>psr</code></li> <li>Windows Memory Diagnostic Scheduler \u2192 <code>mdsched</code></li> <li>Microsoft Support Diagnostic Tool \u2192 <code>msdt</code></li> </ul>"},{"location":"blog/2017/12/25/windows-10-shortcut-commands/#accessories-and-tools","title":"Accessories and Tools","text":"<ul> <li>Calculator \u2192 <code>calc</code></li> <li>Notepad \u2192 <code>notepad</code></li> <li>WordPad \u2192 <code>write</code></li> <li>Paint \u2192 <code>mspaint</code></li> <li>Snipping Tool \u2192 <code>snippingtool</code></li> <li>Sticky Notes \u2192 <code>stikynot</code></li> <li>Character Map \u2192 <code>charmap</code></li> <li>Font Viewer \u2192 <code>fontview</code></li> <li>On-Screen Keyboard \u2192 <code>osk</code></li> <li>Magnifier \u2192 <code>magnify</code></li> <li>Narrator \u2192 <code>narrator</code></li> </ul>"},{"location":"blog/2017/12/25/windows-10-shortcut-commands/#networking-and-remote-access","title":"Networking and Remote Access","text":"<ul> <li>Remote Desktop Connection \u2192 <code>mstsc</code></li> <li>Remote Access Phonebook \u2192 <code>rasphone</code></li> <li>Windows Remote Assistance \u2192 <code>msra</code></li> <li>iSCSI Initiator \u2192 <code>iscsicpl</code></li> <li>Network Connections \u2192 <code>ncpa.cpl</code></li> </ul>"},{"location":"blog/2017/12/25/windows-10-shortcut-commands/#other-useful-shortcuts","title":"Other Useful Shortcuts","text":"<ul> <li>Windows Explorer \u2192 <code>explorer</code></li> <li>Windows Update \u2192 <code>wuapp</code> or <code>wusa</code></li> <li>Windows Media Player \u2192 <code>wmplayer</code></li> <li>Windows Fax and Scan \u2192 <code>wfs</code></li> <li>XPS Viewer \u2192 <code>xpsrchvw</code></li> <li>Internet Explorer \u2192 <code>iexplore</code></li> </ul>"},{"location":"docker/docker/","title":"Docker: From Basics to Advanced","text":"What is Docker? <p>Docker is an open-source platform designed to simplify application deployment. It ensures that applications run reliably regardless of the environment. It allows developers to build, ship, and run applications in isolated environments called containers. Containers package applications with their libraries, dependencies, and configurations, ensuring consistency across development, testing, and production.</p> Why Use Docker? <ul> <li>Consistency: Applications behave the same in development, testing, and production.</li> <li>Lightweight: Containers share the host OS kernel, making them more efficient than virtual machines.</li> <li>Portable: Run applications anywhere, from local machines to cloud platforms.</li> </ul> Getting Started with Docker Installation <p>1. Install Docker on Linux</p> <pre><code># Update the package index\nsudo apt update\n\n# Install required packages\nsudo apt install apt-transport-https ca-certificates curl software-properties-common -y\n\n# Add Docker's official GPG key\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n\n# Add the Docker repository\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n\n# Install Docker Engine\nsudo apt update\nsudo apt install docker-ce docker-ce-cli containerd.io -y\n\n# Verify the installation\ndocker --version\n</code></pre> <p>2. Install Docker on macOS</p> Prerequisites for macOS <ul> <li>Requires macOS 10.14 or newer</li> <li>Ensure adequate resources are available for Docker Desktop</li> </ul> <pre><code># Download Docker Desktop for Mac from:\nhttps://www.docker.com/products/docker-desktop/\n\n# Install Docker by dragging it to the Applications folder\n# Open Docker Desktop and follow setup instructions\n</code></pre> <p>3. Install Docker on Windows</p> Supported Windows Versions <p>Docker Desktop supports Windows 10 64-bit (Professional, Enterprise, Education) and Windows 11</p> <p>Steps:</p> <ol> <li>Download Docker Desktop from Docker's official website.</li> <li>Run the installer and follow on-screen instructions.</li> <li>After installation, verify with:</li> </ol> <pre><code>docker --version\n</code></pre> <p>Useful Tips</p> <p>Enable Non-Root Docker Access on Linux</p> <p>Add your user to the Docker group to run Docker without <code>sudo</code>:</p> <pre><code>sudo usermod -aG docker $USER\n</code></pre> <p>Log out and log back in for changes to take effect.</p> <p>Docker Resource Limits</p> <p>Containers share host resources. Ensure sufficient CPU and memory allocation for performance.</p> <p>Docker Desktop vs Docker Engine</p> <ul> <li>Docker Desktop: Includes GUI tools, ideal for macOS and Windows.</li> <li>Docker Engine: Lightweight CLI-based version for Linux servers.</li> </ul> <p>Verify Your Docker Installation</p> <p>Run a test container:</p> <pre><code>docker run hello-world\n</code></pre> <p>Expected output:</p> <pre><code>Hello from Docker!\nThis message shows that your installation appears to be working correctly.\n</code></pre>"},{"location":"docker/docker/#docker-command-reference","title":"Docker Command Reference","text":""},{"location":"docker/docker/#basic-commands","title":"Basic Commands","text":"<ul> <li>Check Docker Version</li> </ul> <pre><code>docker --version\n</code></pre> <ul> <li>List Docker Images</li> </ul> <pre><code>docker images\n</code></pre> <ul> <li>List Running Containers</li> </ul> <pre><code>docker ps\n</code></pre> <ul> <li>List All Containers</li> </ul> <pre><code>docker ps -a\n</code></pre> <ul> <li>Run a Container</li> </ul> <pre><code>docker run [OPTIONS] IMAGE [COMMAND] [ARG...]\n</code></pre> <ul> <li>Stop a Running Container</li> </ul> <pre><code>docker stop CONTAINER_ID\n</code></pre> <ul> <li>Remove a Container</li> </ul> <pre><code>docker rm CONTAINER_ID\n</code></pre> <ul> <li>Remove an Image</li> </ul> <pre><code>docker rmi IMAGE_ID\n</code></pre>"},{"location":"docker/docker/#building-docker-images","title":"Building Docker Images","text":"<ul> <li>Build an Image from a Dockerfile</li> </ul> <pre><code>docker build -t IMAGE_NAME:TAG PATH\n</code></pre> <ul> <li>Build Without Cache</li> </ul> <pre><code>docker build --no-cache -t IMAGE_NAME:TAG PATH\n</code></pre> <ul> <li>View Build History</li> </ul> <pre><code>docker history IMAGE_NAME:TAG\n</code></pre>"},{"location":"docker/docker/#dockerfile-basics","title":"Dockerfile Basics","text":"<p>Example:</p> <pre><code># Use a base image\nFROM ubuntu:latest\n\n# Set environment variables\nENV MY_ENV_VAR=value\n\n# Install dependencies\nRUN apt-get update &amp;&amp; apt-get install -y curl\n\n# Copy files\nCOPY ./local_file /container_file\n\n# Define default command\nCMD [\"echo\", \"Hello, Docker!\"]\n</code></pre>"},{"location":"docker/docker/#networking-and-volumes","title":"Networking and Volumes","text":"<ul> <li>Create a Network</li> </ul> <pre><code>docker network create my_network\n</code></pre> <ul> <li>Run a Container on a Network</li> </ul> <pre><code>docker run --network my_network IMAGE_NAME\n</code></pre> <ul> <li>Create a Volume</li> </ul> <pre><code>docker volume create my_volume\n</code></pre> <ul> <li>Run with a Volume</li> </ul> <pre><code>docker run -v my_volume:/container_path IMAGE_NAME\n</code></pre>"},{"location":"docker/docker/#docker-compose","title":"Docker Compose","text":"<ul> <li>Start Services</li> </ul> <pre><code>docker-compose up\n</code></pre> <ul> <li>Detached Mode</li> </ul> <pre><code>docker-compose up -d\n</code></pre> <ul> <li>Stop Services</li> </ul> <pre><code>docker-compose down\n</code></pre> <ul> <li>View Logs</li> </ul> <pre><code>docker-compose logs\n</code></pre>"},{"location":"docker/docker/#registry-operations","title":"Registry Operations","text":"<ul> <li>Push to Docker Hub</li> </ul> <pre><code>docker push IMAGE_NAME:TAG\n</code></pre> <ul> <li>Pull from Docker Hub</li> </ul> <pre><code>docker pull IMAGE_NAME:TAG\n</code></pre> <ul> <li>Tag an Image</li> </ul> <pre><code>docker tag SOURCE_IMAGE:TAG TARGET_IMAGE:TAG\n</code></pre> <ul> <li>Login</li> </ul> <pre><code>docker login\n</code></pre>"},{"location":"docker/docker/#troubleshooting-and-scenarios","title":"Troubleshooting and Scenarios","text":""},{"location":"docker/docker/#scenario-1-container-not-starting","title":"Scenario 1: Container Not Starting","text":"<ul> <li>Symptom: Container exits immediately</li> <li>Solution: Check logs</li> </ul> <pre><code>docker logs CONTAINER_ID\n</code></pre> <p>Ensure command is valid and not terminating.</p> Debug Tip <p>Use <code>-it</code> to keep the container interactive during debugging.</p>"},{"location":"docker/docker/#scenario-2-image-build-fails","title":"Scenario 2: Image Build Fails","text":"<ul> <li>Symptom: <code>docker build</code> fails</li> <li> <p>Solution:</p> </li> <li> <p>Review error messages</p> </li> <li>Ensure <code>Dockerfile</code> syntax is correct</li> <li> <p>Use no-cache build:</p> <pre><code>docker build --no-cache -t IMAGE_NAME:TAG .\n</code></pre> </li> </ul> Cache Considerations <p>Cached layers may cause outdated dependencies. Use <code>--no-cache</code> to ensure fresh builds.</p>"},{"location":"docker/docker/#scenario-3-port-already-in-use","title":"Scenario 3: Port Already in Use","text":"<ul> <li>Symptom: Port conflict error</li> <li> <p>Solution:</p> </li> <li> <p>Identify process:</p> <pre><code>lsof -i :PORT\n</code></pre> </li> <li> <p>Stop conflicting process or map a new port:</p> <pre><code>docker run -p NEW_PORT:CONTAINER_PORT IMAGE_NAME\n</code></pre> </li> </ul> Port Conflict Resolution <p>Use <code>docker ps</code> to review port mappings of running containers.</p>"},{"location":"docker/docker/#scenario-4-cannot-remove-containerimage","title":"Scenario 4: Cannot Remove Container/Image","text":"<ul> <li>Symptom: Removal fails with \"in use\" error</li> <li> <p>Solution:</p> </li> <li> <p>Stop all containers:</p> <pre><code>docker stop $(docker ps -q)\n</code></pre> </li> <li> <p>Force remove:</p> <pre><code>docker rm -f CONTAINER_ID\ndocker rmi -f IMAGE_ID\n</code></pre> </li> </ul> Forced Removal <p>The <code>-f</code> flag forces removal even when containers or images are in use.</p>"},{"location":"jenkins/jenkins/","title":"Jenkins Installation and First Pipeline Setup on Ubuntu","text":"Mastering Jenkins: Installing on Ubuntu and Creating Your First Pipeline <p>Jenkins is an open-source automation server that helps with continuous integration and continuous delivery (CI/CD). This guide will walk you through the process of installing Jenkins on Ubuntu, checking system details using a Bash script, and creating your first pipeline using Jenkins' powerful pipeline functionality.</p>"},{"location":"jenkins/jenkins/#step-1-installing-jenkins-on-ubuntu","title":"Step 1: Installing Jenkins on Ubuntu","text":"<p>Before we begin, ensure your system is up-to-date.</p>"},{"location":"jenkins/jenkins/#11-update-system-packages","title":"1.1 Update System Packages","text":"<p>First, update your system\u2019s package list to ensure everything is up to date:</p> <pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y\n</code></pre>"},{"location":"jenkins/jenkins/#12-install-java","title":"1.2 Install Java","text":"<p>Jenkins requires Java to run. Install the default Java Development Kit (JDK):</p> <pre><code>sudo apt install openjdk-11-jdk -y\n</code></pre> <p>To verify the installation, use:</p> <pre><code>java -version\n</code></pre>"},{"location":"jenkins/jenkins/#13-add-jenkins-repository","title":"1.3 Add Jenkins Repository","text":"<p>To install Jenkins, add its official repository to your system:</p> <pre><code>wget -q -O - https://pkg.jenkins.io/jenkins.io.key | sudo tee /etc/apt/trusted.gpg.d/jenkins.asc\n</code></pre> <p>Next, add the Jenkins repository:</p> <pre><code>sudo sh -c 'echo deb http://pkg.jenkins.io/debian/ stable main &gt; /etc/apt/sources.list.d/jenkins.list'\n</code></pre>"},{"location":"jenkins/jenkins/#14-install-jenkins","title":"1.4 Install Jenkins","text":"<p>Once the repository is added, update the package list and install Jenkins:</p> <pre><code>sudo apt update\nsudo apt install jenkins -y\n</code></pre>"},{"location":"jenkins/jenkins/#15-start-jenkins","title":"1.5 Start Jenkins","text":"<p>Enable and start the Jenkins service:</p> <pre><code>sudo systemctl enable jenkins\nsudo systemctl start jenkins\n</code></pre> <p>You can check the status of Jenkins:</p> <pre><code>sudo systemctl status jenkins\n</code></pre>"},{"location":"jenkins/jenkins/#step-2-accessing-jenkins","title":"Step 2: Accessing Jenkins","text":"<p>Jenkins will be running on port 8080 by default. Open your browser and navigate to:</p> <pre><code>http://localhost:8080\n</code></pre>"},{"location":"jenkins/jenkins/#21-unlock-jenkins","title":"2.1 Unlock Jenkins","text":"<p>To unlock Jenkins, you will need the <code>initialAdminPassword</code>. Retrieve it with the following command:</p> <pre><code>sudo cat /var/lib/jenkins/secrets/initialAdminPassword\n</code></pre> <p>Enter the password in the browser prompt.</p>"},{"location":"jenkins/jenkins/#22-install-suggested-plugins","title":"2.2 Install Suggested Plugins","text":"<p>Once you\u2019ve unlocked Jenkins, you'll be prompted to install plugins. Choose the \"Install suggested plugins\" option to proceed with the default plugin installation.</p>"},{"location":"jenkins/jenkins/#step-3-creating-your-first-jenkins-pipeline","title":"Step 3: Creating Your First Jenkins Pipeline","text":""},{"location":"jenkins/jenkins/#31-create-a-new-pipeline-project","title":"3.1 Create a New Pipeline Project","text":"<ol> <li>After logging into Jenkins, click on New Item.</li> <li>Enter a name for your pipeline (e.g., \"First-Pipeline\").</li> <li>Select Pipeline and click OK.</li> </ol>"},{"location":"jenkins/jenkins/#32-configure-the-pipeline","title":"3.2 Configure the Pipeline","text":"<p>In the pipeline configuration page, scroll down to the Pipeline section. Here, you'll define your pipeline script. For this guide, we\u2019ll use a simple pipeline that checks system details using a Bash script.</p> <pre><code>pipeline {\n    agent any\n\n    stages {\n        stage('Check System Details') {\n            steps {\n                script {\n                    sh 'bash check_system_details.sh'\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"jenkins/jenkins/#33-create-a-bash-script","title":"3.3 Create a Bash Script","text":"<p>In your Jenkins workspace, create a script called <code>check_system_details.sh</code>. This script will gather system information.</p> <pre><code>#!/bin/bash\n\necho \"System Information:\"\necho \"--------------------\"\nhostnamectl\necho\ndf -h\necho\nfree -h\necho\nuname -a\n</code></pre> <p>Ensure that the script has executable permissions:</p> <pre><code>chmod +x check_system_details.sh\n</code></pre>"},{"location":"jenkins/jenkins/#34-run-the-pipeline","title":"3.4 Run the Pipeline","text":"<p>Save the pipeline and click Build Now to execute the pipeline. Jenkins will run the <code>check_system_details.sh</code> script, and you should see the system details in the build log.</p>"},{"location":"kubernetes/kubernetes/","title":"Kubernetes: From Basics to Advanced","text":""},{"location":"kubernetes/kubernetes/#why-container-orchestration-and-need-for-containers","title":"Why Container Orchestration and Need for Containers","text":"<p>Before containers, applications were deployed on physical or virtual machines, which posed challenges:</p> <ul> <li>Dependency Conflicts: Applications required specific library versions, causing conflicts on shared machines.</li> <li>Environment Inconsistency: Differences between development, testing, and production environments led to issues like \"it works on my machine.\"</li> <li>Resource Inefficiency: Virtual machines included full operating systems, consuming significant resources.</li> </ul> <p>How Containers Solve This</p> <p>Containers package applications with their dependencies, ensuring consistency across environments.  They are lightweight, sharing the host OS kernel, unlike virtual machines.</p>"},{"location":"kubernetes/kubernetes/#the-rise-of-container-orchestration","title":"The Rise of Container Orchestration","text":"<p>As container usage scaled, manual management became inefficient. Key needs included:</p> <ul> <li>Deploying containers across multiple machines.</li> <li>Scaling applications based on demand.</li> <li>Ensuring high availability and handling failures.</li> <li>Managing networking and storage.</li> </ul> <p>Why Orchestration Tools?</p> <p>Container orchestration tools automate deployment, scaling, and resource management, improving reliability and efficiency.</p>"},{"location":"kubernetes/kubernetes/#why-kubernetes","title":"Why Kubernetes?","text":"<p>Kubernetes, originally developed by Google, became the standard for container orchestration due to:</p> <ul> <li>Scalability: Manages thousands of containers across clusters.</li> <li>Portability: Runs on-premises, in the cloud, or in hybrid setups.</li> <li>Ecosystem: Supported by a vast community and toolset.</li> <li>Flexibility: Handles diverse workloads, from stateless apps to stateful databases.</li> </ul> <p>Key Features</p> <p>Kubernetes adoption is driven by features like auto-scaling, self-healing, and service discovery.</p>"},{"location":"kubernetes/kubernetes/#understanding-oci-and-runc","title":"Understanding OCI and runc","text":""},{"location":"kubernetes/kubernetes/#open-container-initiative-oci","title":"Open Container Initiative (OCI)","text":"<p>OCI Specifications</p> <p>The Open Container Initiative (OCI), under the Linux Foundation, defines standards for container formats and runtimes to ensure interoperability.</p> <ul> <li>Container Image Specification: Defines image structure, layers, and metadata.</li> <li>Runtime Specification: Defines how runtimes create and manage containers.</li> </ul>"},{"location":"kubernetes/kubernetes/#runc","title":"runc","text":"<p>About runc</p> <p>runc is a lightweight, CLI-based container runtime implementing the OCI runtime specification.</p> <ul> <li>Creates containers using Linux namespaces and cgroups.</li> <li>Executes processes in isolated environments.</li> <li>Foundation for higher-level tools like Docker and containerd.</li> </ul> <p>Creating a Container with runc (Red Hat and Ubuntu)</p> <p>1. Install prerequisites  On Red Hat: <code>bash     sudo yum install -y runc</code></p> <pre><code>On **Ubuntu/Debian**:\n```bash\nsudo apt-get update\nsudo apt-get install -y runc\n```\n\n**2. Create root filesystem (using BusyBox for demo)**\n```bash\nmkdir rootfs\ndocker export $(docker create busybox) | tar -C rootfs -xvf -\n```\n\n**3. Generate default config**\n```bash\nrunc spec\n```\n\n**4. Start a container**\n```bash\nsudo runc run mycontainer\n```\n\n**5. Install a package inside the container**\nFor **Red Hat-based container**:\n```bash\nyum install -y vim\n```\nFor **Ubuntu-based container**:\n```bash\napt-get update\napt-get install -y vim\n```\n\n**6. Check resource usage of the container**\nFrom host system:\n```bash\nrunc list\nrunc state mycontainer\n```\n\nUsing Linux tools:\n```bash\ntop\nhtop\nfree -m\n```\n\nThis workflow demonstrates creating, running, and managing a container directly with `runc`, without Docker or higher-level tools.\n</code></pre>"},{"location":"kubernetes/kubernetes/#linux-kernel-features-namespaces-and-cgroups","title":"Linux Kernel Features: Namespaces and cgroups","text":""},{"location":"kubernetes/kubernetes/#namespaces","title":"Namespaces","text":"<p>Namespaces provide isolation for containers, giving each its own:</p> <ul> <li>PID Namespace: Isolated process IDs.</li> <li>Network Namespace: Separate network stack.</li> <li>Mount Namespace: Isolated filesystem.</li> <li>User Namespace: Isolated user and group IDs.</li> </ul>"},{"location":"kubernetes/kubernetes/#cgroups-control-groups","title":"cgroups (Control Groups)","text":"<p>cgroups control resource usage:</p> <ul> <li>CPU: E.g., 50% of a core.</li> <li>Memory: E.g., 1 GB.</li> <li>I/O: Disk bandwidth.</li> </ul> <p>Key Takeaway</p> <p>Containers use namespaces for isolation and cgroups for resource control.  Tools like Docker, containerd, and Kubernetes build on these features.</p>"},{"location":"kubernetes/kubernetes/#installing-local-kubernetes-environments","title":"Installing Local Kubernetes Environments","text":""},{"location":"kubernetes/kubernetes/#minikube","title":"Minikube","text":"<p>About Minikube</p> <p>Minikube runs a single-node Kubernetes cluster locally, ideal for learning and development.</p> <p>Prerequisites</p> <ul> <li>CPU: 2+ CPUs</li> <li>Memory: 2 GB (4 GB recommended)</li> <li>Disk Space: 20 GB free</li> <li>OS: Linux, macOS, or Windows</li> <li>Virtualization: VT-x/AMD-V</li> <li>Tools: Docker or hypervisor (VirtualBox, Hyper-V), kubectl</li> </ul> <p>Installation Steps</p> <pre><code># Linux: Install kubectl\ncurl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nchmod +x kubectl\nsudo mv kubectl /usr/local/bin/\n\n# macOS\nbrew install kubectl\n</code></pre> <p>For Windows, download from https://dl.k8s.io/release/stable.txt and add to PATH.</p> <pre><code># Install Minikube\ncurl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\nsudo install minikube-linux-amd64 /usr/local/bin/minikube\n\n# macOS\nbrew install minikube\n</code></pre> <p>For Windows, download from https://minikube.sigs.k8s.io/docs/start/.</p> <pre><code># Start and Verify\nminikube start --driver=docker\nkubectl get nodes\n</code></pre> <p>Access dashboard:</p> <pre><code>minikube dashboard\n</code></pre> <p>Usage Notes</p> <p>Minikube is for development only. Use <code>minikube stop</code> or <code>minikube delete</code> for cleanup.</p>"},{"location":"kubernetes/kubernetes/#rancher-desktop","title":"Rancher Desktop","text":"<p>About Rancher Desktop</p> <p>Rancher Desktop provides a lightweight Kubernetes cluster (k3s) with a GUI.</p> <p>Prerequisites</p> <ul> <li>CPU: 4+ CPUs</li> <li>Memory: 8 GB</li> <li>OS: Linux, macOS, or Windows</li> <li>Virtualization: QEMU (Linux/macOS), WSL2 (Windows)</li> </ul> <p>Steps</p> <ol> <li>Download from rancherdesktop.io.</li> </ol> <pre><code>sudo apt install ./rancher-desktop-&lt;version&gt;.deb\n</code></pre> <ol> <li>Enable Kubernetes in Preferences &gt; Kubernetes.</li> <li>Choose runtime: containerd or dockerd.</li> <li>Verify with:</li> </ol> <pre><code>kubectl get namespaces\n</code></pre> <p>Highlights</p> <p>- Uses k3s for efficiency. - Supports containerd and dockerd.</p>"},{"location":"kubernetes/kubernetes/#docker-desktop","title":"Docker Desktop","text":"<p>About Docker Desktop</p> <p>Docker Desktop integrates Kubernetes for local clusters.</p> <p>Prerequisites</p> <ul> <li>OS: Windows 10/11 (Pro/Enterprise), macOS</li> <li>Virtualization: WSL2 (Windows), HyperKit (macOS)</li> <li>Memory: 4 GB (8 GB recommended)</li> </ul> <p>Steps</p> <ol> <li>Download from docker.com and install.</li> <li>Enable Kubernetes: Settings &gt; Kubernetes &gt; Enable Kubernetes.</li> <li>Verify:</li> </ol> <pre><code>kubectl cluster-info\n</code></pre> <ol> <li>Deploy Kubernetes Dashboard:</li> </ol> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml\nkubectl proxy\n</code></pre> <p>Access at: http://localhost:8001</p> <p>Usage Notes</p> <p>- May require a paid license for large organizations. - Uses containerd as runtime.</p>"},{"location":"kubernetes/kubernetes/#hands-on-with-online-kubernetes-tools","title":"Hands-On with Online Kubernetes Tools","text":"<ul> <li>Katacoda: Free browser-based labs (https://www.katacoda.com/courses/kubernetes).</li> <li>Play with Kubernetes: Temporary clusters (https://labs.play-with-k8s.com/).</li> </ul>"},{"location":"kubernetes/kubernetes/#cloud-based-kubernetes-services","title":"Cloud-Based Kubernetes Services","text":""},{"location":"kubernetes/kubernetes/#aws-elastic-kubernetes-service-eks","title":"AWS Elastic Kubernetes Service (EKS)","text":"<p>About EKS</p> <p>A managed Kubernetes service integrated with AWS.</p> <p>Setup</p> <pre><code># Configure AWS CLI\naws configure\n\n# Install eksctl\ncurl --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp\nsudo mv /tmp/eksctl /usr/local/bin\n\n# Create cluster\neksctl create cluster --name my-cluster --region us-west-2 --nodegroup-name my-nodes --node-type t3.medium --nodes 2\n\n# Verify\nkubectl get nodes\n</code></pre> <p>Features</p> <p>- Integrates with AWS services (ELB, IAM, CloudWatch). - Supports auto-scaling and HA.</p>"},{"location":"kubernetes/kubernetes/#red-hat-openshift","title":"Red Hat OpenShift","text":"<p>About OpenShift</p> <p>Kubernetes-based platform with developer tools.</p> <p>Setup</p> <ul> <li>Use Red Hat Developer Sandbox.</li> </ul> <p>Features</p> <ul> <li>CI/CD pipelines.</li> <li>Developer UI and CLI (<code>oc</code>).</li> <li>Enhanced security.</li> </ul>"},{"location":"kubernetes/kubernetes/#other-providers","title":"Other Providers","text":"<ul> <li>Google Kubernetes Engine (GKE): GCP integration.</li> <li>Azure Kubernetes Service (AKS): Part of Azure ecosystem.</li> <li>IBM Cloud Kubernetes Service: Security-focused.</li> </ul>"},{"location":"kubernetes/kubernetes/#example-running-a-container-with-containerd","title":"Example: Running a Container with containerd","text":"<pre><code>sudo yum install -y containerd\nsudo systemctl start containerd\nsudo systemctl enable containerd\n\n# Configure\nsudo mkdir -p /etc/containerd\ncontainerd config default | sudo tee /etc/containerd/config.toml\nsudo systemctl restart containerd\n\n# Pull and run Alpine\nsudo ctr image pull docker.io/library/alpine:latest\nexport CTR_NAMESPACE=my-ns\nsudo ctr --namespace $CTR_NAMESPACE run -t --rm docker.io/library/alpine:latest my-container sh\n\n# List namespaces\nsudo ctr namespaces list\n</code></pre>"},{"location":"kubernetes/kubernetes/#advanced-kubernetes-concepts","title":"Advanced Kubernetes Concepts","text":""},{"location":"kubernetes/kubernetes/#deployments-and-services","title":"Deployments and Services","text":"<pre><code>kubectl create deployment my-app --image=nginx:latest --replicas=3\nkubectl expose deployment my-app --type=NodePort --port=80\n</code></pre>"},{"location":"kubernetes/kubernetes/#ingress","title":"Ingress","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-ingress\nspec:\n  rules:\n    - host: myapp.local\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: my-app\n                port:\n                  number: 80\n</code></pre>"},{"location":"kubernetes/kubernetes/#configmaps-and-secrets","title":"ConfigMaps and Secrets","text":"<pre><code>kubectl create configmap my-config --from-literal=key1=value1\nkubectl create secret generic my-secret --from-literal=password=secure123\n</code></pre>"},{"location":"kubernetes/kubernetes/#persistent-volumes-pv-and-persistent-volume-claims-pvc","title":"Persistent Volumes (PV) and Persistent Volume Claims (PVC)","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: my-pv\nspec:\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: /mnt/data\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n</code></pre>"},{"location":"kubernetes/kubernetes/#helm-charts","title":"Helm Charts","text":"<pre><code># Install Helm\ncurl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash\n\n# Deploy a chart\nhelm install my-release nginx-stable/nginx-ingress\n</code></pre>"},{"location":"kubernetes/kubernetes/#conclusion","title":"Conclusion","text":"<p>Kubernetes enables scalable, portable container orchestration. Local tools like Minikube, Rancher Desktop, and Docker Desktop are great for learning, while AWS EKS and OpenShift provide production-grade solutions.</p> <p>By leveraging Linux namespaces, cgroups, containerd, and Helm, Kubernetes simplifies cloud-native application development.</p> <p>Further Learning</p> <ul> <li>Kubernetes Docs</li> <li>Rancher Desktop</li> <li>AWS EKS</li> <li>OpenShift Sandbox</li> </ul>"},{"location":"monitoring-tools/log/","title":"Grafana","text":"<p>Monitoring is a crucial part of DevOps, helping teams detect issues before they become critical. In this guide, we will install Prometheus, Grafana, and Node Exporter using Podman, an alternative to Docker. By the end, you'll have a fully functional monitoring stack with visual dashboards to analyze system metrics.</p>"},{"location":"monitoring-tools/log/#what-are-prometheus-grafana-and-node-exporter","title":"What Are Prometheus, Grafana, and Node Exporter?","text":""},{"location":"monitoring-tools/log/#1-prometheus-the-time-series-database","title":"1. Prometheus: The Time-Series Database","text":"<p>Prometheus is an open-source monitoring system that collects and stores metrics as time-series data. It is widely used in DevOps due to its powerful querying language (PromQL) and easy integration with multiple exporters.</p> <p>\ud83d\udccc Key Features:</p> <ul> <li>Time-series data storage</li> <li>Pull-based metric collection</li> <li>Alerting and rule-based evaluations</li> </ul>"},{"location":"monitoring-tools/log/#2-node-exporter-the-system-metrics-collector","title":"2. Node Exporter: The System Metrics Collector","text":"<p>Node Exporter is a lightweight agent that runs on a machine to collect system metrics such as CPU usage, memory utilization, disk I/O, and network statistics.</p> <p>\ud83d\udccc Common Metrics Collected:</p> <ul> <li>CPU Load: <code>node_cpu_seconds_total</code></li> <li>Memory Usage: <code>node_memory_MemAvailable_bytes</code></li> <li>Disk Space: <code>node_filesystem_avail_bytes</code></li> <li>Network Traffic: <code>node_network_receive_bytes_total</code></li> </ul>"},{"location":"monitoring-tools/log/#3-grafana-the-visualization-tool","title":"3. Grafana: The Visualization Tool","text":"<p>Grafana is an open-source tool that helps visualize and analyze time-series data. It allows you to create dashboards with real-time graphs, alerts, and reports.</p> <p>\ud83d\udccc Why Use Grafana?</p> <ul> <li>Connects to multiple data sources (Prometheus, MySQL, AWS CloudWatch, etc.)</li> <li>Beautiful and interactive dashboards</li> <li>Custom alerts and notifications</li> </ul>"},{"location":"monitoring-tools/log/#step-1-install-and-run-containers","title":"Step 1: Install and Run Containers","text":"<p>We will use Podman, a rootless container runtime similar to Docker, to set up our monitoring stack.</p>"},{"location":"monitoring-tools/log/#11-pull-and-run-a-centos-container","title":"1.1 Pull and Run a CentOS Container","text":"<p>We'll start by running a CentOS container where we will install Node Exporter.</p> <pre><code>podman run -dit --name centos_container -p 9100:9100 centos:latest\n</code></pre> <p>Verify that the container is running:</p> <pre><code>podman ps\n</code></pre>"},{"location":"monitoring-tools/log/#12-run-prometheus","title":"1.2 Run Prometheus","text":"<p>Now, let's run Prometheus, which will scrape metrics from Node Exporter.</p> <pre><code>podman run -d --name prometheus -p 9090:9090 quay.io/prometheus/prometheus\n</code></pre> <p>Check if Prometheus is running properly:</p> <pre><code>podman logs prometheus\n</code></pre> <p>You can now access Prometheus at: \ud83d\udc49 http://localhost:9090/query</p> <p>To enter the Prometheus container:</p> <pre><code>podman exec -it prometheus /bin/sh\n</code></pre>"},{"location":"monitoring-tools/log/#step-2-install-and-run-node-exporter","title":"Step 2: Install and Run Node Exporter","text":"<p>Log into the CentOS container:</p> <pre><code>podman exec -it centos_container /bin/bash\n</code></pre> <p>Download and extract Node Exporter:</p> <pre><code>wget https://github.com/prometheus/node_exporter/releases/download/v1.9.0/node_exporter-1.9.0.linux-386.tar.gz\ntar -xvf node_exporter-1.9.0.linux-386.tar.gz\ncd node_exporter-1.9.0.linux-386\n./node_exporter\n</code></pre> <p>Now, you can check the collected metrics at: \ud83d\udc49 http://localhost:9100/metrics</p>"},{"location":"monitoring-tools/log/#step-3-configure-prometheus-to-scrape-node-exporter-metrics","title":"Step 3: Configure Prometheus to Scrape Node Exporter Metrics","text":"<p>Modify the Prometheus configuration file (<code>prometheus.yml</code>):</p> <pre><code>cd /etc/prometheus\nvim prometheus.yml\n</code></pre> <p>Add the following configuration:</p> <pre><code>global:\n  scrape_interval: 15s # Collect metrics every 15 seconds\n\nscrape_configs:\n  - job_name: \"prometheus\"\n    static_configs:\n      - targets: [\"localhost:9090\"]\n\n  - job_name: \"centos_node_exporter\"\n    static_configs:\n      - targets: [\"localhost:9100\"]\n</code></pre> <p>Restart Prometheus:</p> <pre><code>podman restart prometheus\n</code></pre> <p>Check if Prometheus is scraping Node Exporter: \ud83d\udc49 http://localhost:9090/targets</p>"},{"location":"monitoring-tools/log/#step-4-deploy-grafana","title":"Step 4: Deploy Grafana","text":"<p>Run the Grafana container:</p> <pre><code>podman run -d --name grafana -p 3000:3000 grafana/grafana\n</code></pre> <p>Get the IP address of Prometheus:</p> <pre><code>podman inspect -f '{{ .NetworkSettings.IPAddress }}' prometheus\n</code></pre> <p>Access Grafana at: \ud83d\udc49 http://localhost:3000</p> <p>\ud83d\udccc Default Login:</p> <ul> <li>Username: <code>admin</code></li> <li>Password: <code>admin</code> (change it on first login)</li> </ul>"},{"location":"monitoring-tools/log/#step-5-add-prometheus-as-a-data-source-in-grafana","title":"Step 5: Add Prometheus as a Data Source in Grafana","text":"<ol> <li>Open Grafana (<code>http://localhost:3000</code>).</li> <li>Go to Configuration &gt; Data Sources.</li> <li>Click \"Add Data Source\", select Prometheus.</li> <li>Set the URL as <code>http://prometheus:9090</code>.</li> <li>Click \"Save &amp; Test\".</li> </ol>"},{"location":"monitoring-tools/log/#step-6-import-node-exporter-dashboard","title":"Step 6: Import Node Exporter Dashboard","text":"<ol> <li>In Grafana, go to \"Create\" &gt; \"Import\".</li> <li>Enter the Dashboard ID: <code>1860</code> (or search for \"Node Exporter Full\").</li> <li>Select Prometheus as the data source.</li> <li>Click \"Import\".</li> </ol> <p>You should now see a full system monitoring dashboard! \ud83c\udf89</p>"},{"location":"monitoring-tools/log/#step-7-managing-containers","title":"Step 7: Managing Containers","text":"<p>To stop and remove containers:</p> <pre><code>podman stop centos_container\npodman rm centos_container\n</code></pre> <p>Restart Prometheus and Grafana:</p> <pre><code>podman restart prometheus\npodman restart grafana\n</code></pre>"},{"location":"shell-scripts/scripts/","title":"Shell Scripts for SRE and DevOps","text":"<p>Shell scripts are programs written for command-line interpreters (shells) that automate system tasks and operations. They combine sequences of commands into reusable scripts that can be executed as single units.</p> <p>They are widely used by SRE (Site Reliability Engineers) and DevOps engineers to automate operations, reduce manual effort, and standardize workflows.</p>"},{"location":"shell-scripts/scripts/#historical-context-and-evolution","title":"Historical Context and Evolution","text":"<p>Shell scripting originated in the early Unix systems (1970s) with the Bourne shell (<code>sh</code>). Over time, more feature-rich shells emerged:</p> <ul> <li>Bash (Bourne Again Shell): Default on most Linux distributions</li> <li>Zsh: Extended features with better user interaction</li> <li>Ksh: AIX default shell with advanced scripting capabilities</li> </ul>"},{"location":"shell-scripts/scripts/#why-shell-scripts-are-important-for-sre-and-devops","title":"Why Shell Scripts Are Important for SRE and DevOps","text":"<p>Tip</p> <p>Shell scripts remain a core tool for SRE and DevOps. - They are simple: use plain Linux commands. - They are powerful: integrate system utilities, APIs, and services. - They are universal: available by default on nearly every Unix/Linux system. - They allow conditions, loops, functions, and modularity to make tasks reusable.</p> <p>Examples of usage:</p> <ul> <li>Daily health checks</li> <li>Disk and memory monitoring</li> <li>Service management</li> <li>Deployment pipelines</li> <li>Cron-based automation</li> </ul>"},{"location":"shell-scripts/scripts/#shell-interpreter-and-location","title":"Shell Interpreter and Location","text":"<ul> <li>The interpreter is the program that executes shell scripts.</li> <li>Common locations:</li> </ul> <pre><code>/bin/sh\n/bin/bash\n/usr/bin/zsh\n</code></pre> <ul> <li>Default interpreter: <code>/bin/bash</code> in most Linux systems.</li> <li>Speed: Shell scripts are slower than compiled languages (C, Go), but fast enough for automation.</li> </ul> <p>Note</p> <p>Always define interpreter at the top of scripts with shebang: <code>bash     #!/bin/bash</code></p>"},{"location":"shell-scripts/scripts/#core-components-of-shell-scripts","title":"Core Components of Shell Scripts","text":""},{"location":"shell-scripts/scripts/#shebang-directive","title":"Shebang Directive","text":"<pre><code>#!/bin/bash\n#!/bin/sh     # POSIX-compliant\n#!/usr/bin/zsh\n#!/usr/bin/env bash  # Portable\n</code></pre>"},{"location":"shell-scripts/scripts/#comments-and-documentation","title":"Comments and Documentation","text":"<pre><code># Single-line comment\n\n: '\nMulti-line comment\n'\n\n&lt;&lt;EOF\nHere-document style comment\nEOF\n</code></pre>"},{"location":"shell-scripts/scripts/#variables-and-data-types","title":"Variables and Data Types","text":"<pre><code>NAME=\"value\"          # String\nCOUNT=42              # Integer\nFILES=(*.txt)         # Array\nreadonly CONST=100    # Constant\nexport GLOBAL_VAR     # Env variable\n</code></pre>"},{"location":"shell-scripts/scripts/#special-variables","title":"Special Variables","text":"<pre><code>$0    # Script name\n$1    # First argument\n$#    # Number of args\n$@    # All arguments\n$?    # Exit status\n$$    # PID\n$!    # Background PID\n</code></pre>"},{"location":"shell-scripts/scripts/#how-to-write-shell-scripts","title":"How to Write Shell Scripts","text":""},{"location":"shell-scripts/scripts/#1-conditions","title":"1. Conditions","text":"<pre><code>if [ -f /etc/passwd ]; then\n  echo \"File exists\"\nelse\n  echo \"File missing\"\nfi\n</code></pre>"},{"location":"shell-scripts/scripts/#2-loops","title":"2. Loops","text":"<pre><code>for i in {1..5}; do\n  echo \"Iteration $i\"\ndone\n</code></pre>"},{"location":"shell-scripts/scripts/#3-functions","title":"3. Functions","text":"<pre><code>greet() { echo \"Hello $1\"; }\ngreet \"Admin\"\n</code></pre>"},{"location":"shell-scripts/scripts/#4-case-switch","title":"4. Case (Switch)","text":"<pre><code>case $1 in\n  start) echo \"Starting service\" ;;\n  stop)  echo \"Stopping service\" ;;\n  *)     echo \"Usage: $0 {start|stop}\" ;;\nesac\n</code></pre>"},{"location":"shell-scripts/scripts/#5-modular-scripts","title":"5. Modular Scripts","text":"<pre><code>source ./utils.sh\nsay_hello \"DevOps\"\n</code></pre>"},{"location":"shell-scripts/scripts/#advanced-scripting-techniques","title":"Advanced Scripting Techniques","text":""},{"location":"shell-scripts/scripts/#parameter-expansion","title":"Parameter Expansion","text":"<pre><code>${VAR:-default}    # Use default if unset\n${VAR:=default}    # Set default if unset\n${#VAR}            # Length\n${VAR#pattern}     # Remove prefix\n${VAR%pattern}     # Remove suffix\n</code></pre>"},{"location":"shell-scripts/scripts/#arrays-and-associative-arrays","title":"Arrays and Associative Arrays","text":"<pre><code>FILES=(\"f1\" \"f2\")\ndeclare -A CONFIG\nCONFIG[\"host\"]=\"example.com\"\n</code></pre>"},{"location":"shell-scripts/scripts/#inputoutput-handling","title":"Input/Output Handling","text":"<pre><code>read -p \"Enter: \" INPUT\ncat &lt;&lt;END &gt; file.txt\nmulti-line\nEND\n</code></pre>"},{"location":"shell-scripts/scripts/#error-handling-and-debugging","title":"Error Handling and Debugging","text":"<pre><code>set -euo pipefail\ntrap 'echo \"Error at line $LINENO\"' ERR\nset -x   # debug on\nset +x   # debug off\n</code></pre>"},{"location":"shell-scripts/scripts/#benefits-of-shell-scripting","title":"Benefits of Shell Scripting","text":"<ul> <li>Fast prototyping of automation.</li> <li>Integration with system tools (<code>systemctl</code>, <code>docker</code>, <code>kubectl</code>, <code>rsync</code>).</li> <li>Portability across Linux distributions.</li> <li>Low dependency: no need for extra runtimes.</li> </ul>"},{"location":"shell-scripts/scripts/#advanced-examples-for-production-environments","title":"Advanced Examples for Production Environments","text":""},{"location":"shell-scripts/scripts/#1-comprehensive-system-health-check","title":"1. Comprehensive System Health Check","text":"<p>(Full script with disk and memory checks, logging, colors.)</p>"},{"location":"shell-scripts/scripts/#2-advanced-log-analyzer","title":"2. Advanced Log Analyzer","text":"<p>(Grep-based log analysis with report generation.)</p>"},{"location":"shell-scripts/scripts/#3-kubernetes-deployment-helper","title":"3. Kubernetes Deployment Helper","text":"<p>(Validates YAML, applies deployment, waits for rollout.)</p>"},{"location":"shell-scripts/scripts/#4-secure-configuration-manager","title":"4. Secure Configuration Manager","text":"<p>(Encrypt/decrypt configs with OpenSSL.)</p>"},{"location":"shell-scripts/scripts/#5-advanced-backup-system","title":"5. Advanced Backup System","text":"<p>(Database backups, retention, verification.)</p>"},{"location":"shell-scripts/scripts/#best-practices-for-production-scripts","title":"Best Practices for Production Scripts","text":""},{"location":"shell-scripts/scripts/#security","title":"Security","text":"<ul> <li>Avoid command injection.</li> <li>Use <code>mktemp</code> for temp files.</li> </ul>"},{"location":"shell-scripts/scripts/#performance","title":"Performance","text":"<ul> <li>Prefer shell built-ins over external commands.</li> <li>Process large files with <code>while read</code>.</li> </ul>"},{"location":"shell-scripts/scripts/#portability","title":"Portability","text":"<ul> <li>Prefer POSIX-compliant syntax when possible.</li> </ul>"},{"location":"shell-scripts/scripts/#documentation","title":"Documentation","text":"<ul> <li>Include script headers (purpose, author, usage).</li> <li>Document functions with parameters and return codes.</li> </ul>"},{"location":"shell-scripts/scripts/#integration-with-modern-devops-tools","title":"Integration with Modern DevOps Tools","text":""},{"location":"shell-scripts/scripts/#cicd-pipelines","title":"CI/CD Pipelines","text":"<p>(Shell stages for Jenkins, rollback, Slack alerts.)</p>"},{"location":"shell-scripts/scripts/#cloud-integration","title":"Cloud Integration","text":"<ul> <li>AWS: <code>aws s3 cp</code></li> <li>GCP: <code>gsutil cp</code></li> <li>Azure: <code>az storage blob upload</code></li> </ul>"},{"location":"shell-scripts/scripts/#monitoring-and-logging","title":"Monitoring and Logging","text":""},{"location":"shell-scripts/scripts/#performance-monitoring","title":"Performance Monitoring","text":"<ul> <li>Track script execution time with <code>$SECONDS</code>.</li> <li>Send metrics to monitoring systems.</li> </ul>"},{"location":"shell-scripts/scripts/#structured-logging","title":"Structured Logging","text":"<ul> <li>JSON logging with <code>jq</code> for easy integration.</li> </ul>"},{"location":"sonarqube/sonarqube/","title":"SonarQube Installation, Configuration, and Integration with Jenkins","text":"Mastering SonarQube: Installation, Configuration, and Jenkins Integration <p>SonarQube is an open-source platform that provides continuous inspection of code quality. It performs automatic reviews to detect bugs, vulnerabilities, and code smells in your codebase. This guide will walk you through installing SonarQube on Ubuntu, configuring it, and integrating it with Jenkins to enhance your continuous integration and continuous delivery (CI/CD) pipeline.</p>"},{"location":"sonarqube/sonarqube/#what-is-sonarqube","title":"What is SonarQube?","text":"<p>SonarQube is a powerful tool for continuous code quality inspection. It helps developers identify and fix issues early in the development lifecycle, ensuring that their code is secure, maintainable, and efficient. SonarQube can analyze code in various programming languages, providing feedback on code quality metrics such as bugs, vulnerabilities, code smells, duplications, and test coverage.</p>"},{"location":"sonarqube/sonarqube/#step-1-installing-sonarqube-on-ubuntu","title":"Step 1: Installing SonarQube on Ubuntu","text":""},{"location":"sonarqube/sonarqube/#11-prerequisites","title":"1.1 Prerequisites","text":"<p>Before installing SonarQube, ensure that you have Java (JDK 11 or newer) installed. If not, install it using the following command:</p> <pre><code>sudo apt update\nsudo apt install openjdk-11-jdk -y\n</code></pre> <p>Verify the Java installation:</p> <pre><code>java -version\n</code></pre>"},{"location":"sonarqube/sonarqube/#12-install-dependencies","title":"1.2 Install Dependencies","text":"<p>SonarQube requires PostgreSQL as its database backend. To install PostgreSQL:</p> <pre><code>sudo apt install postgresql postgresql-contrib -y\n</code></pre> <p>After installation, create a new database and user for SonarQube:</p> <pre><code>sudo -u postgres psql\nCREATE USER sonar WITH PASSWORD 'sonar';\nCREATE DATABASE sonar;\nGRANT ALL PRIVILEGES ON DATABASE sonar TO sonar;\n\\q\n</code></pre>"},{"location":"sonarqube/sonarqube/#13-download-and-install-sonarqube","title":"1.3 Download and Install SonarQube","text":"<p>Now, download the latest version of SonarQube from the official website:</p> <pre><code>wget https://binaries.sonarsource.com/CommercialEdition/sonarqube-9.3.0.51899.zip\n</code></pre> <p>Unzip the downloaded file:</p> <pre><code>unzip sonarqube-9.3.0.51899.zip\nsudo mv sonarqube-9.3.0.51899 /opt/sonarqube\n</code></pre>"},{"location":"sonarqube/sonarqube/#14-configure-sonarqube","title":"1.4 Configure SonarQube","text":"<p>Navigate to the SonarQube configuration file:</p> <pre><code>cd /opt/sonarqube/conf\nsudo nano sonar.properties\n</code></pre> <p>Edit the following properties to set up your PostgreSQL database:</p> <pre><code>sonar.jdbc.url=jdbc:postgresql://localhost/sonar\nsonar.jdbc.username=sonar\nsonar.jdbc.password=sonar\n</code></pre>"},{"location":"sonarqube/sonarqube/#15-start-sonarqube","title":"1.5 Start SonarQube","text":"<p>SonarQube is bundled with a script to start the application. You can start SonarQube using the following commands:</p> <pre><code>cd /opt/sonarqube/bin/linux-x86-64\n./sonar.sh start\n</code></pre> <p>You can verify that SonarQube is running by visiting:</p> <pre><code>http://localhost:9000\n</code></pre> <p>The default login credentials are:</p> <ul> <li>Username: admin</li> <li>Password: admin</li> </ul>"},{"location":"sonarqube/sonarqube/#step-2-installing-the-sonarqube-plugin-for-jenkins","title":"Step 2: Installing the SonarQube Plugin for Jenkins","text":"<p>To integrate SonarQube with Jenkins, you need to install the SonarQube Scanner for Jenkins plugin.</p>"},{"location":"sonarqube/sonarqube/#21-install-the-plugin","title":"2.1 Install the Plugin","text":"<ol> <li>Open Jenkins in your browser (<code>http://localhost:8080</code>).</li> <li>Navigate to Manage Jenkins &gt; Manage Plugins.</li> <li>Search for SonarQube Scanner in the Available tab and install it.</li> </ol>"},{"location":"sonarqube/sonarqube/#step-3-configuring-sonarqube-in-jenkins","title":"Step 3: Configuring SonarQube in Jenkins","text":""},{"location":"sonarqube/sonarqube/#31-configure-sonarqube-server-in-jenkins","title":"3.1 Configure SonarQube Server in Jenkins","text":"<ol> <li>Go to Manage Jenkins &gt; Configure System.</li> <li>Scroll down to the SonarQube Servers section.</li> <li>Click Add SonarQube and enter the following details:</li> <li>Name: SonarQube (or any name you prefer)</li> <li>Server URL: <code>http://localhost:9000</code></li> <li>Authentication Token: To generate an authentication token, log in to SonarQube and go to My Account &gt; Security &gt; Generate Tokens.</li> </ol>"},{"location":"sonarqube/sonarqube/#32-install-the-sonarqube-scanner-in-jenkins","title":"3.2 Install the SonarQube Scanner in Jenkins","text":"<ol> <li>In the SonarQube Scanner section, click Add SonarQube Scanner.</li> <li>Enter the Installation Name (e.g., SonarQube Scanner) and set the Version.</li> </ol> <p>Jenkins will automatically detect the SonarQube Scanner when the plugin is installed.</p>"},{"location":"sonarqube/sonarqube/#step-4-create-a-jenkins-pipeline-to-run-sonarqube-analysis","title":"Step 4: Create a Jenkins Pipeline to Run SonarQube Analysis","text":""},{"location":"sonarqube/sonarqube/#41-create-a-new-jenkins-pipeline","title":"4.1 Create a New Jenkins Pipeline","text":"<ol> <li>From the Jenkins dashboard, click on New Item.</li> <li>Choose Pipeline and give it a name (e.g., SonarQube-Pipeline).</li> <li>Click OK.</li> </ol>"},{"location":"sonarqube/sonarqube/#42-define-the-pipeline-script","title":"4.2 Define the Pipeline Script","text":"<p>Add the following script in the Pipeline section. This pipeline will perform a SonarQube analysis on your project:</p> <pre><code>pipeline {\n    agent any\n    environment {\n        SONARQUBE = 'SonarQube'  // The name of the SonarQube server configured in Jenkins\n    }\n    stages {\n        stage('Checkout') {\n            steps {\n                git 'https://github.com/your-repository-url.git'\n            }\n        }\n        stage('SonarQube Analysis') {\n            steps {\n                script {\n                    // Run the SonarQube scanner\n                    sh \"sonar-scanner\"\n                }\n            }\n        }\n        stage('Build') {\n            steps {\n                // Build your project (e.g., using Maven, Gradle, etc.)\n                sh 'mvn clean install'\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"sonarqube/sonarqube/#43-run-the-pipeline","title":"4.3 Run the Pipeline","text":"<p>Save the pipeline and click Build Now. Jenkins will execute the pipeline, run the SonarQube analysis, and publish the results to SonarQube.</p>"},{"location":"sonarqube/sonarqube/#step-5-view-sonarqube-analysis-results","title":"Step 5: View SonarQube Analysis Results","text":"<p>After the pipeline runs, you can view the results by logging into the SonarQube dashboard at:</p> <pre><code>http://localhost:9000\n</code></pre> <p>Here, you\u2019ll see detailed code quality metrics, including:</p> <ul> <li>Code coverage</li> <li>Code duplication</li> <li>Code smells</li> <li>Vulnerabilities</li> <li>Bugs</li> </ul> <p>By following these steps, you\u2019ve successfully installed SonarQube, configured it, and integrated it with Jenkins to perform continuous code quality analysis. With SonarQube integrated into your Jenkins pipeline, you can automatically monitor code quality, identify issues early, and maintain high-quality code throughout your development lifecycle.</p> <p>?? info \"Tip\" For better code quality enforcement, integrate SonarQube with Jenkins' automated build and testing process to prevent merging code that does not meet your quality standards.</p> <pre><code>## Key Features:\n1. **SonarQube Setup**: The guide walks users through installing and configuring SonarQube on Ubuntu with PostgreSQL as the database backend.\n2. **Jenkins Integration**: Instructions on integrating SonarQube with Jenkins via the SonarQube Scanner plugin.\n3. **Pipeline Example**: A Jenkins pipeline example that runs SonarQube analysis as part of the build process.\n4. **User-Friendly Navigation**: This guide is broken down into easy-to-follow steps with detailed explanations for each phase.\n</code></pre>"},{"location":"terraform/overview/","title":"Terraform","text":"<p>Terraform is an Infrastructure as Code (IaC) tool by HashiCorp that allows you to define, provision, and manage infrastructure across cloud providers using declarative configuration files.</p>"},{"location":"terraform/overview/#why-terraform","title":"Why Terraform","text":"<ul> <li>Ideal for creating and managing infrastructure.</li> <li>Uses HCL (HashiCorp Configuration Language) \u2014 a human-readable, declarative language.</li> </ul>"},{"location":"terraform/overview/#example","title":"Example","text":"<pre><code>resource \"aws_instance\" \"example\" {\n  ami           = \"ami-0abcd1234\"\n  instance_type = \"t2.micro\"\n}\n</code></pre>"},{"location":"terraform/overview/#how-terraform-communicates-with-cloud-providers","title":"How Terraform Communicates with Cloud Providers","text":"<ol> <li> <p>Providers (Plugins)    Terraform uses provider plugins to communicate with clouds.</p> </li> <li> <p><code>hashicorp/aws</code> for AWS</p> </li> <li><code>hashicorp/azurerm</code> for Azure</li> <li> <p><code>hashicorp/google</code> for GCP</p> </li> <li> <p>APIs    Each provider calls the official cloud API (REST or SDK) to create resources.</p> </li> <li> <p>Authentication    Terraform needs credentials to access APIs:</p> </li> </ol> <pre><code>provider \"aws\" {\n  access_key = \"YOUR_ACCESS_KEY\"\n  secret_key = \"YOUR_SECRET_KEY\"\n  region     = \"us-east-1\"\n}\n</code></pre> <ol> <li> <p>Terraform Workflow</p> </li> <li> <p><code>init</code> \u2192 Download providers</p> </li> <li><code>plan</code> \u2192 Preview changes</li> <li><code>apply</code> \u2192 Create or modify resources</li> <li><code>state</code> \u2192 Save results locally or remotely</li> </ol>"},{"location":"terraform/overview/#example-flow","title":"Example Flow","text":"<pre><code>resource \"aws_s3_bucket\" \"demo\" {\n  bucket = \"my-demo-bucket\"\n}\n</code></pre> <p>Terraform uses the AWS provider to authenticate, send an API call, and record the result in <code>terraform.tfstate</code>.</p> <p>Summary: Terraform communicates through provider plugins that use official cloud APIs to create and manage resources.</p>"},{"location":"terraform/overview/#aws-cli-installation-configuration","title":"AWS CLI Installation &amp; Configuration","text":""},{"location":"terraform/overview/#1-install-aws-cli","title":"1. Install AWS CLI","text":"<p>macOS</p> <pre><code>brew install awscli\naws --version\n</code></pre> <p>Windows</p> <pre><code>choco install awscli\naws --version\n</code></pre> <p>Manual installers are available at: https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html</p>"},{"location":"terraform/overview/#2-configure-aws-cli","title":"2. Configure AWS CLI","text":"<pre><code>aws configure\n</code></pre> <p>Enter:</p> <pre><code>AWS Access Key ID [None]: test\nAWS Secret Access Key [None]: test\nDefault region name [None]: us-east-1\nDefault output format [None]: json\n</code></pre> <p>Configuration files:</p> <pre><code>~/.aws/credentials\n~/.aws/config\n</code></pre> <p>Windows path:</p> <pre><code>C:\\Users\\&lt;username&gt;\\.aws\\\n</code></pre>"},{"location":"terraform/overview/#3-test-configuration","title":"3. Test Configuration","text":"<pre><code>aws s3 ls\n</code></pre> <p>With LocalStack:</p> <pre><code>aws --endpoint-url=http://localhost:4566 s3 ls\n</code></pre>"},{"location":"terraform/overview/#4-optional-custom-profiles","title":"4. Optional: Custom Profiles","text":"<pre><code>aws configure --profile localstack\naws --profile localstack --endpoint-url=http://localhost:4566 s3 ls\n</code></pre>"},{"location":"terraform/overview/#localstack-setup","title":"LocalStack Setup","text":"<p>LocalStack emulates AWS services locally using Docker \u2014 perfect for testing Terraform and AWS CLI without real AWS credentials.</p>"},{"location":"terraform/overview/#why-use-localstack","title":"Why Use LocalStack","text":"<ul> <li>Avoid AWS costs</li> <li>Work offline</li> <li>Fast and safe testing</li> <li>Supports S3, EC2, Lambda, DynamoDB, and more</li> </ul>"},{"location":"terraform/overview/#1-prerequisites","title":"1. Prerequisites","text":"Tool Purpose macOS Install Command Docker Desktop Runs containers <code>brew install --cask docker</code> Python 3.8+ Required for CLI Preinstalled pip Python package manager <code>python3 -m ensurepip --upgrade</code>"},{"location":"terraform/overview/#2-install-localstack","title":"2. Install LocalStack","text":"<pre><code>pip install localstack awscli-local\nlocalstack --version\n</code></pre>"},{"location":"terraform/overview/#3-start-localstack","title":"3. Start LocalStack","text":"<pre><code>localstack start\n</code></pre> <p>Runs on <code>http://localhost:4566</code>.</p>"},{"location":"terraform/overview/#4-web-dashboard","title":"4. Web Dashboard","text":"<p>Terminal  </p>"},{"location":"terraform/overview/#5-test","title":"5. Test","text":"<pre><code>awslocal s3 ls\nawslocal s3 mb s3://demo-bucket\n</code></pre>"},{"location":"terraform/overview/#6-docker-compose-example","title":"6. Docker Compose Example","text":"<pre><code>version: \"3.8\"\nservices:\n  localstack:\n    image: localstack/localstack\n    ports:\n      - \"4566:4566\"\n    environment:\n      - SERVICES=s3,ec2,lambda\n      - DEBUG=1\n    volumes:\n      - \"./localstack:/var/lib/localstack\"\n</code></pre> <p>Run:</p> <pre><code>docker-compose up\n</code></pre>"},{"location":"terraform/overview/#terraform-localstack-integration","title":"Terraform + LocalStack Integration","text":""},{"location":"terraform/overview/#project-structure","title":"Project Structure","text":"<pre><code>terraform-localstack-demo/\n\u251c\u2500\u2500 main.tf\n\u251c\u2500\u2500 provider.tf\n\u251c\u2500\u2500 outputs.tf\n\u2514\u2500\u2500 hello.txt\n</code></pre>"},{"location":"terraform/overview/#step-1-create-a-test-file","title":"Step 1: Create a Test File","text":"<pre><code>echo \"Hello from Terraform + LocalStack!\" &gt; hello.txt\n</code></pre>"},{"location":"terraform/overview/#step-2-providertf","title":"Step 2: provider.tf","text":"<pre><code>terraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 5.0\"\n    }\n  }\n  required_version = \"&gt;= 1.5.0\"\n}\n\nprovider \"aws\" {\n  region                      = \"us-east-1\"\n  access_key                  = \"test\"\n  secret_key                  = \"test\"\n  s3_force_path_style         = true\n  skip_credentials_validation = true\n  skip_metadata_api_check     = true\n  skip_requesting_account_id  = true\n  endpoints {\n    s3 = \"http://localhost:4566\"\n  }\n}\n</code></pre>"},{"location":"terraform/overview/#step-3-maintf","title":"Step 3: main.tf","text":"<pre><code>resource \"aws_s3_bucket\" \"demo\" {\n  bucket = \"terraform-localstack-demo\"\n}\n\nresource \"aws_s3_object\" \"file_upload\" {\n  bucket = aws_s3_bucket.demo.id\n  key    = \"hello.txt\"\n  source = \"hello.txt\"\n  etag   = filemd5(\"hello.txt\")\n}\n</code></pre>"},{"location":"terraform/overview/#step-4-outputstf","title":"Step 4: outputs.tf","text":"<pre><code>output \"bucket_name\" {\n  value = aws_s3_bucket.demo.bucket\n}\n</code></pre>"},{"location":"terraform/overview/#step-5-deploy","title":"Step 5: Deploy","text":"<pre><code>terraform init\nterraform plan\nterraform apply -auto-approve\n</code></pre> <p>Verify:</p> <pre><code>awslocal s3 ls\nawslocal s3 ls s3://terraform-localstack-demo/\n</code></pre>"},{"location":"terraform/overview/#cleanup-and-destroy","title":"Cleanup and Destroy","text":""},{"location":"terraform/overview/#destroy-resources","title":"Destroy Resources","text":"<pre><code>terraform destroy -auto-approve\n</code></pre> <p>Output:</p> <pre><code>Destroy complete! Resources: 2 destroyed.\n</code></pre>"},{"location":"terraform/overview/#optional-preview-destroy","title":"Optional: Preview Destroy","text":"<pre><code>terraform plan -destroy\n</code></pre>"},{"location":"terraform/overview/#verify-deletion","title":"Verify Deletion","text":"<pre><code>awslocal s3 ls\n</code></pre>"},{"location":"terraform/overview/#reset-localstack-optional","title":"Reset LocalStack (Optional)","text":"<pre><code>localstack stop\ndocker rm -f localstack\ndocker volume prune -f\nlocalstack start\n</code></pre>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/archive/2018/","title":"2018","text":""},{"location":"blog/archive/2017/","title":"2017","text":""}]}