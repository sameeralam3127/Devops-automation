{"config":{"separator":"[\\s\\-_,:!=\\[\\]()\\\\\"`/]+|\\.(?!\\d)"},"items":[{"location":"","level":1,"title":"DevOps Automation","text":"<p>Welcome to DevOps Automation – your resource for everything DevOps and Site Reliability Engineering (SRE). Whether you are experienced or just starting, this site will help you explore tools, practical tips, and connect with a professional community.</p> <p>Practical advice to help you deliver and manage code effectively.</p> <p>Discover the latest tools and approaches to keep your systems reliable and secure.</p> <p>Step-by-step guides for setup, deployment, and system improvement.</p> <p>I am Sameer Alam, a DevOps engineer and SRE practitioner. My focus is simplifying complex systems and ensuring reliability. Over the years, I have gained insights into what makes systems operate efficiently.</p> <p>See my projects: GitHub Blog Explore my earlier blog posts from 2016, when I first began documenting my learning. For questions or ideas, feel free to reach out.</p> <p>– Sameer Alam</p>","path":["DevOps Automation"],"tags":[]},{"location":"ansible/ansible/","level":1,"title":"Introduction to Ansible","text":"","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"ansible/ansible/#1-what-is-ansible","level":2,"title":"1. What is Ansible?","text":"<p>Ansible is an open-source IT automation tool that allows you to automate provisioning, configuration management, application deployment, and orchestration.</p> <ul> <li>It uses simple YAML files (called playbooks) to define automation tasks.</li> <li>It is agentless, requiring only SSH access (or WinRM for Windows) to manage remote systems.</li> </ul> <p>Earlier approaches/tools used before Ansible:</p> <ul> <li>Manual scripting with Bash/Shell scripts or Python scripts.</li> <li>Configuration management tools such as Puppet and Chef, which required agents and had steeper learning curves.</li> <li>Orchestration and provisioning tools like Terraform (still widely used, but focused more on infrastructure provisioning rather than configuration management).</li> </ul> <p>Note</p> <p>Ansible excels at configuration management, whereas Terraform is stronger for infrastructure provisioning.</p>","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"ansible/ansible/#2-core-concepts","level":2,"title":"2. Core Concepts","text":"<ol> <li>Inventory – A file listing the machines (hosts) you want to manage.</li> <li>Playbooks – YAML files that define sets of tasks for execution.</li> <li>Modules – Pre-built units of work (installing packages, copying files, managing services, etc.).</li> <li>Roles – A structured way to organize playbooks and files for reuse.</li> </ol>","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"ansible/ansible/#3-configuration-file","level":2,"title":"3. Configuration File","text":"<p>Ansible can use a configuration file (<code>ansible.cfg</code>). It is loaded in the following order:</p> <ol> <li><code>ANSIBLE_CONFIG</code> (if set as an environment variable)</li> <li><code>ansible.cfg</code> (in the current directory)</li> <li><code>~/.ansible.cfg</code> (in the home directory)</li> <li><code>/etc/ansible/ansible.cfg</code></li> </ol> <p>Tip</p> <p>Always create a project-level <code>ansible.cfg</code> to avoid conflicts with global settings.</p>","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"ansible/ansible/#4-installing-ansible","level":2,"title":"4. Installing Ansible","text":"<ul> <li>Using pip (Python package manager):</li> </ul> <pre><code>pip install ansible\n</code></pre> <ul> <li>On Ubuntu/Debian:</li> </ul> <pre><code>sudo apt update\nsudo apt install ansible\n</code></pre> <ul> <li>On macOS (Homebrew):</li> </ul> <pre><code>brew install ansible\n</code></pre> <ul> <li>On Windows (via WSL):   Install Ansible inside Ubuntu/Debian WSL using the steps above.</li> </ul> <p>Verify installation:</p> <pre><code>ansible --version\n</code></pre> <p>Warning</p> <p>Do not mix OS package installs (apt, brew) with pip installs on the same machine to avoid version conflicts.</p>","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"ansible/ansible/#5-working-with-inventory","level":2,"title":"5. Working with Inventory","text":"<p>Create a file named <code>hosts.ini</code>:</p> <pre><code>[webservers]\n192.168.1.10\n192.168.1.11\n\n[databases]\ndb1.example.com\n</code></pre> <p>List hosts:</p> <pre><code>ansible all --list-hosts\n</code></pre>","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"ansible/ansible/#6-running-ad-hoc-commands","level":2,"title":"6. Running Ad-Hoc Commands","text":"<ul> <li>Ping all hosts:</li> </ul> <pre><code>ansible all -m ping\nansible all -m ping -vvv\n</code></pre> <ul> <li>Run commands with password prompt:</li> </ul> <pre><code>ansible web -m command -a \"uptime\" -k\nansible web -m command -a \"yum install httpd* -y\" -k\n</code></pre> <ul> <li>Copy and fetch files:</li> </ul> <pre><code>ansible all -m copy -a \"src=/etc/passwd dest=/tmp\"\nansible all -m fetch -a \"src=/var/log/yum.log dest=/logs\"\n</code></pre> <ul> <li>File management:</li> </ul> <pre><code>ansible all -m file -a \"path=/tmp/india mode=777\"\nansible all -m file -a \"path=/tmp/india state=absent\"\n</code></pre> <ul> <li>Run shell scripts:</li> </ul> <pre><code>ansible all -m shell -a \"sh /tmp/scripts.sh\"\n</code></pre>","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"ansible/ansible/#7-writing-your-first-playbook","level":2,"title":"7. Writing Your First Playbook","text":"<p><code>playbook.yml</code>:</p> <pre><code>---\n- name: Install Apache on webservers\n  hosts: webservers\n  become: yes\n  tasks:\n    - name: Ensure Apache is installed\n      apt:\n        name: apache2\n        state: present\n      when: ansible_os_family == \"Debian\"\n</code></pre> <p>Run it:</p> <pre><code>ansible-playbook -i hosts.ini playbook.yml\n</code></pre>","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"ansible/ansible/#8-gathering-facts","level":2,"title":"8. Gathering Facts","text":"<p>Ansible automatically collects system information (facts):</p> <pre><code>ansible all -m setup\nansible all -m setup -a \"filter=ansible_python_version\"\n</code></pre>","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"ansible/ansible/#9-verbose-mode","level":2,"title":"9. Verbose Mode","text":"<ul> <li><code>-v</code> basic verbose</li> <li><code>-vv</code> more details</li> <li><code>-vvv</code> debug level (SSH, facts, module arguments)</li> </ul>","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"ansible/ansible/#10-disabling-host-key-checking","level":2,"title":"10. Disabling Host Key Checking","text":"<pre><code>export ANSIBLE_HOST_KEY_CHECKING=False\n</code></pre> <p>Or update <code>ansible.cfg</code>:</p> <pre><code>[defaults]\nhost_key_checking = False\n</code></pre>","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"ansible/ansible/#11-troubleshooting-common-issues","level":2,"title":"11. Troubleshooting Common Issues","text":"<ul> <li>Python Interpreter Issue:   Some hosts may not have Python installed or use a different version. Define interpreter in inventory:</li> </ul> <pre><code>[webservers]\n192.168.1.10 ansible_python_interpreter=/usr/bin/python3\n</code></pre> <ul> <li>SSH Key Authentication (Passwordless):</li> </ul> <pre><code>ssh-keygen\nssh-copy-id user@host\n</code></pre> <ul> <li>Connection Problems:   Use <code>ansible -vvv</code> for detailed debugging.</li> </ul> <p>Note</p> <p>Always verify SSH connectivity manually before using Ansible.</p>","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"ansible/ansible/#12-advanced-usage","level":2,"title":"12. Advanced Usage","text":"<ul> <li>Modules: Explore with:</li> </ul> <pre><code>ansible-doc -l\nansible-doc copy\n</code></pre> <ul> <li>Packages:</li> </ul> <pre><code>ansible all -m package -a \"name=httpd state=present\"\nansible all -m package -a \"name=samba* state=latest use=yum\"\n</code></pre>","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"ansible/ansible/#13-roles","level":2,"title":"13. Roles","text":"<p>Organize reusable playbooks:</p> <ul> <li><code>tasks/</code></li> <li><code>handlers/</code></li> <li><code>templates/</code></li> <li><code>files/</code></li> <li><code>vars/</code> / <code>defaults/</code></li> </ul>","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"ansible/ansible/#14-automation-tower-awxansible-tower","level":2,"title":"14. Automation Tower (AWX/Ansible Tower)","text":"<ul> <li>A web-based UI and REST API to manage playbooks.</li> <li>Provides RBAC, job scheduling, logging, and notifications.</li> <li>AWX is the open-source upstream project of Ansible Tower.</li> </ul>","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"ansible/ansible/#15-terraform-vs-ansible","level":2,"title":"15. Terraform vs Ansible","text":"<ul> <li>Terraform: Best for provisioning and managing infrastructure as code (e.g., creating servers, networks, cloud resources).</li> <li>Ansible: Best for configuration management, application deployment, and post-provisioning tasks.</li> <li>They are often used together: Terraform provisions, Ansible configures.</li> </ul> <p>Tip</p> <p>Use Terraform to build infrastructure, then hand off to Ansible for configuration and deployment.</p>","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"ansible/ansible/#16-community-and-learning-resources","level":2,"title":"16. Community and Learning Resources","text":"<ul> <li>Official Documentation: Ansible Docs</li> <li>Ansible Galaxy: A hub for finding and sharing community-developed roles.</li> <li>Community: Forums, GitHub repositories, and mailing lists.</li> <li> <p>Practice Ideas:</p> </li> <li> <p>Automate a web server deployment.</p> </li> <li>Configure a database.</li> <li>Deploy a multi-tier app with roles.</li> </ul> <p>Success</p> <p>Recommendation: Use virtual machines, WSL, or cloud instances to practice Ansible playbooks. Next step: Start with simple ad-hoc commands, then move to playbooks and roles for real automation.</p>","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"ansible/ansible/#troubleshooting-roles-and-advanced-details","level":1,"title":"Troubleshooting, Roles, and Advanced Details","text":"","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"ansible/ansible/#1-troubleshooting-and-environment-variables","level":2,"title":"1. Troubleshooting and Environment Variables","text":"","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"ansible/ansible/#environment-variables-vs-config-file-precedence","level":2,"title":"Environment Variables vs. Config File Precedence","text":"<ul> <li>Environment variables (e.g., <code>export ANSIBLE_CONFIG=/custom/path/ansible.cfg</code>) take highest precedence.</li> <li>Config file settings (e.g., <code>ansible.cfg</code>) are overridden by environment variables.</li> <li>Order of precedence (highest to lowest):</li> <li>Command-line flags (e.g., <code>--private-key</code>)</li> <li>Environment variables (e.g., <code>ANSIBLE_PRIVATE_KEY_FILE</code>)</li> <li>Config file settings</li> <li>Default values</li> </ul>","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"ansible/ansible/#common-troubleshooting-tips","level":2,"title":"Common Troubleshooting Tips","text":"<ol> <li>SSH Connection Issues:</li> </ol> <pre><code># Test SSH connectivity manually first:\nssh user@target-host\n\n# Enable verbose Ansible output:\nansible -i inventory.ini all -m ping -vvv\n</code></pre> <ol> <li>Python Interpreter Errors:</li> </ol> <pre><code># In inventory.ini, specify Python path:\n[webservers]\nweb1 ansible_host=192.168.1.10 ansible_python_interpreter=/usr/bin/python3\n</code></pre> <ol> <li>Permission Denied:    Use <code>--become</code> (<code>-b</code>) to escalate privileges:</li> </ol> <pre><code>ansible -i inventory.ini all -m package -a \"name=nginx\" --become\n</code></pre> <ol> <li>Debug Modules:    Use the <code>debug</code> module to print variables:    <pre><code>- name: Display variables\n  debug:\n    var: my_variable\n</code></pre></li> </ol>","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"ansible/ansible/#2-creating-roles-simplified","level":2,"title":"2. Creating Roles Simplified","text":"","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"ansible/ansible/#step-by-step-role-creation","level":2,"title":"Step-by-Step Role Creation","text":"<ol> <li>Generate role structure:</li> </ol> <pre><code>ansible-galaxy role init my_role\n</code></pre> <p>Creates:</p> <pre><code>my_role/\n├── defaults/    # Low-priority variables\n├── tasks/       # Main tasks\n├── handlers/    # Handlers\n├── templates/   # Jinja2 templates\n├── files/       # Static files\n├── vars/        # High-priority variables\n└── meta/        # Role dependencies\n</code></pre> <ol> <li>Add tasks (<code>tasks/main.yml</code>):</li> </ol> <pre><code>- name: Install Nginx\n  apt:\n    name: nginx\n    state: present\n  notify: restart nginx\n</code></pre> <ol> <li>Add handlers (<code>handlers/main.yml</code>):</li> </ol> <pre><code>- name: restart nginx\n  service:\n    name: nginx\n    state: restarted\n</code></pre> <ol> <li>Use the role in a playbook:    <pre><code>- hosts: webservers\n  roles:\n    - my_role\n</code></pre></li> </ol>","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"ansible/ansible/#3-group-variables-inventory-and-host-variables","level":2,"title":"3. Group Variables, Inventory, and Host Variables","text":"","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"ansible/ansible/#dynamic-inventory","level":2,"title":"Dynamic Inventory","text":"<p>Use dynamic inventories for cloud providers (AWS, Azure):</p> <pre><code>ansible-inventory -i aws_ec2.yml --list\n</code></pre>","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"ansible/ansible/#group-variables","level":2,"title":"Group Variables","text":"<ol> <li>Directory structure:</li> </ol> <pre><code>inventory/\n├── group_vars/\n│   ├── webservers.yml\n│   └── databases.yml\n└── hosts.ini\n</code></pre> <ol> <li>Define group variables (<code>group_vars/webservers.yml</code>):    <pre><code>---\nhttp_port: 80\nntp_servers: [0.pool.ntp.org, 1.pool.ntp.org]\n</code></pre></li> </ol>","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"ansible/ansible/#host-variables","level":2,"title":"Host Variables","text":"<ol> <li>In inventory file:</li> </ol> <pre><code>[webservers]\nweb1 ansible_host=192.168.1.10 http_port=8080\n</code></pre> <ol> <li>In <code>host_vars/</code> directory:    <pre><code>inventory/\n├── host_vars/\n│   └── web1.yml\n</code></pre> <code>web1.yml</code>:    <pre><code>---\nhttp_port: 8080\n</code></pre></li> </ol>","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"ansible/ansible/#4-advanced-configuration-details","level":2,"title":"4. Advanced Configuration Details","text":"","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"ansible/ansible/#custom-config-file","level":2,"title":"Custom Config File","text":"<p>Create a project-specific <code>ansible.cfg</code>:</p> <pre><code>[defaults]\ninventory = ./inventory/hosts.ini\nroles_path = ./roles\nretry_files_enabled = False\nhost_key_checking = False\n\n[privilege_escalation]\nbecome = True\nbecome_method = sudo\nbecome_user = root\n</code></pre>","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"ansible/ansible/#vault-for-secrets","level":2,"title":"Vault for Secrets","text":"<p>Encrypt sensitive data:</p> <pre><code>ansible-vault create secrets.yml\nansible-playbook site.yml --ask-vault-pass\n</code></pre>","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"ansible/ansible/#5-example-full-project-structure","level":2,"title":"5. Example: Full Project Structure","text":"<pre><code>my_ansible_project/\n├── ansible.cfg\n├── inventory/\n│   ├── hosts.ini\n│   ├── group_vars/\n│   │   └── webservers.yml\n│   └── host_vars/\n│       └── web1.yml\n├── roles/\n│   └── nginx/\n│       ├── tasks/\n│       ├── handlers/\n│       └── templates/\n└── playbooks/\n    └── deploy.yml\n</code></pre>","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"ansible/ansible/#6-quick-command-reference","level":2,"title":"6. Quick Command Reference","text":"<pre><code># Test connectivity\nansible all -m ping\n\n# Run a playbook with custom inventory\nansible-playbook -i inventory/custom.ini playbook.yml\n\n# Dry-run (check changes)\nansible-playbook playbook.yml --check\n\n# Limit to specific hosts\nansible-playbook playbook.yml --limit webservers\n\n# Use vault\nansible-playbook playbook.yml --vault-id @prompt\n</code></pre>","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"ansible/ansible/#7-best-practices","level":2,"title":"7. Best Practices","text":"<ol> <li>Use <code>ansible-lint</code> to validate playbooks.</li> <li>Version control all Ansible content.</li> <li>Use <code>--check</code> mode before applying changes.</li> <li>Document variables and roles with <code>README.md</code>.</li> </ol> <p>By following this guide, you’ll streamline your Ansible workflows, avoid common pitfalls, and create reusable, maintainable automation code!</p>","path":["Ansible","Introduction to Ansible"],"tags":[]},{"location":"docker/docker/","level":1,"title":"Docker: From Basics to Advanced","text":"What is Docker? <p>Docker is an open-source platform designed to simplify application deployment. It ensures that applications run reliably regardless of the environment. It allows developers to build, ship, and run applications in isolated environments called containers. Containers package applications with their libraries, dependencies, and configurations, ensuring consistency across development, testing, and production.</p> Why Use Docker? <ul> <li>Consistency: Applications behave the same in development, testing, and production.</li> <li>Lightweight: Containers share the host OS kernel, making them more efficient than virtual machines.</li> <li>Portable: Run applications anywhere, from local machines to cloud platforms.</li> </ul> Getting Started with Docker Installation <p>1. Install Docker on Linux</p> <pre><code># Update the package index\nsudo apt update\n\n# Install required packages\nsudo apt install apt-transport-https ca-certificates curl software-properties-common -y\n\n# Add Docker's official GPG key\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n\n# Add the Docker repository\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n\n# Install Docker Engine\nsudo apt update\nsudo apt install docker-ce docker-ce-cli containerd.io -y\n\n# Verify the installation\ndocker --version\n</code></pre> <p>2. Install Docker on macOS</p> Prerequisites for macOS <ul> <li>Requires macOS 10.14 or newer</li> <li>Ensure adequate resources are available for Docker Desktop</li> </ul> <pre><code># Download Docker Desktop for Mac from:\nhttps://www.docker.com/products/docker-desktop/\n\n# Install Docker by dragging it to the Applications folder\n# Open Docker Desktop and follow setup instructions\n</code></pre> <p>3. Install Docker on Windows</p> Supported Windows Versions <p>Docker Desktop supports Windows 10 64-bit (Professional, Enterprise, Education) and Windows 11</p> <p>Steps:</p> <ol> <li>Download Docker Desktop from Docker's official website.</li> <li>Run the installer and follow on-screen instructions.</li> <li>After installation, verify with:</li> </ol> <pre><code>docker --version\n</code></pre> <p>Useful Tips</p> <p>Enable Non-Root Docker Access on Linux</p> <p>Add your user to the Docker group to run Docker without <code>sudo</code>:</p> <pre><code>sudo usermod -aG docker $USER\n</code></pre> <p>Log out and log back in for changes to take effect.</p> <p>Docker Resource Limits</p> <p>Containers share host resources. Ensure sufficient CPU and memory allocation for performance.</p> <p>Docker Desktop vs Docker Engine</p> <ul> <li>Docker Desktop: Includes GUI tools, ideal for macOS and Windows.</li> <li>Docker Engine: Lightweight CLI-based version for Linux servers.</li> </ul> <p>Verify Your Docker Installation</p> <p>Run a test container:</p> <pre><code>docker run hello-world\n</code></pre> <p>Expected output:</p> <pre><code>Hello from Docker!\nThis message shows that your installation appears to be working correctly.\n</code></pre>","path":["Docker","Docker: From Basics to Advanced"],"tags":[]},{"location":"docker/docker/#docker-command-reference","level":2,"title":"Docker Command Reference","text":"","path":["Docker","Docker: From Basics to Advanced"],"tags":[]},{"location":"docker/docker/#basic-commands","level":2,"title":"Basic Commands","text":"<ul> <li>Check Docker Version</li> </ul> <pre><code>docker --version\n</code></pre> <ul> <li>List Docker Images</li> </ul> <pre><code>docker images\n</code></pre> <ul> <li>List Running Containers</li> </ul> <pre><code>docker ps\n</code></pre> <ul> <li>List All Containers</li> </ul> <pre><code>docker ps -a\n</code></pre> <ul> <li>Run a Container</li> </ul> <pre><code>docker run [OPTIONS] IMAGE [COMMAND] [ARG...]\n</code></pre> <ul> <li>Stop a Running Container</li> </ul> <pre><code>docker stop CONTAINER_ID\n</code></pre> <ul> <li>Remove a Container</li> </ul> <pre><code>docker rm CONTAINER_ID\n</code></pre> <ul> <li>Remove an Image</li> </ul> <pre><code>docker rmi IMAGE_ID\n</code></pre>","path":["Docker","Docker: From Basics to Advanced"],"tags":[]},{"location":"docker/docker/#building-docker-images","level":2,"title":"Building Docker Images","text":"<ul> <li>Build an Image from a Dockerfile</li> </ul> <pre><code>docker build -t IMAGE_NAME:TAG PATH\n</code></pre> <ul> <li>Build Without Cache</li> </ul> <pre><code>docker build --no-cache -t IMAGE_NAME:TAG PATH\n</code></pre> <ul> <li>View Build History</li> </ul> <pre><code>docker history IMAGE_NAME:TAG\n</code></pre>","path":["Docker","Docker: From Basics to Advanced"],"tags":[]},{"location":"docker/docker/#dockerfile-basics","level":2,"title":"Dockerfile Basics","text":"<p>Example:</p> <pre><code># Use a base image\nFROM ubuntu:latest\n\n# Set environment variables\nENV MY_ENV_VAR=value\n\n# Install dependencies\nRUN apt-get update &amp;&amp; apt-get install -y curl\n\n# Copy files\nCOPY ./local_file /container_file\n\n# Define default command\nCMD [\"echo\", \"Hello, Docker!\"]\n</code></pre>","path":["Docker","Docker: From Basics to Advanced"],"tags":[]},{"location":"docker/docker/#networking-and-volumes","level":2,"title":"Networking and Volumes","text":"<ul> <li>Create a Network</li> </ul> <pre><code>docker network create my_network\n</code></pre> <ul> <li>Run a Container on a Network</li> </ul> <pre><code>docker run --network my_network IMAGE_NAME\n</code></pre> <ul> <li>Create a Volume</li> </ul> <pre><code>docker volume create my_volume\n</code></pre> <ul> <li>Run with a Volume</li> </ul> <pre><code>docker run -v my_volume:/container_path IMAGE_NAME\n</code></pre>","path":["Docker","Docker: From Basics to Advanced"],"tags":[]},{"location":"docker/docker/#docker-compose","level":2,"title":"Docker Compose","text":"<ul> <li>Start Services</li> </ul> <pre><code>docker-compose up\n</code></pre> <ul> <li>Detached Mode</li> </ul> <pre><code>docker-compose up -d\n</code></pre> <ul> <li>Stop Services</li> </ul> <pre><code>docker-compose down\n</code></pre> <ul> <li>View Logs</li> </ul> <pre><code>docker-compose logs\n</code></pre>","path":["Docker","Docker: From Basics to Advanced"],"tags":[]},{"location":"docker/docker/#registry-operations","level":2,"title":"Registry Operations","text":"<ul> <li>Push to Docker Hub</li> </ul> <pre><code>docker push IMAGE_NAME:TAG\n</code></pre> <ul> <li>Pull from Docker Hub</li> </ul> <pre><code>docker pull IMAGE_NAME:TAG\n</code></pre> <ul> <li>Tag an Image</li> </ul> <pre><code>docker tag SOURCE_IMAGE:TAG TARGET_IMAGE:TAG\n</code></pre> <ul> <li>Login</li> </ul> <pre><code>docker login\n</code></pre>","path":["Docker","Docker: From Basics to Advanced"],"tags":[]},{"location":"docker/docker/#troubleshooting-and-scenarios","level":2,"title":"Troubleshooting and Scenarios","text":"","path":["Docker","Docker: From Basics to Advanced"],"tags":[]},{"location":"docker/docker/#scenario-1-container-not-starting","level":2,"title":"Scenario 1: Container Not Starting","text":"<ul> <li>Symptom: Container exits immediately</li> <li>Solution: Check logs</li> </ul> <pre><code>docker logs CONTAINER_ID\n</code></pre> <p>Ensure command is valid and not terminating.</p> Debug Tip <p>Use <code>-it</code> to keep the container interactive during debugging.</p>","path":["Docker","Docker: From Basics to Advanced"],"tags":[]},{"location":"docker/docker/#scenario-2-image-build-fails","level":2,"title":"Scenario 2: Image Build Fails","text":"<ul> <li>Symptom: <code>docker build</code> fails</li> <li> <p>Solution:</p> </li> <li> <p>Review error messages</p> </li> <li>Ensure <code>Dockerfile</code> syntax is correct</li> <li> <p>Use no-cache build:</p> <pre><code>docker build --no-cache -t IMAGE_NAME:TAG .\n</code></pre> </li> </ul> Cache Considerations <p>Cached layers may cause outdated dependencies. Use <code>--no-cache</code> to ensure fresh builds.</p>","path":["Docker","Docker: From Basics to Advanced"],"tags":[]},{"location":"docker/docker/#scenario-3-port-already-in-use","level":2,"title":"Scenario 3: Port Already in Use","text":"<ul> <li>Symptom: Port conflict error</li> <li> <p>Solution:</p> </li> <li> <p>Identify process:</p> <pre><code>lsof -i :PORT\n</code></pre> </li> <li> <p>Stop conflicting process or map a new port:</p> <pre><code>docker run -p NEW_PORT:CONTAINER_PORT IMAGE_NAME\n</code></pre> </li> </ul> Port Conflict Resolution <p>Use <code>docker ps</code> to review port mappings of running containers.</p>","path":["Docker","Docker: From Basics to Advanced"],"tags":[]},{"location":"docker/docker/#scenario-4-cannot-remove-containerimage","level":2,"title":"Scenario 4: Cannot Remove Container/Image","text":"<ul> <li>Symptom: Removal fails with \"in use\" error</li> <li> <p>Solution:</p> </li> <li> <p>Stop all containers:</p> <pre><code>docker stop $(docker ps -q)\n</code></pre> </li> <li> <p>Force remove:</p> <pre><code>docker rm -f CONTAINER_ID\ndocker rmi -f IMAGE_ID\n</code></pre> </li> </ul> Forced Removal <p>The <code>-f</code> flag forces removal even when containers or images are in use.</p>","path":["Docker","Docker: From Basics to Advanced"],"tags":[]},{"location":"jenkins/jenkins/","level":1,"title":"Jenkins Installation and First Pipeline Setup on Ubuntu","text":"Mastering Jenkins: Installing on Ubuntu and Creating Your First Pipeline <p>Jenkins is an open-source automation server that helps with continuous integration and continuous delivery (CI/CD). This guide will walk you through the process of installing Jenkins on Ubuntu, checking system details using a Bash script, and creating your first pipeline using Jenkins' powerful pipeline functionality.</p>","path":["Jenkins","Jenkins Installation and First Pipeline Setup on Ubuntu"],"tags":[]},{"location":"jenkins/jenkins/#step-1-installing-jenkins-on-ubuntu","level":2,"title":"Step 1: Installing Jenkins on Ubuntu","text":"<p>Before we begin, ensure your system is up-to-date.</p>","path":["Jenkins","Jenkins Installation and First Pipeline Setup on Ubuntu"],"tags":[]},{"location":"jenkins/jenkins/#11-update-system-packages","level":2,"title":"1.1 Update System Packages","text":"<p>First, update your system’s package list to ensure everything is up to date:</p> <pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y\n</code></pre>","path":["Jenkins","Jenkins Installation and First Pipeline Setup on Ubuntu"],"tags":[]},{"location":"jenkins/jenkins/#12-install-java","level":2,"title":"1.2 Install Java","text":"<p>Jenkins requires Java to run. Install the default Java Development Kit (JDK):</p> <pre><code>sudo apt install openjdk-11-jdk -y\n</code></pre> <p>To verify the installation, use:</p> <pre><code>java -version\n</code></pre>","path":["Jenkins","Jenkins Installation and First Pipeline Setup on Ubuntu"],"tags":[]},{"location":"jenkins/jenkins/#13-add-jenkins-repository","level":2,"title":"1.3 Add Jenkins Repository","text":"<p>To install Jenkins, add its official repository to your system:</p> <pre><code>wget -q -O - https://pkg.jenkins.io/jenkins.io.key | sudo tee /etc/apt/trusted.gpg.d/jenkins.asc\n</code></pre> <p>Next, add the Jenkins repository:</p> <pre><code>sudo sh -c 'echo deb http://pkg.jenkins.io/debian/ stable main &gt; /etc/apt/sources.list.d/jenkins.list'\n</code></pre>","path":["Jenkins","Jenkins Installation and First Pipeline Setup on Ubuntu"],"tags":[]},{"location":"jenkins/jenkins/#14-install-jenkins","level":2,"title":"1.4 Install Jenkins","text":"<p>Once the repository is added, update the package list and install Jenkins:</p> <pre><code>sudo apt update\nsudo apt install jenkins -y\n</code></pre>","path":["Jenkins","Jenkins Installation and First Pipeline Setup on Ubuntu"],"tags":[]},{"location":"jenkins/jenkins/#15-start-jenkins","level":2,"title":"1.5 Start Jenkins","text":"<p>Enable and start the Jenkins service:</p> <pre><code>sudo systemctl enable jenkins\nsudo systemctl start jenkins\n</code></pre> <p>You can check the status of Jenkins:</p> <pre><code>sudo systemctl status jenkins\n</code></pre>","path":["Jenkins","Jenkins Installation and First Pipeline Setup on Ubuntu"],"tags":[]},{"location":"jenkins/jenkins/#step-2-accessing-jenkins","level":2,"title":"Step 2: Accessing Jenkins","text":"<p>Jenkins will be running on port 8080 by default. Open your browser and navigate to:</p> <pre><code>http://localhost:8080\n</code></pre>","path":["Jenkins","Jenkins Installation and First Pipeline Setup on Ubuntu"],"tags":[]},{"location":"jenkins/jenkins/#21-unlock-jenkins","level":2,"title":"2.1 Unlock Jenkins","text":"<p>To unlock Jenkins, you will need the <code>initialAdminPassword</code>. Retrieve it with the following command:</p> <pre><code>sudo cat /var/lib/jenkins/secrets/initialAdminPassword\n</code></pre> <p>Enter the password in the browser prompt.</p>","path":["Jenkins","Jenkins Installation and First Pipeline Setup on Ubuntu"],"tags":[]},{"location":"jenkins/jenkins/#22-install-suggested-plugins","level":2,"title":"2.2 Install Suggested Plugins","text":"<p>Once you’ve unlocked Jenkins, you'll be prompted to install plugins. Choose the \"Install suggested plugins\" option to proceed with the default plugin installation.</p>","path":["Jenkins","Jenkins Installation and First Pipeline Setup on Ubuntu"],"tags":[]},{"location":"jenkins/jenkins/#step-3-creating-your-first-jenkins-pipeline","level":2,"title":"Step 3: Creating Your First Jenkins Pipeline","text":"","path":["Jenkins","Jenkins Installation and First Pipeline Setup on Ubuntu"],"tags":[]},{"location":"jenkins/jenkins/#31-create-a-new-pipeline-project","level":2,"title":"3.1 Create a New Pipeline Project","text":"<ol> <li>After logging into Jenkins, click on New Item.</li> <li>Enter a name for your pipeline (e.g., \"First-Pipeline\").</li> <li>Select Pipeline and click OK.</li> </ol>","path":["Jenkins","Jenkins Installation and First Pipeline Setup on Ubuntu"],"tags":[]},{"location":"jenkins/jenkins/#32-configure-the-pipeline","level":2,"title":"3.2 Configure the Pipeline","text":"<p>In the pipeline configuration page, scroll down to the Pipeline section. Here, you'll define your pipeline script. For this guide, we’ll use a simple pipeline that checks system details using a Bash script.</p> <pre><code>pipeline {\n    agent any\n\n    stages {\n        stage('Check System Details') {\n            steps {\n                script {\n                    sh 'bash check_system_details.sh'\n                }\n            }\n        }\n    }\n}\n</code></pre>","path":["Jenkins","Jenkins Installation and First Pipeline Setup on Ubuntu"],"tags":[]},{"location":"jenkins/jenkins/#33-create-a-bash-script","level":2,"title":"3.3 Create a Bash Script","text":"<p>In your Jenkins workspace, create a script called <code>check_system_details.sh</code>. This script will gather system information.</p> <pre><code>#!/bin/bash\n\necho \"System Information:\"\necho \"--------------------\"\nhostnamectl\necho\ndf -h\necho\nfree -h\necho\nuname -a\n</code></pre> <p>Ensure that the script has executable permissions:</p> <pre><code>chmod +x check_system_details.sh\n</code></pre>","path":["Jenkins","Jenkins Installation and First Pipeline Setup on Ubuntu"],"tags":[]},{"location":"jenkins/jenkins/#34-run-the-pipeline","level":2,"title":"3.4 Run the Pipeline","text":"<p>Save the pipeline and click Build Now to execute the pipeline. Jenkins will run the <code>check_system_details.sh</code> script, and you should see the system details in the build log.</p>","path":["Jenkins","Jenkins Installation and First Pipeline Setup on Ubuntu"],"tags":[]},{"location":"kubernetes/kubernetes/","level":1,"title":"Kubernetes: From Basics to Advanced","text":"","path":["Kubernetes","Kubernetes: From Basics to Advanced"],"tags":[]},{"location":"kubernetes/kubernetes/#why-container-orchestration-and-need-for-containers","level":2,"title":"Why Container Orchestration and Need for Containers","text":"<p>Before containers, applications were deployed on physical or virtual machines, which posed challenges:</p> <ul> <li>Dependency Conflicts: Applications required specific library versions, causing conflicts on shared machines.</li> <li>Environment Inconsistency: Differences between development, testing, and production environments led to issues like \"it works on my machine.\"</li> <li>Resource Inefficiency: Virtual machines included full operating systems, consuming significant resources.</li> </ul> <p>How Containers Solve This</p> <p>Containers package applications with their dependencies, ensuring consistency across environments.  They are lightweight, sharing the host OS kernel, unlike virtual machines.</p>","path":["Kubernetes","Kubernetes: From Basics to Advanced"],"tags":[]},{"location":"kubernetes/kubernetes/#the-rise-of-container-orchestration","level":2,"title":"The Rise of Container Orchestration","text":"<p>As container usage scaled, manual management became inefficient. Key needs included:</p> <ul> <li>Deploying containers across multiple machines.</li> <li>Scaling applications based on demand.</li> <li>Ensuring high availability and handling failures.</li> <li>Managing networking and storage.</li> </ul> <p>Why Orchestration Tools?</p> <p>Container orchestration tools automate deployment, scaling, and resource management, improving reliability and efficiency.</p>","path":["Kubernetes","Kubernetes: From Basics to Advanced"],"tags":[]},{"location":"kubernetes/kubernetes/#why-kubernetes","level":2,"title":"Why Kubernetes?","text":"<p>Kubernetes, originally developed by Google, became the standard for container orchestration due to:</p> <ul> <li>Scalability: Manages thousands of containers across clusters.</li> <li>Portability: Runs on-premises, in the cloud, or in hybrid setups.</li> <li>Ecosystem: Supported by a vast community and toolset.</li> <li>Flexibility: Handles diverse workloads, from stateless apps to stateful databases.</li> </ul> <p>Key Features</p> <p>Kubernetes adoption is driven by features like auto-scaling, self-healing, and service discovery.</p>","path":["Kubernetes","Kubernetes: From Basics to Advanced"],"tags":[]},{"location":"kubernetes/kubernetes/#understanding-oci-and-runc","level":2,"title":"Understanding OCI and runc","text":"","path":["Kubernetes","Kubernetes: From Basics to Advanced"],"tags":[]},{"location":"kubernetes/kubernetes/#open-container-initiative-oci","level":2,"title":"Open Container Initiative (OCI)","text":"<p>OCI Specifications</p> <p>The Open Container Initiative (OCI), under the Linux Foundation, defines standards for container formats and runtimes to ensure interoperability.</p> <ul> <li>Container Image Specification: Defines image structure, layers, and metadata.</li> <li>Runtime Specification: Defines how runtimes create and manage containers.</li> </ul>","path":["Kubernetes","Kubernetes: From Basics to Advanced"],"tags":[]},{"location":"kubernetes/kubernetes/#runc","level":2,"title":"runc","text":"<p>About runc</p> <p>runc is a lightweight, CLI-based container runtime implementing the OCI runtime specification.</p> <ul> <li>Creates containers using Linux namespaces and cgroups.</li> <li>Executes processes in isolated environments.</li> <li>Foundation for higher-level tools like Docker and containerd.</li> </ul> <p>Creating a Container with runc (Red Hat and Ubuntu)</p> <p>1. Install prerequisites  On Red Hat: <code>bash     sudo yum install -y runc</code></p> <pre><code>On **Ubuntu/Debian**:\n```bash\nsudo apt-get update\nsudo apt-get install -y runc\n```\n\n**2. Create root filesystem (using BusyBox for demo)**\n```bash\nmkdir rootfs\ndocker export $(docker create busybox) | tar -C rootfs -xvf -\n```\n\n**3. Generate default config**\n```bash\nrunc spec\n```\n\n**4. Start a container**\n```bash\nsudo runc run mycontainer\n```\n\n**5. Install a package inside the container**\nFor **Red Hat-based container**:\n```bash\nyum install -y vim\n```\nFor **Ubuntu-based container**:\n```bash\napt-get update\napt-get install -y vim\n```\n\n**6. Check resource usage of the container**\nFrom host system:\n```bash\nrunc list\nrunc state mycontainer\n```\n\nUsing Linux tools:\n```bash\ntop\nhtop\nfree -m\n```\n\nThis workflow demonstrates creating, running, and managing a container directly with `runc`, without Docker or higher-level tools.\n</code></pre>","path":["Kubernetes","Kubernetes: From Basics to Advanced"],"tags":[]},{"location":"kubernetes/kubernetes/#linux-kernel-features-namespaces-and-cgroups","level":2,"title":"Linux Kernel Features: Namespaces and cgroups","text":"","path":["Kubernetes","Kubernetes: From Basics to Advanced"],"tags":[]},{"location":"kubernetes/kubernetes/#namespaces","level":2,"title":"Namespaces","text":"<p>Namespaces provide isolation for containers, giving each its own:</p> <ul> <li>PID Namespace: Isolated process IDs.</li> <li>Network Namespace: Separate network stack.</li> <li>Mount Namespace: Isolated filesystem.</li> <li>User Namespace: Isolated user and group IDs.</li> </ul>","path":["Kubernetes","Kubernetes: From Basics to Advanced"],"tags":[]},{"location":"kubernetes/kubernetes/#cgroups-control-groups","level":2,"title":"cgroups (Control Groups)","text":"<p>cgroups control resource usage:</p> <ul> <li>CPU: E.g., 50% of a core.</li> <li>Memory: E.g., 1 GB.</li> <li>I/O: Disk bandwidth.</li> </ul> <p>Key Takeaway</p> <p>Containers use namespaces for isolation and cgroups for resource control.  Tools like Docker, containerd, and Kubernetes build on these features.</p>","path":["Kubernetes","Kubernetes: From Basics to Advanced"],"tags":[]},{"location":"kubernetes/kubernetes/#installing-local-kubernetes-environments","level":2,"title":"Installing Local Kubernetes Environments","text":"","path":["Kubernetes","Kubernetes: From Basics to Advanced"],"tags":[]},{"location":"kubernetes/kubernetes/#minikube","level":2,"title":"Minikube","text":"<p>About Minikube</p> <p>Minikube runs a single-node Kubernetes cluster locally, ideal for learning and development.</p> <p>Prerequisites</p> <ul> <li>CPU: 2+ CPUs</li> <li>Memory: 2 GB (4 GB recommended)</li> <li>Disk Space: 20 GB free</li> <li>OS: Linux, macOS, or Windows</li> <li>Virtualization: VT-x/AMD-V</li> <li>Tools: Docker or hypervisor (VirtualBox, Hyper-V), kubectl</li> </ul> <p>Installation Steps</p> <pre><code># Linux: Install kubectl\ncurl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nchmod +x kubectl\nsudo mv kubectl /usr/local/bin/\n\n# macOS\nbrew install kubectl\n</code></pre> <p>For Windows, download from https://dl.k8s.io/release/stable.txt and add to PATH.</p> <pre><code># Install Minikube\ncurl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\nsudo install minikube-linux-amd64 /usr/local/bin/minikube\n\n# macOS\nbrew install minikube\n</code></pre> <p>For Windows, download from https://minikube.sigs.k8s.io/docs/start/.</p> <pre><code># Start and Verify\nminikube start --driver=docker\nkubectl get nodes\n</code></pre> <p>Access dashboard:</p> <pre><code>minikube dashboard\n</code></pre> <p>Usage Notes</p> <p>Minikube is for development only. Use <code>minikube stop</code> or <code>minikube delete</code> for cleanup.</p>","path":["Kubernetes","Kubernetes: From Basics to Advanced"],"tags":[]},{"location":"kubernetes/kubernetes/#rancher-desktop","level":2,"title":"Rancher Desktop","text":"<p>About Rancher Desktop</p> <p>Rancher Desktop provides a lightweight Kubernetes cluster (k3s) with a GUI.</p> <p>Prerequisites</p> <ul> <li>CPU: 4+ CPUs</li> <li>Memory: 8 GB</li> <li>OS: Linux, macOS, or Windows</li> <li>Virtualization: QEMU (Linux/macOS), WSL2 (Windows)</li> </ul> <p>Steps</p> <ol> <li>Download from rancherdesktop.io.</li> </ol> <pre><code>sudo apt install ./rancher-desktop-&lt;version&gt;.deb\n</code></pre> <ol> <li>Enable Kubernetes in Preferences &gt; Kubernetes.</li> <li>Choose runtime: containerd or dockerd.</li> <li>Verify with:</li> </ol> <pre><code>kubectl get namespaces\n</code></pre> <p>Highlights</p> <p>- Uses k3s for efficiency. - Supports containerd and dockerd.</p>","path":["Kubernetes","Kubernetes: From Basics to Advanced"],"tags":[]},{"location":"kubernetes/kubernetes/#docker-desktop","level":2,"title":"Docker Desktop","text":"<p>About Docker Desktop</p> <p>Docker Desktop integrates Kubernetes for local clusters.</p> <p>Prerequisites</p> <ul> <li>OS: Windows 10/11 (Pro/Enterprise), macOS</li> <li>Virtualization: WSL2 (Windows), HyperKit (macOS)</li> <li>Memory: 4 GB (8 GB recommended)</li> </ul> <p>Steps</p> <ol> <li>Download from docker.com and install.</li> <li>Enable Kubernetes: Settings &gt; Kubernetes &gt; Enable Kubernetes.</li> <li>Verify:</li> </ol> <pre><code>kubectl cluster-info\n</code></pre> <ol> <li>Deploy Kubernetes Dashboard:</li> </ol> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml\nkubectl proxy\n</code></pre> <p>Access at: http://localhost:8001</p> <p>Usage Notes</p> <p>- May require a paid license for large organizations. - Uses containerd as runtime.</p>","path":["Kubernetes","Kubernetes: From Basics to Advanced"],"tags":[]},{"location":"kubernetes/kubernetes/#hands-on-with-online-kubernetes-tools","level":2,"title":"Hands-On with Online Kubernetes Tools","text":"<ul> <li>Katacoda: Free browser-based labs (https://www.katacoda.com/courses/kubernetes).</li> <li>Play with Kubernetes: Temporary clusters (https://labs.play-with-k8s.com/).</li> </ul>","path":["Kubernetes","Kubernetes: From Basics to Advanced"],"tags":[]},{"location":"kubernetes/kubernetes/#cloud-based-kubernetes-services","level":2,"title":"Cloud-Based Kubernetes Services","text":"","path":["Kubernetes","Kubernetes: From Basics to Advanced"],"tags":[]},{"location":"kubernetes/kubernetes/#aws-elastic-kubernetes-service-eks","level":2,"title":"AWS Elastic Kubernetes Service (EKS)","text":"<p>About EKS</p> <p>A managed Kubernetes service integrated with AWS.</p> <p>Setup</p> <pre><code># Configure AWS CLI\naws configure\n\n# Install eksctl\ncurl --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp\nsudo mv /tmp/eksctl /usr/local/bin\n\n# Create cluster\neksctl create cluster --name my-cluster --region us-west-2 --nodegroup-name my-nodes --node-type t3.medium --nodes 2\n\n# Verify\nkubectl get nodes\n</code></pre> <p>Features</p> <p>- Integrates with AWS services (ELB, IAM, CloudWatch). - Supports auto-scaling and HA.</p>","path":["Kubernetes","Kubernetes: From Basics to Advanced"],"tags":[]},{"location":"kubernetes/kubernetes/#red-hat-openshift","level":2,"title":"Red Hat OpenShift","text":"<p>About OpenShift</p> <p>Kubernetes-based platform with developer tools.</p> <p>Setup</p> <ul> <li>Use Red Hat Developer Sandbox.</li> </ul> <p>Features</p> <ul> <li>CI/CD pipelines.</li> <li>Developer UI and CLI (<code>oc</code>).</li> <li>Enhanced security.</li> </ul>","path":["Kubernetes","Kubernetes: From Basics to Advanced"],"tags":[]},{"location":"kubernetes/kubernetes/#other-providers","level":2,"title":"Other Providers","text":"<ul> <li>Google Kubernetes Engine (GKE): GCP integration.</li> <li>Azure Kubernetes Service (AKS): Part of Azure ecosystem.</li> <li>IBM Cloud Kubernetes Service: Security-focused.</li> </ul>","path":["Kubernetes","Kubernetes: From Basics to Advanced"],"tags":[]},{"location":"kubernetes/kubernetes/#example-running-a-container-with-containerd","level":2,"title":"Example: Running a Container with containerd","text":"<pre><code>sudo yum install -y containerd\nsudo systemctl start containerd\nsudo systemctl enable containerd\n\n# Configure\nsudo mkdir -p /etc/containerd\ncontainerd config default | sudo tee /etc/containerd/config.toml\nsudo systemctl restart containerd\n\n# Pull and run Alpine\nsudo ctr image pull docker.io/library/alpine:latest\nexport CTR_NAMESPACE=my-ns\nsudo ctr --namespace $CTR_NAMESPACE run -t --rm docker.io/library/alpine:latest my-container sh\n\n# List namespaces\nsudo ctr namespaces list\n</code></pre>","path":["Kubernetes","Kubernetes: From Basics to Advanced"],"tags":[]},{"location":"kubernetes/kubernetes/#advanced-kubernetes-concepts","level":2,"title":"Advanced Kubernetes Concepts","text":"","path":["Kubernetes","Kubernetes: From Basics to Advanced"],"tags":[]},{"location":"kubernetes/kubernetes/#deployments-and-services","level":2,"title":"Deployments and Services","text":"<pre><code>kubectl create deployment my-app --image=nginx:latest --replicas=3\nkubectl expose deployment my-app --type=NodePort --port=80\n</code></pre>","path":["Kubernetes","Kubernetes: From Basics to Advanced"],"tags":[]},{"location":"kubernetes/kubernetes/#ingress","level":2,"title":"Ingress","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-ingress\nspec:\n  rules:\n    - host: myapp.local\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: my-app\n                port:\n                  number: 80\n</code></pre>","path":["Kubernetes","Kubernetes: From Basics to Advanced"],"tags":[]},{"location":"kubernetes/kubernetes/#configmaps-and-secrets","level":2,"title":"ConfigMaps and Secrets","text":"<pre><code>kubectl create configmap my-config --from-literal=key1=value1\nkubectl create secret generic my-secret --from-literal=password=secure123\n</code></pre>","path":["Kubernetes","Kubernetes: From Basics to Advanced"],"tags":[]},{"location":"kubernetes/kubernetes/#persistent-volumes-pv-and-persistent-volume-claims-pvc","level":2,"title":"Persistent Volumes (PV) and Persistent Volume Claims (PVC)","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: my-pv\nspec:\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: /mnt/data\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n</code></pre>","path":["Kubernetes","Kubernetes: From Basics to Advanced"],"tags":[]},{"location":"kubernetes/kubernetes/#helm-charts","level":2,"title":"Helm Charts","text":"<pre><code># Install Helm\ncurl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash\n\n# Deploy a chart\nhelm install my-release nginx-stable/nginx-ingress\n</code></pre>","path":["Kubernetes","Kubernetes: From Basics to Advanced"],"tags":[]},{"location":"kubernetes/kubernetes/#conclusion","level":2,"title":"Conclusion","text":"<p>Kubernetes enables scalable, portable container orchestration. Local tools like Minikube, Rancher Desktop, and Docker Desktop are great for learning, while AWS EKS and OpenShift provide production-grade solutions.</p> <p>By leveraging Linux namespaces, cgroups, containerd, and Helm, Kubernetes simplifies cloud-native application development.</p> <p>Further Learning</p> <ul> <li>Kubernetes Docs</li> <li>Rancher Desktop</li> <li>AWS EKS</li> <li>OpenShift Sandbox</li> </ul>","path":["Kubernetes","Kubernetes: From Basics to Advanced"],"tags":[]},{"location":"monitoring-tools/log/","level":1,"title":"Step 1: Install and Run Containers","text":"<p>Monitoring is a crucial part of DevOps, helping teams detect issues before they become critical. In this guide, we will install Prometheus, Grafana, and Node Exporter using Podman, an alternative to Docker. By the end, you'll have a fully functional monitoring stack with visual dashboards to analyze system metrics.</p>","path":["Monitoring tools","Step 1: Install and Run Containers"],"tags":[]},{"location":"monitoring-tools/log/#what-are-prometheus-grafana-and-node-exporter","level":2,"title":"What Are Prometheus, Grafana, and Node Exporter?","text":"","path":["Monitoring tools","Step 1: Install and Run Containers"],"tags":[]},{"location":"monitoring-tools/log/#1-prometheus-the-time-series-database","level":2,"title":"1. Prometheus: The Time-Series Database","text":"<p>Prometheus is an open-source monitoring system that collects and stores metrics as time-series data. It is widely used in DevOps due to its powerful querying language (PromQL) and easy integration with multiple exporters.</p> <p>📌 Key Features:</p> <ul> <li>Time-series data storage</li> <li>Pull-based metric collection</li> <li>Alerting and rule-based evaluations</li> </ul>","path":["Monitoring tools","Step 1: Install and Run Containers"],"tags":[]},{"location":"monitoring-tools/log/#2-node-exporter-the-system-metrics-collector","level":2,"title":"2. Node Exporter: The System Metrics Collector","text":"<p>Node Exporter is a lightweight agent that runs on a machine to collect system metrics such as CPU usage, memory utilization, disk I/O, and network statistics.</p> <p>📌 Common Metrics Collected:</p> <ul> <li>CPU Load: <code>node_cpu_seconds_total</code></li> <li>Memory Usage: <code>node_memory_MemAvailable_bytes</code></li> <li>Disk Space: <code>node_filesystem_avail_bytes</code></li> <li>Network Traffic: <code>node_network_receive_bytes_total</code></li> </ul>","path":["Monitoring tools","Step 1: Install and Run Containers"],"tags":[]},{"location":"monitoring-tools/log/#3-grafana-the-visualization-tool","level":2,"title":"3. Grafana: The Visualization Tool","text":"<p>Grafana is an open-source tool that helps visualize and analyze time-series data. It allows you to create dashboards with real-time graphs, alerts, and reports.</p> <p>📌 Why Use Grafana?</p> <ul> <li>Connects to multiple data sources (Prometheus, MySQL, AWS CloudWatch, etc.)</li> <li>Beautiful and interactive dashboards</li> <li>Custom alerts and notifications</li> </ul>","path":["Monitoring tools","Step 1: Install and Run Containers"],"tags":[]},{"location":"monitoring-tools/log/#step-1-install-and-run-containers","level":1,"title":"Step 1: Install and Run Containers","text":"<p>We will use Podman, a rootless container runtime similar to Docker, to set up our monitoring stack.</p>","path":["Monitoring tools","Step 1: Install and Run Containers"],"tags":[]},{"location":"monitoring-tools/log/#11-pull-and-run-a-centos-container","level":2,"title":"1.1 Pull and Run a CentOS Container","text":"<p>We'll start by running a CentOS container where we will install Node Exporter.</p> <pre><code>podman run -dit --name centos_container -p 9100:9100 centos:latest\n</code></pre> <p>Verify that the container is running:</p> <pre><code>podman ps\n</code></pre>","path":["Monitoring tools","Step 1: Install and Run Containers"],"tags":[]},{"location":"monitoring-tools/log/#12-run-prometheus","level":2,"title":"1.2 Run Prometheus","text":"<p>Now, let's run Prometheus, which will scrape metrics from Node Exporter.</p> <pre><code>podman run -d --name prometheus -p 9090:9090 quay.io/prometheus/prometheus\n</code></pre> <p>Check if Prometheus is running properly:</p> <pre><code>podman logs prometheus\n</code></pre> <p>You can now access Prometheus at: 👉 http://localhost:9090/query</p> <p>To enter the Prometheus container:</p> <pre><code>podman exec -it prometheus /bin/sh\n</code></pre>","path":["Monitoring tools","Step 1: Install and Run Containers"],"tags":[]},{"location":"monitoring-tools/log/#step-2-install-and-run-node-exporter","level":1,"title":"Step 2: Install and Run Node Exporter","text":"<p>Log into the CentOS container:</p> <pre><code>podman exec -it centos_container /bin/bash\n</code></pre> <p>Download and extract Node Exporter:</p> <pre><code>wget https://github.com/prometheus/node_exporter/releases/download/v1.9.0/node_exporter-1.9.0.linux-386.tar.gz\ntar -xvf node_exporter-1.9.0.linux-386.tar.gz\ncd node_exporter-1.9.0.linux-386\n./node_exporter\n</code></pre> <p>Now, you can check the collected metrics at: 👉 http://localhost:9100/metrics</p>","path":["Monitoring tools","Step 1: Install and Run Containers"],"tags":[]},{"location":"monitoring-tools/log/#step-3-configure-prometheus-to-scrape-node-exporter-metrics","level":1,"title":"Step 3: Configure Prometheus to Scrape Node Exporter Metrics","text":"<p>Modify the Prometheus configuration file (<code>prometheus.yml</code>):</p> <pre><code>cd /etc/prometheus\nvim prometheus.yml\n</code></pre> <p>Add the following configuration:</p> <pre><code>global:\n  scrape_interval: 15s # Collect metrics every 15 seconds\n\nscrape_configs:\n  - job_name: \"prometheus\"\n    static_configs:\n      - targets: [\"localhost:9090\"]\n\n  - job_name: \"centos_node_exporter\"\n    static_configs:\n      - targets: [\"localhost:9100\"]\n</code></pre> <p>Restart Prometheus:</p> <pre><code>podman restart prometheus\n</code></pre> <p>Check if Prometheus is scraping Node Exporter: 👉 http://localhost:9090/targets</p>","path":["Monitoring tools","Step 1: Install and Run Containers"],"tags":[]},{"location":"monitoring-tools/log/#step-4-deploy-grafana","level":1,"title":"Step 4: Deploy Grafana","text":"<p>Run the Grafana container:</p> <pre><code>podman run -d --name grafana -p 3000:3000 grafana/grafana\n</code></pre> <p>Get the IP address of Prometheus:</p> <pre><code>podman inspect -f '{{ .NetworkSettings.IPAddress }}' prometheus\n</code></pre> <p>Access Grafana at: 👉 http://localhost:3000</p> <p>📌 Default Login:</p> <ul> <li>Username: <code>admin</code></li> <li>Password: <code>admin</code> (change it on first login)</li> </ul>","path":["Monitoring tools","Step 1: Install and Run Containers"],"tags":[]},{"location":"monitoring-tools/log/#step-5-add-prometheus-as-a-data-source-in-grafana","level":1,"title":"Step 5: Add Prometheus as a Data Source in Grafana","text":"<ol> <li>Open Grafana (<code>http://localhost:3000</code>).</li> <li>Go to Configuration &gt; Data Sources.</li> <li>Click \"Add Data Source\", select Prometheus.</li> <li>Set the URL as <code>http://prometheus:9090</code>.</li> <li>Click \"Save &amp; Test\".</li> </ol>","path":["Monitoring tools","Step 1: Install and Run Containers"],"tags":[]},{"location":"monitoring-tools/log/#step-6-import-node-exporter-dashboard","level":1,"title":"Step 6: Import Node Exporter Dashboard","text":"<ol> <li>In Grafana, go to \"Create\" &gt; \"Import\".</li> <li>Enter the Dashboard ID: <code>1860</code> (or search for \"Node Exporter Full\").</li> <li>Select Prometheus as the data source.</li> <li>Click \"Import\".</li> </ol> <p>You should now see a full system monitoring dashboard! 🎉</p>","path":["Monitoring tools","Step 1: Install and Run Containers"],"tags":[]},{"location":"monitoring-tools/log/#step-7-managing-containers","level":1,"title":"Step 7: Managing Containers","text":"<p>To stop and remove containers:</p> <pre><code>podman stop centos_container\npodman rm centos_container\n</code></pre> <p>Restart Prometheus and Grafana:</p> <pre><code>podman restart prometheus\npodman restart grafana\n</code></pre>","path":["Monitoring tools","Step 1: Install and Run Containers"],"tags":[]},{"location":"shell-scripts/scripts/","level":1,"title":"Shell Scripts for SRE and DevOps","text":"<p>Shell scripts are programs written for command-line interpreters (shells) that automate system tasks and operations. They combine sequences of commands into reusable scripts that can be executed as single units.</p> <p>They are widely used by SRE (Site Reliability Engineers) and DevOps engineers to automate operations, reduce manual effort, and standardize workflows.</p>","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#historical-context-and-evolution","level":2,"title":"Historical Context and Evolution","text":"<p>Shell scripting originated in the early Unix systems (1970s) with the Bourne shell (<code>sh</code>). Over time, more feature-rich shells emerged:</p> <ul> <li>Bash (Bourne Again Shell): Default on most Linux distributions</li> <li>Zsh: Extended features with better user interaction</li> <li>Ksh: AIX default shell with advanced scripting capabilities</li> </ul>","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#why-shell-scripts-are-important-for-sre-and-devops","level":2,"title":"Why Shell Scripts Are Important for SRE and DevOps","text":"<p>Tip</p> <p>Shell scripts remain a core tool for SRE and DevOps. - They are simple: use plain Linux commands. - They are powerful: integrate system utilities, APIs, and services. - They are universal: available by default on nearly every Unix/Linux system. - They allow conditions, loops, functions, and modularity to make tasks reusable.</p> <p>Examples of usage:</p> <ul> <li>Daily health checks</li> <li>Disk and memory monitoring</li> <li>Service management</li> <li>Deployment pipelines</li> <li>Cron-based automation</li> </ul>","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#shell-interpreter-and-location","level":2,"title":"Shell Interpreter and Location","text":"<ul> <li>The interpreter is the program that executes shell scripts.</li> <li>Common locations:</li> </ul> <pre><code>/bin/sh\n/bin/bash\n/usr/bin/zsh\n</code></pre> <ul> <li>Default interpreter: <code>/bin/bash</code> in most Linux systems.</li> <li>Speed: Shell scripts are slower than compiled languages (C, Go), but fast enough for automation.</li> </ul> <p>Note</p> <p>Always define interpreter at the top of scripts with shebang: <code>bash     #!/bin/bash</code></p>","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#core-components-of-shell-scripts","level":2,"title":"Core Components of Shell Scripts","text":"","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#shebang-directive","level":2,"title":"Shebang Directive","text":"<pre><code>#!/bin/bash\n#!/bin/sh     # POSIX-compliant\n#!/usr/bin/zsh\n#!/usr/bin/env bash  # Portable\n</code></pre>","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#comments-and-documentation","level":2,"title":"Comments and Documentation","text":"<pre><code># Single-line comment\n\n: '\nMulti-line comment\n'\n\n&lt;&lt;EOF\nHere-document style comment\nEOF\n</code></pre>","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#variables-and-data-types","level":2,"title":"Variables and Data Types","text":"<pre><code>NAME=\"value\"          # String\nCOUNT=42              # Integer\nFILES=(*.txt)         # Array\nreadonly CONST=100    # Constant\nexport GLOBAL_VAR     # Env variable\n</code></pre>","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#special-variables","level":2,"title":"Special Variables","text":"<pre><code>$0    # Script name\n$1    # First argument\n$#    # Number of args\n$@    # All arguments\n$?    # Exit status\n$$    # PID\n$!    # Background PID\n</code></pre>","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#how-to-write-shell-scripts","level":2,"title":"How to Write Shell Scripts","text":"","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#1-conditions","level":2,"title":"1. Conditions","text":"<pre><code>if [ -f /etc/passwd ]; then\n  echo \"File exists\"\nelse\n  echo \"File missing\"\nfi\n</code></pre>","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#2-loops","level":2,"title":"2. Loops","text":"<pre><code>for i in {1..5}; do\n  echo \"Iteration $i\"\ndone\n</code></pre>","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#3-functions","level":2,"title":"3. Functions","text":"<pre><code>greet() { echo \"Hello $1\"; }\ngreet \"Admin\"\n</code></pre>","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#4-case-switch","level":2,"title":"4. Case (Switch)","text":"<pre><code>case $1 in\n  start) echo \"Starting service\" ;;\n  stop)  echo \"Stopping service\" ;;\n  *)     echo \"Usage: $0 {start|stop}\" ;;\nesac\n</code></pre>","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#5-modular-scripts","level":2,"title":"5. Modular Scripts","text":"<pre><code>source ./utils.sh\nsay_hello \"DevOps\"\n</code></pre>","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#advanced-scripting-techniques","level":2,"title":"Advanced Scripting Techniques","text":"","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#parameter-expansion","level":2,"title":"Parameter Expansion","text":"<pre><code>${VAR:-default}    # Use default if unset\n${VAR:=default}    # Set default if unset\n${#VAR}            # Length\n${VAR#pattern}     # Remove prefix\n${VAR%pattern}     # Remove suffix\n</code></pre>","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#arrays-and-associative-arrays","level":2,"title":"Arrays and Associative Arrays","text":"<pre><code>FILES=(\"f1\" \"f2\")\ndeclare -A CONFIG\nCONFIG[\"host\"]=\"example.com\"\n</code></pre>","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#inputoutput-handling","level":2,"title":"Input/Output Handling","text":"<pre><code>read -p \"Enter: \" INPUT\ncat &lt;&lt;END &gt; file.txt\nmulti-line\nEND\n</code></pre>","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#error-handling-and-debugging","level":2,"title":"Error Handling and Debugging","text":"<pre><code>set -euo pipefail\ntrap 'echo \"Error at line $LINENO\"' ERR\nset -x   # debug on\nset +x   # debug off\n</code></pre>","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#benefits-of-shell-scripting","level":2,"title":"Benefits of Shell Scripting","text":"<ul> <li>Fast prototyping of automation.</li> <li>Integration with system tools (<code>systemctl</code>, <code>docker</code>, <code>kubectl</code>, <code>rsync</code>).</li> <li>Portability across Linux distributions.</li> <li>Low dependency: no need for extra runtimes.</li> </ul>","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#advanced-examples-for-production-environments","level":2,"title":"Advanced Examples for Production Environments","text":"","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#1-comprehensive-system-health-check","level":2,"title":"1. Comprehensive System Health Check","text":"<p>(Full script with disk and memory checks, logging, colors.)</p>","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#2-advanced-log-analyzer","level":2,"title":"2. Advanced Log Analyzer","text":"<p>(Grep-based log analysis with report generation.)</p>","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#3-kubernetes-deployment-helper","level":2,"title":"3. Kubernetes Deployment Helper","text":"<p>(Validates YAML, applies deployment, waits for rollout.)</p>","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#4-secure-configuration-manager","level":2,"title":"4. Secure Configuration Manager","text":"<p>(Encrypt/decrypt configs with OpenSSL.)</p>","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#5-advanced-backup-system","level":2,"title":"5. Advanced Backup System","text":"<p>(Database backups, retention, verification.)</p>","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#best-practices-for-production-scripts","level":2,"title":"Best Practices for Production Scripts","text":"","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#security","level":2,"title":"Security","text":"<ul> <li>Avoid command injection.</li> <li>Use <code>mktemp</code> for temp files.</li> </ul>","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#performance","level":2,"title":"Performance","text":"<ul> <li>Prefer shell built-ins over external commands.</li> <li>Process large files with <code>while read</code>.</li> </ul>","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#portability","level":2,"title":"Portability","text":"<ul> <li>Prefer POSIX-compliant syntax when possible.</li> </ul>","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#documentation","level":2,"title":"Documentation","text":"<ul> <li>Include script headers (purpose, author, usage).</li> <li>Document functions with parameters and return codes.</li> </ul>","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#integration-with-modern-devops-tools","level":2,"title":"Integration with Modern DevOps Tools","text":"","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#cicd-pipelines","level":2,"title":"CI/CD Pipelines","text":"<p>(Shell stages for Jenkins, rollback, Slack alerts.)</p>","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#cloud-integration","level":2,"title":"Cloud Integration","text":"<ul> <li>AWS: <code>aws s3 cp</code></li> <li>GCP: <code>gsutil cp</code></li> <li>Azure: <code>az storage blob upload</code></li> </ul>","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#monitoring-and-logging","level":2,"title":"Monitoring and Logging","text":"","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#performance-monitoring","level":2,"title":"Performance Monitoring","text":"<ul> <li>Track script execution time with <code>$SECONDS</code>.</li> <li>Send metrics to monitoring systems.</li> </ul>","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"shell-scripts/scripts/#structured-logging","level":2,"title":"Structured Logging","text":"<ul> <li>JSON logging with <code>jq</code> for easy integration.</li> </ul>","path":["Shell scripts","Shell Scripts for SRE and DevOps"],"tags":[]},{"location":"sonarqube/sonarqube/","level":1,"title":"SonarQube Installation, Configuration, and Integration with Jenkins","text":"Mastering SonarQube: Installation, Configuration, and Jenkins Integration <p>SonarQube is an open-source platform that provides continuous inspection of code quality. It performs automatic reviews to detect bugs, vulnerabilities, and code smells in your codebase. This guide will walk you through installing SonarQube on Ubuntu, configuring it, and integrating it with Jenkins to enhance your continuous integration and continuous delivery (CI/CD) pipeline.</p>","path":["Sonarqube","SonarQube Installation, Configuration, and Integration with Jenkins"],"tags":[]},{"location":"sonarqube/sonarqube/#what-is-sonarqube","level":2,"title":"What is SonarQube?","text":"<p>SonarQube is a powerful tool for continuous code quality inspection. It helps developers identify and fix issues early in the development lifecycle, ensuring that their code is secure, maintainable, and efficient. SonarQube can analyze code in various programming languages, providing feedback on code quality metrics such as bugs, vulnerabilities, code smells, duplications, and test coverage.</p>","path":["Sonarqube","SonarQube Installation, Configuration, and Integration with Jenkins"],"tags":[]},{"location":"sonarqube/sonarqube/#step-1-installing-sonarqube-on-ubuntu","level":2,"title":"Step 1: Installing SonarQube on Ubuntu","text":"","path":["Sonarqube","SonarQube Installation, Configuration, and Integration with Jenkins"],"tags":[]},{"location":"sonarqube/sonarqube/#11-prerequisites","level":2,"title":"1.1 Prerequisites","text":"<p>Before installing SonarQube, ensure that you have Java (JDK 11 or newer) installed. If not, install it using the following command:</p> <pre><code>sudo apt update\nsudo apt install openjdk-11-jdk -y\n</code></pre> <p>Verify the Java installation:</p> <pre><code>java -version\n</code></pre>","path":["Sonarqube","SonarQube Installation, Configuration, and Integration with Jenkins"],"tags":[]},{"location":"sonarqube/sonarqube/#12-install-dependencies","level":2,"title":"1.2 Install Dependencies","text":"<p>SonarQube requires PostgreSQL as its database backend. To install PostgreSQL:</p> <pre><code>sudo apt install postgresql postgresql-contrib -y\n</code></pre> <p>After installation, create a new database and user for SonarQube:</p> <pre><code>sudo -u postgres psql\nCREATE USER sonar WITH PASSWORD 'sonar';\nCREATE DATABASE sonar;\nGRANT ALL PRIVILEGES ON DATABASE sonar TO sonar;\n\\q\n</code></pre>","path":["Sonarqube","SonarQube Installation, Configuration, and Integration with Jenkins"],"tags":[]},{"location":"sonarqube/sonarqube/#13-download-and-install-sonarqube","level":2,"title":"1.3 Download and Install SonarQube","text":"<p>Now, download the latest version of SonarQube from the official website:</p> <pre><code>wget https://binaries.sonarsource.com/CommercialEdition/sonarqube-9.3.0.51899.zip\n</code></pre> <p>Unzip the downloaded file:</p> <pre><code>unzip sonarqube-9.3.0.51899.zip\nsudo mv sonarqube-9.3.0.51899 /opt/sonarqube\n</code></pre>","path":["Sonarqube","SonarQube Installation, Configuration, and Integration with Jenkins"],"tags":[]},{"location":"sonarqube/sonarqube/#14-configure-sonarqube","level":2,"title":"1.4 Configure SonarQube","text":"<p>Navigate to the SonarQube configuration file:</p> <pre><code>cd /opt/sonarqube/conf\nsudo nano sonar.properties\n</code></pre> <p>Edit the following properties to set up your PostgreSQL database:</p> <pre><code>sonar.jdbc.url=jdbc:postgresql://localhost/sonar\nsonar.jdbc.username=sonar\nsonar.jdbc.password=sonar\n</code></pre>","path":["Sonarqube","SonarQube Installation, Configuration, and Integration with Jenkins"],"tags":[]},{"location":"sonarqube/sonarqube/#15-start-sonarqube","level":2,"title":"1.5 Start SonarQube","text":"<p>SonarQube is bundled with a script to start the application. You can start SonarQube using the following commands:</p> <pre><code>cd /opt/sonarqube/bin/linux-x86-64\n./sonar.sh start\n</code></pre> <p>You can verify that SonarQube is running by visiting:</p> <pre><code>http://localhost:9000\n</code></pre> <p>The default login credentials are:</p> <ul> <li>Username: admin</li> <li>Password: admin</li> </ul>","path":["Sonarqube","SonarQube Installation, Configuration, and Integration with Jenkins"],"tags":[]},{"location":"sonarqube/sonarqube/#step-2-installing-the-sonarqube-plugin-for-jenkins","level":2,"title":"Step 2: Installing the SonarQube Plugin for Jenkins","text":"<p>To integrate SonarQube with Jenkins, you need to install the SonarQube Scanner for Jenkins plugin.</p>","path":["Sonarqube","SonarQube Installation, Configuration, and Integration with Jenkins"],"tags":[]},{"location":"sonarqube/sonarqube/#21-install-the-plugin","level":2,"title":"2.1 Install the Plugin","text":"<ol> <li>Open Jenkins in your browser (<code>http://localhost:8080</code>).</li> <li>Navigate to Manage Jenkins &gt; Manage Plugins.</li> <li>Search for SonarQube Scanner in the Available tab and install it.</li> </ol>","path":["Sonarqube","SonarQube Installation, Configuration, and Integration with Jenkins"],"tags":[]},{"location":"sonarqube/sonarqube/#step-3-configuring-sonarqube-in-jenkins","level":2,"title":"Step 3: Configuring SonarQube in Jenkins","text":"","path":["Sonarqube","SonarQube Installation, Configuration, and Integration with Jenkins"],"tags":[]},{"location":"sonarqube/sonarqube/#31-configure-sonarqube-server-in-jenkins","level":2,"title":"3.1 Configure SonarQube Server in Jenkins","text":"<ol> <li>Go to Manage Jenkins &gt; Configure System.</li> <li>Scroll down to the SonarQube Servers section.</li> <li>Click Add SonarQube and enter the following details:</li> <li>Name: SonarQube (or any name you prefer)</li> <li>Server URL: <code>http://localhost:9000</code></li> <li>Authentication Token: To generate an authentication token, log in to SonarQube and go to My Account &gt; Security &gt; Generate Tokens.</li> </ol>","path":["Sonarqube","SonarQube Installation, Configuration, and Integration with Jenkins"],"tags":[]},{"location":"sonarqube/sonarqube/#32-install-the-sonarqube-scanner-in-jenkins","level":2,"title":"3.2 Install the SonarQube Scanner in Jenkins","text":"<ol> <li>In the SonarQube Scanner section, click Add SonarQube Scanner.</li> <li>Enter the Installation Name (e.g., SonarQube Scanner) and set the Version.</li> </ol> <p>Jenkins will automatically detect the SonarQube Scanner when the plugin is installed.</p>","path":["Sonarqube","SonarQube Installation, Configuration, and Integration with Jenkins"],"tags":[]},{"location":"sonarqube/sonarqube/#step-4-create-a-jenkins-pipeline-to-run-sonarqube-analysis","level":2,"title":"Step 4: Create a Jenkins Pipeline to Run SonarQube Analysis","text":"","path":["Sonarqube","SonarQube Installation, Configuration, and Integration with Jenkins"],"tags":[]},{"location":"sonarqube/sonarqube/#41-create-a-new-jenkins-pipeline","level":2,"title":"4.1 Create a New Jenkins Pipeline","text":"<ol> <li>From the Jenkins dashboard, click on New Item.</li> <li>Choose Pipeline and give it a name (e.g., SonarQube-Pipeline).</li> <li>Click OK.</li> </ol>","path":["Sonarqube","SonarQube Installation, Configuration, and Integration with Jenkins"],"tags":[]},{"location":"sonarqube/sonarqube/#42-define-the-pipeline-script","level":2,"title":"4.2 Define the Pipeline Script","text":"<p>Add the following script in the Pipeline section. This pipeline will perform a SonarQube analysis on your project:</p> <pre><code>pipeline {\n    agent any\n    environment {\n        SONARQUBE = 'SonarQube'  // The name of the SonarQube server configured in Jenkins\n    }\n    stages {\n        stage('Checkout') {\n            steps {\n                git 'https://github.com/your-repository-url.git'\n            }\n        }\n        stage('SonarQube Analysis') {\n            steps {\n                script {\n                    // Run the SonarQube scanner\n                    sh \"sonar-scanner\"\n                }\n            }\n        }\n        stage('Build') {\n            steps {\n                // Build your project (e.g., using Maven, Gradle, etc.)\n                sh 'mvn clean install'\n            }\n        }\n    }\n}\n</code></pre>","path":["Sonarqube","SonarQube Installation, Configuration, and Integration with Jenkins"],"tags":[]},{"location":"sonarqube/sonarqube/#43-run-the-pipeline","level":2,"title":"4.3 Run the Pipeline","text":"<p>Save the pipeline and click Build Now. Jenkins will execute the pipeline, run the SonarQube analysis, and publish the results to SonarQube.</p>","path":["Sonarqube","SonarQube Installation, Configuration, and Integration with Jenkins"],"tags":[]},{"location":"sonarqube/sonarqube/#step-5-view-sonarqube-analysis-results","level":2,"title":"Step 5: View SonarQube Analysis Results","text":"<p>After the pipeline runs, you can view the results by logging into the SonarQube dashboard at:</p> <pre><code>http://localhost:9000\n</code></pre> <p>Here, you’ll see detailed code quality metrics, including:</p> <ul> <li>Code coverage</li> <li>Code duplication</li> <li>Code smells</li> <li>Vulnerabilities</li> <li>Bugs</li> </ul> <p>By following these steps, you’ve successfully installed SonarQube, configured it, and integrated it with Jenkins to perform continuous code quality analysis. With SonarQube integrated into your Jenkins pipeline, you can automatically monitor code quality, identify issues early, and maintain high-quality code throughout your development lifecycle.</p> <p>?? info \"Tip\" For better code quality enforcement, integrate SonarQube with Jenkins' automated build and testing process to prevent merging code that does not meet your quality standards.</p> <pre><code>## Key Features:\n1. **SonarQube Setup**: The guide walks users through installing and configuring SonarQube on Ubuntu with PostgreSQL as the database backend.\n2. **Jenkins Integration**: Instructions on integrating SonarQube with Jenkins via the SonarQube Scanner plugin.\n3. **Pipeline Example**: A Jenkins pipeline example that runs SonarQube analysis as part of the build process.\n4. **User-Friendly Navigation**: This guide is broken down into easy-to-follow steps with detailed explanations for each phase.\n</code></pre>","path":["Sonarqube","SonarQube Installation, Configuration, and Integration with Jenkins"],"tags":[]},{"location":"terraform/overview/","level":1,"title":"Overview","text":"<p>Terraform is an Infrastructure as Code (IaC) tool by HashiCorp that allows you to define, provision, and manage infrastructure across cloud providers using declarative configuration files.</p>","path":["Terraform","Overview"],"tags":[]},{"location":"terraform/overview/#why-terraform","level":2,"title":"Why Terraform","text":"<ul> <li>Ideal for creating and managing infrastructure.</li> <li>Uses HCL (HashiCorp Configuration Language) — a human-readable, declarative language.</li> </ul>","path":["Terraform","Overview"],"tags":[]},{"location":"terraform/overview/#example","level":2,"title":"Example","text":"<pre><code>resource \"aws_instance\" \"example\" {\n  ami           = \"ami-0abcd1234\"\n  instance_type = \"t2.micro\"\n}\n</code></pre>","path":["Terraform","Overview"],"tags":[]},{"location":"terraform/overview/#how-terraform-communicates-with-cloud-providers","level":2,"title":"How Terraform Communicates with Cloud Providers","text":"<ol> <li> <p>Providers (Plugins)    Terraform uses provider plugins to communicate with clouds.</p> </li> <li> <p><code>hashicorp/aws</code> for AWS</p> </li> <li><code>hashicorp/azurerm</code> for Azure</li> <li> <p><code>hashicorp/google</code> for GCP</p> </li> <li> <p>APIs    Each provider calls the official cloud API (REST or SDK) to create resources.</p> </li> <li> <p>Authentication    Terraform needs credentials to access APIs:</p> </li> </ol> <pre><code>provider \"aws\" {\n  access_key = \"YOUR_ACCESS_KEY\"\n  secret_key = \"YOUR_SECRET_KEY\"\n  region     = \"us-east-1\"\n}\n</code></pre> <ol> <li> <p>Terraform Workflow</p> </li> <li> <p><code>init</code> → Download providers</p> </li> <li><code>plan</code> → Preview changes</li> <li><code>apply</code> → Create or modify resources</li> <li><code>state</code> → Save results locally or remotely</li> </ol>","path":["Terraform","Overview"],"tags":[]},{"location":"terraform/overview/#example-flow","level":2,"title":"Example Flow","text":"<pre><code>resource \"aws_s3_bucket\" \"demo\" {\n  bucket = \"my-demo-bucket\"\n}\n</code></pre> <p>Terraform uses the AWS provider to authenticate, send an API call, and record the result in <code>terraform.tfstate</code>.</p> <p>Summary: Terraform communicates through provider plugins that use official cloud APIs to create and manage resources.</p>","path":["Terraform","Overview"],"tags":[]},{"location":"terraform/overview/#aws-cli-installation-configuration","level":2,"title":"AWS CLI Installation &amp; Configuration","text":"","path":["Terraform","Overview"],"tags":[]},{"location":"terraform/overview/#1-install-aws-cli","level":2,"title":"1. Install AWS CLI","text":"<p>macOS</p> <pre><code>brew install awscli\naws --version\n</code></pre> <p>Windows</p> <pre><code>choco install awscli\naws --version\n</code></pre> <p>Manual installers are available at: https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html</p>","path":["Terraform","Overview"],"tags":[]},{"location":"terraform/overview/#2-configure-aws-cli","level":2,"title":"2. Configure AWS CLI","text":"<pre><code>aws configure\n</code></pre> <p>Enter:</p> <pre><code>AWS Access Key ID [None]: test\nAWS Secret Access Key [None]: test\nDefault region name [None]: us-east-1\nDefault output format [None]: json\n</code></pre> <p>Configuration files:</p> <pre><code>~/.aws/credentials\n~/.aws/config\n</code></pre> <p>Windows path:</p> <pre><code>C:\\Users\\&lt;username&gt;\\.aws\\\n</code></pre>","path":["Terraform","Overview"],"tags":[]},{"location":"terraform/overview/#3-test-configuration","level":2,"title":"3. Test Configuration","text":"<pre><code>aws s3 ls\n</code></pre> <p>With LocalStack:</p> <pre><code>aws --endpoint-url=http://localhost:4566 s3 ls\n</code></pre>","path":["Terraform","Overview"],"tags":[]},{"location":"terraform/overview/#4-optional-custom-profiles","level":2,"title":"4. Optional: Custom Profiles","text":"<pre><code>aws configure --profile localstack\naws --profile localstack --endpoint-url=http://localhost:4566 s3 ls\n</code></pre>","path":["Terraform","Overview"],"tags":[]},{"location":"terraform/overview/#localstack-setup","level":2,"title":"LocalStack Setup","text":"<p>LocalStack emulates AWS services locally using Docker — perfect for testing Terraform and AWS CLI without real AWS credentials.</p>","path":["Terraform","Overview"],"tags":[]},{"location":"terraform/overview/#why-use-localstack","level":2,"title":"Why Use LocalStack","text":"<ul> <li>Avoid AWS costs</li> <li>Work offline</li> <li>Fast and safe testing</li> <li>Supports S3, EC2, Lambda, DynamoDB, and more</li> </ul>","path":["Terraform","Overview"],"tags":[]},{"location":"terraform/overview/#1-prerequisites","level":2,"title":"1. Prerequisites","text":"Tool Purpose macOS Install Command Docker Desktop Runs containers <code>brew install --cask docker</code> Python 3.8+ Required for CLI Preinstalled pip Python package manager <code>python3 -m ensurepip --upgrade</code>","path":["Terraform","Overview"],"tags":[]},{"location":"terraform/overview/#2-install-localstack","level":2,"title":"2. Install LocalStack","text":"<pre><code>pip install localstack awscli-local\nlocalstack --version\n</code></pre>","path":["Terraform","Overview"],"tags":[]},{"location":"terraform/overview/#3-start-localstack","level":2,"title":"3. Start LocalStack","text":"<pre><code>localstack start\n</code></pre> <p>Runs on <code>http://localhost:4566</code>.</p>","path":["Terraform","Overview"],"tags":[]},{"location":"terraform/overview/#4-web-dashboard","level":2,"title":"4. Web Dashboard","text":"<p>Terminal  </p>","path":["Terraform","Overview"],"tags":[]},{"location":"terraform/overview/#5-test","level":2,"title":"5. Test","text":"<pre><code>awslocal s3 ls\nawslocal s3 mb s3://demo-bucket\n</code></pre>","path":["Terraform","Overview"],"tags":[]},{"location":"terraform/overview/#6-docker-compose-example","level":2,"title":"6. Docker Compose Example","text":"<pre><code>version: \"3.8\"\nservices:\n  localstack:\n    image: localstack/localstack\n    ports:\n      - \"4566:4566\"\n    environment:\n      - SERVICES=s3,ec2,lambda\n      - DEBUG=1\n    volumes:\n      - \"./localstack:/var/lib/localstack\"\n</code></pre> <p>Run:</p> <pre><code>docker-compose up\n</code></pre>","path":["Terraform","Overview"],"tags":[]},{"location":"terraform/overview/#terraform-localstack-integration","level":2,"title":"Terraform + LocalStack Integration","text":"","path":["Terraform","Overview"],"tags":[]},{"location":"terraform/overview/#project-structure","level":2,"title":"Project Structure","text":"<pre><code>terraform-localstack-demo/\n├── main.tf\n├── provider.tf\n├── outputs.tf\n└── hello.txt\n</code></pre>","path":["Terraform","Overview"],"tags":[]},{"location":"terraform/overview/#step-1-create-a-test-file","level":2,"title":"Step 1: Create a Test File","text":"<pre><code>echo \"Hello from Terraform + LocalStack!\" &gt; hello.txt\n</code></pre>","path":["Terraform","Overview"],"tags":[]},{"location":"terraform/overview/#step-2-providertf","level":2,"title":"Step 2: provider.tf","text":"<pre><code>terraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 5.0\"\n    }\n  }\n  required_version = \"&gt;= 1.5.0\"\n}\n\nprovider \"aws\" {\n  region                      = \"us-east-1\"\n  access_key                  = \"test\"\n  secret_key                  = \"test\"\n  s3_force_path_style         = true\n  skip_credentials_validation = true\n  skip_metadata_api_check     = true\n  skip_requesting_account_id  = true\n  endpoints {\n    s3 = \"http://localhost:4566\"\n  }\n}\n</code></pre>","path":["Terraform","Overview"],"tags":[]},{"location":"terraform/overview/#step-3-maintf","level":2,"title":"Step 3: main.tf","text":"<pre><code>resource \"aws_s3_bucket\" \"demo\" {\n  bucket = \"terraform-localstack-demo\"\n}\n\nresource \"aws_s3_object\" \"file_upload\" {\n  bucket = aws_s3_bucket.demo.id\n  key    = \"hello.txt\"\n  source = \"hello.txt\"\n  etag   = filemd5(\"hello.txt\")\n}\n</code></pre>","path":["Terraform","Overview"],"tags":[]},{"location":"terraform/overview/#step-4-outputstf","level":2,"title":"Step 4: outputs.tf","text":"<pre><code>output \"bucket_name\" {\n  value = aws_s3_bucket.demo.bucket\n}\n</code></pre>","path":["Terraform","Overview"],"tags":[]},{"location":"terraform/overview/#step-5-deploy","level":2,"title":"Step 5: Deploy","text":"<pre><code>terraform init\nterraform plan\nterraform apply -auto-approve\n</code></pre> <p>Verify:</p> <pre><code>awslocal s3 ls\nawslocal s3 ls s3://terraform-localstack-demo/\n</code></pre>","path":["Terraform","Overview"],"tags":[]},{"location":"terraform/overview/#cleanup-and-destroy","level":2,"title":"Cleanup and Destroy","text":"","path":["Terraform","Overview"],"tags":[]},{"location":"terraform/overview/#destroy-resources","level":2,"title":"Destroy Resources","text":"<pre><code>terraform destroy -auto-approve\n</code></pre> <p>Output:</p> <pre><code>Destroy complete! Resources: 2 destroyed.\n</code></pre>","path":["Terraform","Overview"],"tags":[]},{"location":"terraform/overview/#optional-preview-destroy","level":2,"title":"Optional: Preview Destroy","text":"<pre><code>terraform plan -destroy\n</code></pre>","path":["Terraform","Overview"],"tags":[]},{"location":"terraform/overview/#verify-deletion","level":2,"title":"Verify Deletion","text":"<pre><code>awslocal s3 ls\n</code></pre>","path":["Terraform","Overview"],"tags":[]},{"location":"terraform/overview/#reset-localstack-optional","level":2,"title":"Reset LocalStack (Optional)","text":"<pre><code>localstack stop\ndocker rm -f localstack\ndocker volume prune -f\nlocalstack start\n</code></pre>","path":["Terraform","Overview"],"tags":[]}]}