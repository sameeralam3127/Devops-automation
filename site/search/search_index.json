{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-devops-automation","title":"Welcome to DevOps Automation","text":"<p>Hi there! \ud83d\udc4b Welcome to DevOps Automation \u2013 your simple spot for everything DevOps and Site Reliability Engineering (SRE). Whether you're an expert or just starting, I'm here to help you learn cool tools, easy tips, and join a friendly community.</p> <p>DevOps Tips \ud83c\udfaf</p> <p>Get simple advice to help you connect and deliver code smoothly.</p> <p>SRE Tools and Insights \ud83d\udd27</p> <p>Discover the latest tools and ideas to keep your systems safe and running well.</p> <p>Hands-On Tutorials \ud83d\udcd8</p> <p>Follow easy, step-by-step guides to set up, deploy, and improve your systems like a pro.</p> <p>Community Forums \ud83c\udf10</p> <p>Join our welcoming community! Share your stories, solve problems, and find great resources in the world of DevOps and SRE.</p>"},{"location":"#about-me","title":"\ud83d\udc68\u200d\ud83d\udcbb About Me","text":"<p>I'm Sameer Alam, a DevOps engineer and SRE fan. I love making complex systems simple and smooth. Over the years, I've learned a lot about what makes systems work.</p> <p>\ud83d\udd17 See my projects: My GitHub</p>"},{"location":"#my-learning-journey","title":"\ud83d\udcda My Learning Journey","text":"<p>Check out my old blog posts from 2016, when I first started on Blogger.</p> <p>\ud83d\udd17 Old Blog</p>"},{"location":"#lets-connect","title":"\ud83d\udce9 Let's Connect!","text":"<p>Have a question or idea? Feel free to reach out.</p> <p>Happy learning and building! \ud83d\ude80 \u2013 Sameer</p>"},{"location":"ansible/ansible/","title":"Ansible","text":"<p>What is Ansible?   - Ansible is an open-source IT automation tool that uses simple YAML files (called playbooks) to define automation tasks.   - It\u2019s agentless, meaning you only need SSH access (or WinRM for Windows) to manage remote machines.</p> <ul> <li>Core Concepts:</li> <li>Inventory: A file listing the machines (hosts) you want to manage.</li> <li>Playbooks: YAML files that define a set of tasks to be executed on your hosts.</li> <li>Modules: Pre-built units of work (like installing packages, managing services, etc.) that Ansible uses to perform tasks.</li> <li>Roles: A way to organize playbooks and related files for reuse and clarity.</li> </ul>"},{"location":"ansible/ansible/#2-setting-up-your-environment","title":"2. Setting Up Your Environment","text":"<ul> <li>Installation:</li> <li>Using pip (Python package manager): <pre><code>pip install ansible\n</code></pre></li> <li>Using a package manager (for Ubuntu/Debian): <pre><code>sudo apt update\nsudo apt install ansible\n</code></pre></li> <li> <p>For macOS: <pre><code>brew install ansible\n</code></pre></p> </li> <li> <p>Verify Installation: <pre><code>ansible --version\n</code></pre></p> </li> </ul>"},{"location":"ansible/ansible/#3-creating-an-inventory-file","title":"3. Creating an Inventory File","text":"<p>An inventory file lists the hosts Ansible will manage. Create a file named <code>hosts.ini</code>:</p> <pre><code>[webservers]\n192.168.1.10\n192.168.1.11\n\n[databases]\ndb1.example.com\n</code></pre>"},{"location":"ansible/ansible/#4-writing-your-first-playbook","title":"4. Writing Your First Playbook","text":"<p>A playbook is a YAML file that defines what you want to do. Here\u2019s an example playbook that installs Apache on hosts in the <code>webservers</code> group:</p> <pre><code>---\n- name: Install Apache on webservers\n  hosts: webservers\n  become: yes  # Elevate privileges if needed (e.g., using sudo)\n  tasks:\n    - name: Ensure Apache is installed\n      apt:\n        name: apache2\n        state: present\n      when: ansible_os_family == \"Debian\"  # Optional: conditionally run for Debian-based systems\n</code></pre> <p>Run the playbook:</p> <pre><code>ansible-playbook -i hosts.ini playbook.yml\n</code></pre>"},{"location":"ansible/ansible/#5-exploring-modules-and-tasks","title":"5. Exploring Modules and Tasks","text":"<ul> <li>Modules: Ansible has hundreds of modules for tasks like file management, package installation, service management, and more.  </li> <li> <p>Examples: <code>yum</code>, <code>apt</code>, <code>service</code>, <code>copy</code>, <code>template</code>, etc.</p> </li> <li> <p>Tasks: Each task in a playbook calls a module with specific parameters. Understanding how to structure these tasks is key to writing effective playbooks.</p> </li> </ul>"},{"location":"ansible/ansible/#6-learning-yaml","title":"6. Learning YAML","text":"<p>Since Ansible playbooks are written in YAML, familiarize yourself with: - Syntax: Indentation, key-value pairs, lists, etc. - Best Practices: Keep files clean and well-commented.</p> <p>A good starting resource is the YAML Tutorial.</p>"},{"location":"ansible/ansible/#7-organizing-with-roles","title":"7. Organizing with Roles","text":"<p>As your playbooks grow, you might want to use roles to organize your code: - Structure of a Role:   - <code>tasks/</code> \u2013 contains main tasks.   - <code>handlers/</code> \u2013 tasks that run when notified.   - <code>templates/</code> \u2013 Jinja2 templates for configuration files.   - <code>files/</code> \u2013 static files to be transferred.   - <code>vars/</code> and <code>defaults/</code> \u2013 variables for configuration.</p> <p>Roles help in reusing and maintaining playbooks more efficiently.</p>"},{"location":"ansible/ansible/#8-hands-on-practice-and-further-learning","title":"8. Hands-On Practice and Further Learning","text":"<ul> <li>Practice Projects:</li> <li>Automate the deployment of a web server.</li> <li>Set up a database server.</li> <li> <p>Create a multi-tier application deployment.</p> </li> <li> <p>Documentation &amp; Community:</p> </li> <li>Official Documentation: Ansible Documentation</li> <li>Community Resources: Ansible Galaxy (for roles), forums, and GitHub repositories.</li> <li> <p>Courses &amp; Tutorials: Look for free online courses, YouTube tutorials, or platforms like Udemy and Pluralsight.</p> </li> <li> <p>Experiment: </p> </li> <li>Use virtual machines or cloud instances (e.g., AWS, Azure) to test your playbooks.</li> <li>Try debugging with <code>ansible-playbook --check --diff</code> to see what changes would be made.</li> </ul>"},{"location":"docker/docker/","title":"Containerization techniques","text":"What is Docker? <p>Docker is an open-source platform designed to simplify application deployment. It ensures that your applications run smoothly, regardless of where they are executed. Docker is a platform that enables developers to build, ship, and run applications in isolated environments called containers. These containers bundle everything an application needs, including libraries, dependencies, and configurations, ensuring it works consistently across different environments.</p> Why Use Docker? <ul> <li>Consistency: Applications behave the same in development, testing, and production.  </li> <li>Lightweight: Containers share the host OS, making them more efficient than virtual machines.  </li> <li>Portable: Run your application anywhere, from your laptop to the cloud.</li> </ul> Getting Started with Docker Installation <p>1. Install Docker on Linux <pre><code># Update the package index\nsudo apt update\n\n# Install required packages\nsudo apt install apt-transport-https ca-certificates curl software-properties-common -y\n\n# Add Docker\u2019s official GPG key\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n\n# Add the Docker repository\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n\n# Install Docker Engine\nsudo apt update\nsudo apt install docker-ce docker-ce-cli containerd.io -y\n\n# Verify the installation\ndocker --version\n</code></pre></p>"},{"location":"docker/docker/#install-docker-on-macos","title":"Install Docker on macOS","text":"Pre-requisites for macOS <ul> <li>Docker requires macOS 10.14 or newer.</li> <li>Ensure you have enough system resources for Docker Desktop.</li> </ul> <pre><code># Download Docker Desktop for Mac from the official site:\nhttps://www.docker.com/products/docker-desktop/\n\n# Install Docker by dragging it to the Applications folder.\n# Open Docker Desktop, and follow the setup instructions.\n</code></pre>"},{"location":"docker/docker/#docker-on-windows","title":"Docker on Windows","text":"Supported Windows Versions <p>Docker Desktop supports Windows 10 64-bit (Professional, Enterprise, and Education) and Windows 11.</p> <ol> <li>Download Docker Desktop from Docker's official website.  </li> <li>Run the installer and follow the on-screen instructions.  </li> <li>After installation, verify it using the command:     <pre><code>docker --version\n</code></pre></li> </ol> <p>\ud83d\udcd2 Useful Tips</p> <p>Enable Non-Root Docker Access on Linux</p> <p>To run Docker commands without <code>sudo</code>, add your user to the Docker group: <pre><code>sudo usermod -aG docker $USER\n</code></pre> Log out and log back in for the changes to take effect.</p> <p>Docker Resource Limits</p> <p>Containers share system resources with the host. Ensure you allocate sufficient CPU and memory for optimal performance.</p> <p>Docker Desktop vs Docker Engine</p> <ul> <li>Docker Desktop: Includes GUI tools and is ideal for macOS and Windows.  </li> <li>Docker Engine: A lightweight CLI-based version for Linux servers.</li> </ul> <p>\u2705 Verify Your Docker Installation</p> <p>After installation, run a test container to verify Docker is working correctly:</p> <pre><code>docker run hello-world\n</code></pre> <p>If Docker is installed correctly, you should see the following output: <pre><code>Hello from Docker!\nThis message shows that your installation appears to be working correctly.\n</code></pre></p>"},{"location":"docker/docker/#docker-command-cheat-sheet","title":"Docker Command Cheat Sheet","text":"<ul> <li> <p>Check Docker Version   Verify the Docker installation and version:   <pre><code>docker --version\n</code></pre></p> </li> <li> <p>List Docker Images   View all images stored locally:   <pre><code>docker images\n</code></pre></p> </li> <li> <p>List Running Containers   Display currently running containers:   <pre><code>docker ps\n</code></pre></p> </li> <li> <p>List All Containers (including stopped)   Show both active and inactive containers:   <pre><code>docker ps -a\n</code></pre></p> </li> <li> <p>Run a Container   Start a new container:   <pre><code>docker run [OPTIONS] IMAGE [COMMAND] [ARG...]\n</code></pre></p> </li> <li> <p>Stop a Running Container   Gracefully stop a running container:   <pre><code>docker stop CONTAINER_ID\n</code></pre></p> </li> <li> <p>Remove a Container   Delete a stopped container:   <pre><code>docker rm CONTAINER_ID\n</code></pre></p> </li> <li> <p>Remove an Image   Delete a local image:   <pre><code>docker rmi IMAGE_ID\n</code></pre></p> </li> </ul>"},{"location":"docker/docker/#building-docker-images","title":"Building Docker Images","text":"<ul> <li> <p>Build an Image from a Dockerfile   Create a Docker image using a <code>Dockerfile</code>:   <pre><code>docker build -t IMAGE_NAME:TAG PATH\n</code></pre></p> </li> <li> <p>Build an Image without Using Cache   Force a fresh build without cache:   <pre><code>docker build --no-cache -t IMAGE_NAME:TAG PATH\n</code></pre></p> </li> <li> <p>View Build History   Check the layer history of an image:   <pre><code>docker history IMAGE_NAME:TAG\n</code></pre></p> </li> </ul>"},{"location":"docker/docker/#dockerfile-basics","title":"Dockerfile Basics","text":"<ul> <li>Basic Dockerfile Example <pre><code># Use a base image\nFROM ubuntu:latest\n\n# Set environment variables\nENV MY_ENV_VAR=value\n\n# Install dependencies\nRUN apt-get update &amp;&amp; apt-get install -y curl\n\n# Copy files into the image\nCOPY ./local_file /container_file\n\n# Define the default command\nCMD [\"echo\", \"Hello, Docker!\"]\n</code></pre></li> </ul>"},{"location":"docker/docker/#networking-and-volumes","title":"Networking and Volumes","text":"<ul> <li> <p>Create a Network   Set up a custom Docker network:   <pre><code>docker network create my_network\n</code></pre></p> </li> <li> <p>Run a Container on a Network   Connect a container to a specific network:   <pre><code>docker run --network my_network IMAGE_NAME\n</code></pre></p> </li> <li> <p>Create a Volume   Create a persistent volume for data storage:   <pre><code>docker volume create my_volume\n</code></pre></p> </li> <li> <p>Run a Container with a Volume   Mount a volume to a container:   <pre><code>docker run -v my_volume:/container_path IMAGE_NAME\n</code></pre></p> </li> </ul>"},{"location":"docker/docker/#docker-compose","title":"Docker Compose","text":"<ul> <li> <p>Start Services   Bring up services defined in a <code>docker-compose.yml</code>:   <pre><code>docker-compose up\n</code></pre></p> </li> <li> <p>Start Services in Detached Mode   Run services in the background:   <pre><code>docker-compose up -d\n</code></pre></p> </li> <li> <p>Stop Services   Shut down all services:   <pre><code>docker-compose down\n</code></pre></p> </li> <li> <p>View Logs   Check logs for all services:   <pre><code>docker-compose logs\n</code></pre></p> </li> </ul>"},{"location":"docker/docker/#pipeline-commands","title":"Pipeline Commands","text":"<ul> <li> <p>Push an Image to Docker Hub   Upload an image to a Docker registry:   <pre><code>docker push IMAGE_NAME:TAG\n</code></pre></p> </li> <li> <p>Pull an Image from Docker Hub   Download an image from a registry:   <pre><code>docker pull IMAGE_NAME:TAG\n</code></pre></p> </li> <li> <p>Tag an Image   Add a new tag to an image:   <pre><code>docker tag SOURCE_IMAGE:TAG TARGET_IMAGE:TAG\n</code></pre></p> </li> <li> <p>Login to Docker Hub   Authenticate to push or pull images:   <pre><code>docker login\n</code></pre></p> </li> </ul>"},{"location":"docker/docker/#troubleshooting-and-scenarios","title":"Troubleshooting and Scenarios","text":""},{"location":"docker/docker/#scenario-1-container-not-starting","title":"Scenario 1: Container Not Starting","text":"<ul> <li>Symptom: You run a container, but it exits immediately.  </li> <li>Solution:   Check the container logs:   <pre><code>docker logs CONTAINER_ID\n</code></pre>   Ensure the container\u2019s command is valid and not exiting with errors.</li> </ul> Tip <p>Check the Dockerfile or the container command to see if it exits immediately due to a failure or invalid command. Adding <code>-it</code> flag for an interactive terminal can help debug.</p>"},{"location":"docker/docker/#scenario-2-image-build-fails","title":"Scenario 2: Image Build Fails","text":"<ul> <li>Symptom: <code>docker build</code> fails with an error.  </li> <li>Solution:</li> <li>Check the error message and fix syntax issues in your <code>Dockerfile</code>.  </li> <li>Use <code>--no-cache</code> to bypass cached layers:     <pre><code>docker build --no-cache -t IMAGE_NAME:TAG .\n</code></pre></li> </ul> Cache Issues <p>Sometimes, the Docker build cache can cause problems with outdated dependencies or commands. Use <code>--no-cache</code> to ensure a clean build.</p>"},{"location":"docker/docker/#scenario-3-port-already-in-use","title":"Scenario 3: Port Already in Use","text":"<ul> <li>Symptom: You get a <code>port already in use</code> error when running a container.  </li> <li>Solution:</li> <li>Identify the process using the port:     <pre><code>lsof -i :PORT\n</code></pre></li> <li>Stop the conflicting process or choose another port for the container:     <pre><code>docker run -p NEW_PORT:CONTAINER_PORT IMAGE_NAME\n</code></pre></li> </ul> Port Conflicts <p>Port conflicts occur when two processes try to bind to the same port. Use <code>docker ps</code> to list running containers and their ports.</p>"},{"location":"docker/docker/#scenario-4-cannot-remove-containerimage","title":"Scenario 4: Cannot Remove Container/Image","text":"<ul> <li>Symptom: <code>docker rm</code> or <code>docker rmi</code> fails with a \"container/image in use\" error.  </li> <li>Solution:</li> <li>Stop all running containers:     <pre><code>docker stop $(docker ps -q)\n</code></pre></li> <li>Force remove the container/image:     <pre><code>docker rm -f CONTAINER_ID\ndocker rmi -f IMAGE_ID\n</code></pre></li> </ul> Force Remove <p>The <code>-f</code> flag forces the removal of containers or images that are in use or running.</p>"},{"location":"jenkins/jenkins/","title":"Jenkins Installation and First Pipeline Setup on Ubuntu","text":"Mastering Jenkins: Installing on Ubuntu and Creating Your First Pipeline <p>Jenkins is an open-source automation server that helps with continuous integration and continuous delivery (CI/CD). This guide will walk you through the process of installing Jenkins on Ubuntu, checking system details using a Bash script, and creating your first pipeline using Jenkins' powerful pipeline functionality.</p>"},{"location":"jenkins/jenkins/#step-1-installing-jenkins-on-ubuntu","title":"Step 1: Installing Jenkins on Ubuntu","text":"<p>Before we begin, ensure your system is up-to-date.</p>"},{"location":"jenkins/jenkins/#11-update-system-packages","title":"1.1 Update System Packages","text":"<p>First, update your system\u2019s package list to ensure everything is up to date:</p> <pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y\n</code></pre>"},{"location":"jenkins/jenkins/#12-install-java","title":"1.2 Install Java","text":"<p>Jenkins requires Java to run. Install the default Java Development Kit (JDK):</p> <pre><code>sudo apt install openjdk-11-jdk -y\n</code></pre> <p>To verify the installation, use:</p> <pre><code>java -version\n</code></pre>"},{"location":"jenkins/jenkins/#13-add-jenkins-repository","title":"1.3 Add Jenkins Repository","text":"<p>To install Jenkins, add its official repository to your system:</p> <pre><code>wget -q -O - https://pkg.jenkins.io/jenkins.io.key | sudo tee /etc/apt/trusted.gpg.d/jenkins.asc\n</code></pre> <p>Next, add the Jenkins repository:</p> <pre><code>sudo sh -c 'echo deb http://pkg.jenkins.io/debian/ stable main &gt; /etc/apt/sources.list.d/jenkins.list'\n</code></pre>"},{"location":"jenkins/jenkins/#14-install-jenkins","title":"1.4 Install Jenkins","text":"<p>Once the repository is added, update the package list and install Jenkins:</p> <pre><code>sudo apt update\nsudo apt install jenkins -y\n</code></pre>"},{"location":"jenkins/jenkins/#15-start-jenkins","title":"1.5 Start Jenkins","text":"<p>Enable and start the Jenkins service:</p> <pre><code>sudo systemctl enable jenkins\nsudo systemctl start jenkins\n</code></pre> <p>You can check the status of Jenkins:</p> <pre><code>sudo systemctl status jenkins\n</code></pre>"},{"location":"jenkins/jenkins/#step-2-accessing-jenkins","title":"Step 2: Accessing Jenkins","text":"<p>Jenkins will be running on port 8080 by default. Open your browser and navigate to:</p> <pre><code>http://localhost:8080\n</code></pre>"},{"location":"jenkins/jenkins/#21-unlock-jenkins","title":"2.1 Unlock Jenkins","text":"<p>To unlock Jenkins, you will need the <code>initialAdminPassword</code>. Retrieve it with the following command:</p> <pre><code>sudo cat /var/lib/jenkins/secrets/initialAdminPassword\n</code></pre> <p>Enter the password in the browser prompt.</p>"},{"location":"jenkins/jenkins/#22-install-suggested-plugins","title":"2.2 Install Suggested Plugins","text":"<p>Once you\u2019ve unlocked Jenkins, you'll be prompted to install plugins. Choose the \"Install suggested plugins\" option to proceed with the default plugin installation.</p>"},{"location":"jenkins/jenkins/#step-3-creating-your-first-jenkins-pipeline","title":"Step 3: Creating Your First Jenkins Pipeline","text":""},{"location":"jenkins/jenkins/#31-create-a-new-pipeline-project","title":"3.1 Create a New Pipeline Project","text":"<ol> <li>After logging into Jenkins, click on New Item.</li> <li>Enter a name for your pipeline (e.g., \"First-Pipeline\").</li> <li>Select Pipeline and click OK.</li> </ol>"},{"location":"jenkins/jenkins/#32-configure-the-pipeline","title":"3.2 Configure the Pipeline","text":"<p>In the pipeline configuration page, scroll down to the Pipeline section. Here, you'll define your pipeline script. For this guide, we\u2019ll use a simple pipeline that checks system details using a Bash script.</p> <pre><code>pipeline {\n    agent any\n\n    stages {\n        stage('Check System Details') {\n            steps {\n                script {\n                    sh 'bash check_system_details.sh'\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"jenkins/jenkins/#33-create-a-bash-script","title":"3.3 Create a Bash Script","text":"<p>In your Jenkins workspace, create a script called <code>check_system_details.sh</code>. This script will gather system information.</p> <pre><code>#!/bin/bash\n\necho \"System Information:\"\necho \"--------------------\"\nhostnamectl\necho\ndf -h\necho\nfree -h\necho\nuname -a\n</code></pre> <p>Ensure that the script has executable permissions:</p> <pre><code>chmod +x check_system_details.sh\n</code></pre>"},{"location":"jenkins/jenkins/#34-run-the-pipeline","title":"3.4 Run the Pipeline","text":"<p>Save the pipeline and click Build Now to execute the pipeline. Jenkins will run the <code>check_system_details.sh</code> script, and you should see the system details in the build log.</p>"},{"location":"kubernetes/kubernetes/","title":"Kubernetes","text":"Mastering Kubernetes: From Introduction to Deploying NGINX <p>Kubernetes is a powerful open-source platform that automates the management, deployment, and scaling of containerized applications. In this guide, we'll walk you through the basics of Kubernetes, how to set it up with Docker Desktop, use the <code>kubectl</code> command-line tool, and deploy an NGINX instance to see how it works.</p>"},{"location":"kubernetes/kubernetes/#what-is-kubernetes","title":"What is Kubernetes?","text":"<p>Kubernetes, often abbreviated as K8s, is a container orchestration platform designed to simplify the deployment and management of containerized applications. It abstracts away the underlying infrastructure and makes it easier to deploy and scale applications seamlessly.</p> <p>Here\u2019s a quick overview of the main concepts in Kubernetes:</p>"},{"location":"kubernetes/kubernetes/#key-concepts","title":"Key Concepts","text":"<ol> <li>Pod:</li> <li>What It Is: The smallest and simplest Kubernetes object. A Pod can host one or more containers that share the same network and storage.</li> <li> <p>What It Does: It serves as the environment where your application containers run.</p> </li> <li> <p>Deployment:</p> </li> <li>What It Is: Manages the deployment of Pods. Ensures the desired number of Pods are running and handles updates.</li> <li> <p>What It Does: Helps with scaling and automating the update process for your application.</p> </li> <li> <p>Service:</p> </li> <li>What It Is: A way to expose and access a set of Pods with a stable network address.</li> <li> <p>What It Does: Handles load balancing and makes sure that services can find and communicate with each other reliably.</p> </li> <li> <p>ReplicaSet:</p> </li> <li>What It Is: Ensures that the specified number of Pod replicas are running.</li> <li> <p>What It Does: Helps maintain the availability of your application by scaling Pods when necessary.</p> </li> <li> <p>Namespace:</p> </li> <li>What It Is: Provides a way to divide cluster resources into separate, logical groups.</li> <li>What It Does: Useful for managing different environments (e.g., development, production) within the same cluster.</li> </ol>"},{"location":"kubernetes/kubernetes/#setting-up-kubernetes-in-docker-desktop","title":"Setting Up Kubernetes in Docker Desktop","text":"<p>Docker Desktop includes a built-in Kubernetes cluster that can be easily configured for local development. Here's how you can set it up:</p> <ol> <li> <p>Open Docker Desktop:</p> <ul> <li>Start Docker Desktop from your application menu.</li> </ul> </li> <li> <p>Go to Settings:</p> <ul> <li>Click on the gear icon (\u2699\ufe0f) in the top-right corner.</li> </ul> </li> <li> <p>Select the Kubernetes Tab:</p> <ul> <li>Click on the \"Kubernetes\" tab from the sidebar.</li> </ul> </li> <li> <p>Enable Kubernetes:</p> <ul> <li>Check the box that says \"Enable Kubernetes\".</li> </ul> </li> <li> <p>Apply &amp; Restart:</p> <ul> <li>Click \"Apply &amp; Restart\" to apply the changes. Docker Desktop will restart to configure Kubernetes.</li> </ul> </li> <li> <p>Wait for Setup:</p> <ul> <li>It may take a few minutes for Kubernetes to start. Docker Desktop will show the status of the Kubernetes setup.</li> </ul> <p>??? info \"Tip\"     If you are new to Kubernetes, Docker Desktop is a great way to get started as it provides an easy local environment without needing to set up a full Kubernetes cluster.</p> </li> </ol>"},{"location":"kubernetes/kubernetes/#what-is-kubectl","title":"What is <code>kubectl</code>?","text":"<p><code>kubectl</code> is the command-line tool for interacting with Kubernetes clusters. It allows you to create, manage, and troubleshoot your Kubernetes resources.</p>"},{"location":"kubernetes/kubernetes/#key-features-of-kubectl","title":"Key Features of <code>kubectl</code>:","text":"<ul> <li>Run Commands: Manage Pods, Deployments, and other resources.</li> <li>Change Configurations: Apply YAML files to create or update Kubernetes resources.</li> <li>Check Status: View the status of your resources to troubleshoot or monitor your application.</li> </ul>"},{"location":"kubernetes/kubernetes/#common-kubectl-commands","title":"Common <code>kubectl</code> Commands:","text":"<ul> <li>Get Resources:   <pre><code>kubectl get [resource]\n</code></pre></li> <li> <p>Lists resources such as Pods, Services, Deployments, etc.</p> </li> <li> <p>Describe Resource:   <pre><code>kubectl describe [resource] [name]\n</code></pre></p> </li> <li> <p>Shows detailed information about a specific resource.</p> </li> <li> <p>Apply Configuration:   <pre><code>kubectl apply -f [file.yaml]\n</code></pre></p> </li> <li> <p>Applies changes from a YAML configuration file.</p> </li> <li> <p>Delete Resource:   <pre><code>kubectl delete -f [file.yaml]\n</code></pre></p> </li> <li>Deletes resources defined in a YAML file.</li> </ul> Deleting Resources <p>Be careful when using the <code>kubectl delete</code> command, as it permanently removes resources like Pods and Services from your cluster.</p>"},{"location":"kubernetes/kubernetes/#checking-versions","title":"Checking Versions","text":""},{"location":"kubernetes/kubernetes/#check-kubectl-version","title":"Check <code>kubectl</code> Version","text":"<p>To check the version of <code>kubectl</code>, run the following command:</p> <pre><code>kubectl version --client\n</code></pre> <p>For both the client and server versions of Kubernetes, use:</p> <pre><code>kubectl version\n</code></pre>"},{"location":"kubernetes/kubernetes/#deploying-nginx-instances","title":"Deploying NGINX Instances","text":"<p>Now, let\u2019s walk through deploying multiple NGINX instances on your Kubernetes cluster.</p>"},{"location":"kubernetes/kubernetes/#step-1-create-a-deployment","title":"Step 1: Create a Deployment","text":"<ol> <li>Open Terminal:</li> <li> <p>Use your terminal or command prompt.</p> </li> <li> <p>Create Deployment YAML File:</p> </li> <li>Save the following YAML definition to a file named <code>nginx-deployment.yaml</code>:</li> </ol> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n        ports:\n        - containerPort: 80\n</code></pre> <ul> <li> <p>What It Does: Defines a Deployment with 3 replicas of NGINX.</p> </li> <li> <p>Apply the Deployment:</p> </li> <li>Run:</li> </ul> <pre><code>kubectl apply -f nginx-deployment.yaml\n</code></pre>"},{"location":"kubernetes/kubernetes/#step-2-expose-the-deployment","title":"Step 2: Expose the Deployment","text":"<ol> <li>Create Service YAML File:</li> <li>Save the following YAML to <code>nginx-service.yaml</code>:</li> </ol> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\nspec:\n  selector:\n    app: nginx\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n  type: LoadBalancer\n</code></pre> <ul> <li> <p>What It Does: Creates a Service to expose the NGINX Deployment.</p> </li> <li> <p>Apply the Service:</p> </li> <li>Run:</li> </ul> <pre><code>kubectl apply -f nginx-service.yaml\n</code></pre> <ol> <li>Access NGINX in Browser:</li> <li>Open a web browser and navigate to:</li> </ol> <pre><code>http://127.0.0.1/\n</code></pre>"},{"location":"kubernetes/kubernetes/#step-3-check-the-status","title":"Step 3: Check the Status","text":"<ol> <li>List Pods:</li> <li>Check the status of the Pods:</li> </ol> <pre><code>kubectl get pods\n</code></pre> <ol> <li>Check Deployment:</li> <li>Verify the deployment:</li> </ol> <pre><code>kubectl get deployments\n</code></pre> <ol> <li>Check Service:</li> <li>View the status of the Service:</li> </ol> <pre><code>kubectl get services\n</code></pre> <ul> <li>Note: Docker Desktop includes a Kubernetes dashboard, which provides a visual view of your resources.</li> </ul>"},{"location":"kubernetes/kubernetes/#step-4-clean-up","title":"Step 4: Clean Up","text":"<ol> <li>Delete the Service:</li> <li>To remove the Service:</li> </ol> <pre><code>kubectl delete -f nginx-service.yaml\n</code></pre> <ol> <li>Delete the Deployment:</li> <li>To remove the Deployment:</li> </ol> <pre><code>kubectl delete -f nginx-deployment.yaml\n</code></pre> <ol> <li>Verify Deletion:</li> <li>Ensure that the Pods and Services have been removed:</li> </ol> <pre><code>kubectl get pods\nkubectl get services\n</code></pre>"},{"location":"monitoring-tools/log/","title":"Grafana","text":"<p>Monitoring is a crucial part of DevOps, helping teams detect issues before they become critical. In this guide, we will install Prometheus, Grafana, and Node Exporter using Podman, an alternative to Docker. By the end, you'll have a fully functional monitoring stack with visual dashboards to analyze system metrics.  </p>"},{"location":"monitoring-tools/log/#what-are-prometheus-grafana-and-node-exporter","title":"What Are Prometheus, Grafana, and Node Exporter?","text":""},{"location":"monitoring-tools/log/#1-prometheus-the-time-series-database","title":"1. Prometheus: The Time-Series Database","text":"<p>Prometheus is an open-source monitoring system that collects and stores metrics as time-series data. It is widely used in DevOps due to its powerful querying language (PromQL) and easy integration with multiple exporters.  </p> <p>\ud83d\udccc Key Features: - Time-series data storage - Pull-based metric collection - Alerting and rule-based evaluations  </p>"},{"location":"monitoring-tools/log/#2-node-exporter-the-system-metrics-collector","title":"2. Node Exporter: The System Metrics Collector","text":"<p>Node Exporter is a lightweight agent that runs on a machine to collect system metrics such as CPU usage, memory utilization, disk I/O, and network statistics.  </p> <p>\ud83d\udccc Common Metrics Collected: - CPU Load: <code>node_cpu_seconds_total</code> - Memory Usage: <code>node_memory_MemAvailable_bytes</code> - Disk Space: <code>node_filesystem_avail_bytes</code> - Network Traffic: <code>node_network_receive_bytes_total</code> </p>"},{"location":"monitoring-tools/log/#3-grafana-the-visualization-tool","title":"3. Grafana: The Visualization Tool","text":"<p>Grafana is an open-source tool that helps visualize and analyze time-series data. It allows you to create dashboards with real-time graphs, alerts, and reports.  </p> <p>\ud83d\udccc Why Use Grafana? - Connects to multiple data sources (Prometheus, MySQL, AWS CloudWatch, etc.) - Beautiful and interactive dashboards - Custom alerts and notifications  </p>"},{"location":"monitoring-tools/log/#step-1-install-and-run-containers","title":"Step 1: Install and Run Containers","text":"<p>We will use Podman, a rootless container runtime similar to Docker, to set up our monitoring stack.</p>"},{"location":"monitoring-tools/log/#11-pull-and-run-a-centos-container","title":"1.1 Pull and Run a CentOS Container","text":"<p>We'll start by running a CentOS container where we will install Node Exporter.</p> <pre><code>podman run -dit --name centos_container -p 9100:9100 centos:latest\n</code></pre> <p>Verify that the container is running:</p> <pre><code>podman ps\n</code></pre>"},{"location":"monitoring-tools/log/#12-run-prometheus","title":"1.2 Run Prometheus","text":"<p>Now, let's run Prometheus, which will scrape metrics from Node Exporter.</p> <pre><code>podman run -d --name prometheus -p 9090:9090 quay.io/prometheus/prometheus\n</code></pre> <p>Check if Prometheus is running properly:</p> <pre><code>podman logs prometheus\n</code></pre> <p>You can now access Prometheus at: \ud83d\udc49 http://localhost:9090/query</p> <p>To enter the Prometheus container:</p> <pre><code>podman exec -it prometheus /bin/sh\n</code></pre>"},{"location":"monitoring-tools/log/#step-2-install-and-run-node-exporter","title":"Step 2: Install and Run Node Exporter","text":"<p>Log into the CentOS container:</p> <pre><code>podman exec -it centos_container /bin/bash\n</code></pre> <p>Download and extract Node Exporter:</p> <pre><code>wget https://github.com/prometheus/node_exporter/releases/download/v1.9.0/node_exporter-1.9.0.linux-386.tar.gz\ntar -xvf node_exporter-1.9.0.linux-386.tar.gz\ncd node_exporter-1.9.0.linux-386\n./node_exporter\n</code></pre> <p>Now, you can check the collected metrics at: \ud83d\udc49 http://localhost:9100/metrics</p>"},{"location":"monitoring-tools/log/#step-3-configure-prometheus-to-scrape-node-exporter-metrics","title":"Step 3: Configure Prometheus to Scrape Node Exporter Metrics","text":"<p>Modify the Prometheus configuration file (<code>prometheus.yml</code>):  </p> <pre><code>cd /etc/prometheus\nvim prometheus.yml\n</code></pre> <p>Add the following configuration:</p> <pre><code>global:\n  scrape_interval: 15s # Collect metrics every 15 seconds\n\nscrape_configs:\n  - job_name: \"prometheus\"\n    static_configs:\n      - targets: [\"localhost:9090\"]\n\n  - job_name: \"centos_node_exporter\"\n    static_configs:\n      - targets: [\"localhost:9100\"]\n</code></pre> <p>Restart Prometheus:</p> <pre><code>podman restart prometheus\n</code></pre> <p>Check if Prometheus is scraping Node Exporter: \ud83d\udc49 http://localhost:9090/targets</p>"},{"location":"monitoring-tools/log/#step-4-deploy-grafana","title":"Step 4: Deploy Grafana","text":"<p>Run the Grafana container:</p> <pre><code>podman run -d --name grafana -p 3000:3000 grafana/grafana\n</code></pre> <p>Get the IP address of Prometheus:</p> <pre><code>podman inspect -f '{{ .NetworkSettings.IPAddress }}' prometheus\n</code></pre> <p>Access Grafana at: \ud83d\udc49 http://localhost:3000 </p> <p>\ud83d\udccc Default Login: - Username: <code>admin</code> - Password: <code>admin</code> (change it on first login)  </p>"},{"location":"monitoring-tools/log/#step-5-add-prometheus-as-a-data-source-in-grafana","title":"Step 5: Add Prometheus as a Data Source in Grafana","text":"<ol> <li>Open Grafana (<code>http://localhost:3000</code>).  </li> <li>Go to Configuration &gt; Data Sources.  </li> <li>Click \"Add Data Source\", select Prometheus.  </li> <li>Set the URL as <code>http://prometheus:9090</code>.  </li> <li>Click \"Save &amp; Test\".  </li> </ol>"},{"location":"monitoring-tools/log/#step-6-import-node-exporter-dashboard","title":"Step 6: Import Node Exporter Dashboard","text":"<ol> <li>In Grafana, go to \"Create\" &gt; \"Import\".  </li> <li>Enter the Dashboard ID: <code>1860</code> (or search for \"Node Exporter Full\").  </li> <li>Select Prometheus as the data source.  </li> <li>Click \"Import\".  </li> </ol> <p>You should now see a full system monitoring dashboard! \ud83c\udf89  </p>"},{"location":"monitoring-tools/log/#step-7-managing-containers","title":"Step 7: Managing Containers","text":"<p>To stop and remove containers:</p> <pre><code>podman stop centos_container\npodman rm centos_container\n</code></pre> <p>Restart Prometheus and Grafana:</p> <pre><code>podman restart prometheus\npodman restart grafana\n</code></pre>"},{"location":"shell-scripts/scripts/","title":"Shell Scripts for SRE and DevOps Engineers","text":"<p>Definition</p> <p>A shell script is a plain text file containing a series of commands executed by the shell (the command-line interpreter). Shell scripts help automate repetitive tasks, manage system operations, and perform batch processing. The most common shell on Linux systems is Bash (Bourne Again SHell). Advantages</p> <ul> <li>Automation: Streamline routine tasks.</li> <li>Efficiency: Execute multiple commands with a single script.</li> <li>System Administration: Manage files, processes, and system operations.</li> <li>Batch Processing: Process data or files in bulk.</li> </ul>"},{"location":"shell-scripts/scripts/#shebang-line","title":"Shebang Line","text":"<pre><code>#!/bin/bash\n</code></pre> <p>Tip</p> <p>The shebang tells the system which interpreter (here, Bash) to use for executing the script.</p>"},{"location":"shell-scripts/scripts/#comments","title":"Comments","text":"<pre><code># This is a comment\n</code></pre> <p>Note</p> <p>Use comments to explain your code. Anything after <code>#</code> is ignored by the interpreter.</p>"},{"location":"shell-scripts/scripts/#variables","title":"Variables","text":"<pre><code>name=\"Alice\"\necho \"Hello, $name\"\n</code></pre> <p>Tip</p> <p>Variables store data. Remember: do not leave spaces around the equals sign.</p>"},{"location":"shell-scripts/scripts/#command-substitution","title":"Command Substitution","text":"<pre><code>current_date=$(date)\necho \"Today is $current_date\"\n</code></pre> <p>Note</p> <p>Use command substitution to capture the output of commands into a variable.</p>"},{"location":"shell-scripts/scripts/#conditionals","title":"Conditionals","text":"<pre><code>if [ condition ]; then\n    # commands\nelif [ another_condition ]; then\n    # commands\nelse\n    # commands\nfi\n</code></pre> <p>Tip</p> <p>Conditionals allow your script to take different actions based on specific conditions.</p>"},{"location":"shell-scripts/scripts/#loops","title":"Loops","text":""},{"location":"shell-scripts/scripts/#for-loop","title":"For Loop","text":"<pre><code>for i in {1..5}; do\n    echo \"Number: $i\"\ndone\n</code></pre> <p>Note</p> <p>Iterate through a list or sequence with a for loop.</p>"},{"location":"shell-scripts/scripts/#while-loop","title":"While Loop","text":"<pre><code>count=1\nwhile [ $count -le 5 ]; do\n    echo \"Count: $count\"\n    ((count++))\ndone\n</code></pre> <p>Tip</p> <p>A while loop runs as long as the condition is true.</p>"},{"location":"shell-scripts/scripts/#functions","title":"Functions","text":"<pre><code>greet() {\n    echo \"Hello, $1!\"\n}\ngreet \"Bob\"\n</code></pre> <p>Note</p> <p>Functions let you reuse code. Pass parameters to functions for flexibility.</p>"},{"location":"shell-scripts/scripts/#practical-script-examples","title":"Practical Script Examples","text":"<p>Below are some practical examples that illustrate key concepts.</p>"},{"location":"shell-scripts/scripts/#1-hello-world-script","title":"1. Hello World Script","text":"<pre><code>#!/bin/bash\necho \"Hello, World!\"\n</code></pre> <p>Explanation</p> <p>A simple script to print \"Hello, World!\" to the console.</p>"},{"location":"shell-scripts/scripts/#2-disk-usage-monitor","title":"2. Disk Usage Monitor \ud83d\udcca","text":"<pre><code>#!/bin/bash\nTHRESHOLD=80\nPARTITION=\"/\"\nLOG_FILE=\"/var/log/disk_usage.log\"\n\nusage=$(df -h \"$PARTITION\" | awk 'NR==2 {print $5}' | tr -d '%')\nif [ \"$usage\" -ge \"$THRESHOLD\" ]; then\n  echo \"$(date): Disk usage on $PARTITION is ${usage}%.\" &gt;&gt; \"$LOG_FILE\"\nfi\n</code></pre> <p>Explanation</p> <p>This script checks if the disk usage exceeds 80% and logs a message.</p>"},{"location":"shell-scripts/scripts/#3-memory-usage-monitor","title":"3. Memory Usage Monitor \ud83d\udcbe","text":"<pre><code>#!/bin/bash\nTHRESHOLD=500  # in MB\nLOG_FILE=\"/var/log/memory_usage.log\"\n\nfree_mem=$(free -m | awk '/^Mem:/{print $7}')\nif [ \"$free_mem\" -lt \"$THRESHOLD\" ]; then\n  echo \"$(date): Low memory: ${free_mem}MB available.\" &gt;&gt; \"$LOG_FILE\"\nfi\n</code></pre> <p>Explanation</p> <p>Monitors available memory and logs if it drops below the threshold.</p>"},{"location":"shell-scripts/scripts/#4-service-auto-restart","title":"4. Service Auto-Restart \ud83d\udd04","text":"<pre><code>#!/bin/bash\nSERVICE=\"nginx\"\nLOG_FILE=\"/var/log/service_monitor.log\"\n\nif ! systemctl is-active --quiet \"$SERVICE\"; then\n  echo \"$(date): $SERVICE is down. Attempting restart...\" &gt;&gt; \"$LOG_FILE\"\n  systemctl restart \"$SERVICE\"\n  if systemctl is-active --quiet \"$SERVICE\"; then\n    echo \"$(date): $SERVICE restarted successfully.\" &gt;&gt; \"$LOG_FILE\"\n  else\n    echo \"$(date): Failed to restart $SERVICE.\" &gt;&gt; \"$LOG_FILE\"\n  fi\nfi\n</code></pre> <p>Explanation</p> <p>Checks if a service (nginx) is active and attempts a restart if it isn\u2019t.</p>"},{"location":"shell-scripts/scripts/#5-network-connectivity-checker","title":"5. Network Connectivity Checker \ud83c\udf10","text":"<pre><code>#!/bin/bash\nHOSTS=(\"8.8.8.8\" \"1.1.1.1\")\nLOG_FILE=\"/var/log/network_connectivity.log\"\n\nfor host in \"${HOSTS[@]}\"; do\n  if ! ping -c 2 \"$host\" &amp;&gt;/dev/null; then\n    echo \"$(date): Host $host is unreachable.\" &gt;&gt; \"$LOG_FILE\"\n  fi\ndone\n</code></pre> <p>Explanation</p> <p>Pings a list of hosts and logs unreachable ones.</p>"},{"location":"shell-scripts/scripts/#6-ssl-certificate-expiry-checker","title":"6. SSL Certificate Expiry Checker \ud83d\udd12","text":"<pre><code>#!/bin/bash\nDOMAIN=\"example.com\"\nEXP_THRESHOLD=30  # days\nLOG_FILE=\"/var/log/ssl_expiry.log\"\n\nexpiry_date=$(echo | openssl s_client -servername \"$DOMAIN\" -connect \"$DOMAIN\":443 2&gt;/dev/null \\\n             | openssl x509 -noout -dates | grep 'notAfter' | cut -d= -f2)\nexpiry_seconds=$(date -d \"$expiry_date\" +%s)\ncurrent_seconds=$(date +%s)\ndays_left=$(( (expiry_seconds - current_seconds) / 86400 ))\n\nif [ \"$days_left\" -le \"$EXP_THRESHOLD\" ]; then\n  echo \"$(date): SSL certificate for $DOMAIN expires in $days_left days.\" &gt;&gt; \"$LOG_FILE\"\nfi\n</code></pre> <p>Explanation</p> <p>Checks the expiration date of an SSL certificate and logs if it\u2019s near expiry.</p>"},{"location":"shell-scripts/scripts/#ssl-certificate-installation","title":"SSL Certificate Installation","text":"<p>This section details an advanced script that automates SSL certificate tasks such as detection, installation, renewal, and HTTPS validation.</p> <pre><code>#!/bin/bash\n\n# Function to check if the OS is RHEL or Debian-based\ndetect_os() {\n    if [ -f /etc/redhat-release ]; then\n        OS=\"RHEL\"\n    elif [ -f /etc/debian_version ]; then\n        OS=\"Debian\"\n    else\n        echo \"Unsupported OS\"\n        exit 1\n    fi\n}\n\n# Function to check and install Certbot\ninstall_certbot() {\n    echo \"Checking if Certbot is installed...\"\n\n    if command -v certbot &amp;&gt; /dev/null; then\n        echo \"Certbot is already installed.\"\n        return\n    fi\n\n    echo \"Certbot not found. Installing Certbot...\"\n\n    if [ \"$OS\" == \"Debian\" ]; then\n        apt-get update\n        apt-get install -y certbot\n    elif [ \"$OS\" == \"RHEL\" ]; then\n        yum install -y epel-release\n        yum install -y certbot\n    else\n        echo \"Unsupported OS for Certbot installation\"\n        exit 1\n    fi\n}\n\n# Function to check the existing SSL certificate details\ncheck_existing_ssl() {\n    echo \"Checking existing SSL certificates for HTTPD and NGINX...\"\n\n    for config_file in /etc/httpd/conf.d/ssl.conf /etc/nginx/nginx.conf; do\n        if [ -f \"$config_file\" ]; then\n            echo \"SSL Configuration found in: $config_file\"\n            ssl_cert=$(grep -i \"SSLCertificateFile\\|ssl_certificate\" $config_file | awk '{print $2}')\n            if [ -f \"$ssl_cert\" ]; then\n                echo \"SSL Certificate Details:\"\n                openssl x509 -in $ssl_cert -noout -text | grep -E 'Issuer:|Subject:|Not After :'\n            else\n                echo \"SSL Certificate not found in the specified location: $ssl_cert\"\n            fi\n        else\n            echo \"SSL configuration file not found for $config_file\"\n        fi\n    done\n}\n\n# Function to create a private key and CSR\ncreate_key_and_csr() {\n    echo \"Creating Private Key and CSR...\"\n    mkdir -p /certs\n\n    read -p \"Enter Country (2 letter code): \" country\n    read -p \"Enter State or Province Name: \" state\n    read -p \"Enter Locality Name (e.g., city): \" locality\n    read -p \"Enter Organization Name: \" organization\n    read -p \"Enter Organizational Unit Name: \" unit\n    read -p \"Enter Common Name (e.g., domain name): \" common_name\n    read -p \"Enter Email Address: \" email\n\n    if [ -z \"$common_name\" ]; then\n        echo \"Error: Common Name (domain) is required.\"\n        exit 1\n    fi\n\n    openssl req -newkey rsa:2048 -nodes -keyout /certs/mydomain.key -out /certs/mydomain.csr -subj \"/C=$country/ST=$state/L=$locality/O=$organization/OU=$unit/CN=$common_name/emailAddress=$email\"\n    if [ $? -ne 0 ]; then\n        echo \"Error creating CSR\"\n        exit 1\n    fi\n    echo \"Private key and CSR created successfully.\"\n}\n\n# Function to obtain a certificate using Let's Encrypt\ncreate_ssl_certificate() {\n    echo \"Generating SSL certificate using Let's Encrypt...\"\n\n    install_certbot\n\n    certbot certonly --standalone --non-interactive --agree-tos --email $email -d $common_name\n    if [ $? -ne 0 ]; then\n        echo \"Error generating SSL certificate\"\n        exit 1\n    fi\n    echo \"SSL certificate generated successfully.\"\n}\n\n# Function to move certificate and key to correct locations for HTTPD and NGINX\ninstall_certificate() {\n    echo \"Installing SSL certificate for HTTPD and NGINX...\"\n    mkdir -p /etc/httpd/ssl /etc/nginx/ssl\n\n    mv /etc/letsencrypt/live/$common_name/fullchain.pem /etc/httpd/ssl/mydomain.crt\n    mv /etc/letsencrypt/live/$common_name/privkey.pem /etc/httpd/ssl/mydomain.key\n    mv /etc/letsencrypt/live/$common_name/fullchain.pem /etc/nginx/ssl/mydomain.crt\n    mv /etc/letsencrypt/live/$common_name/privkey.pem /etc/nginx/ssl/mydomain.key\n\n    # Set permissions\n    chmod 600 /etc/httpd/ssl/mydomain.key\n    chown root:root /etc/httpd/ssl/mydomain.key\n    chmod 644 /etc/httpd/ssl/mydomain.crt\n    chown root:root /etc/httpd/ssl/mydomain.crt\n    chmod 600 /etc/nginx/ssl/mydomain.key\n    chown root:root /etc/nginx/ssl/mydomain.key\n    chmod 644 /etc/nginx/ssl/mydomain.crt\n    chown root:root /etc/nginx/ssl/mydomain.crt\n\n    # Update HTTPD conf\n    if grep -q \"SSLCertificateFile\" /etc/httpd/conf.d/ssl.conf; then\n        sed -i \"s#SSLCertificateFile .*#SSLCertificateFile /etc/httpd/ssl/mydomain.crt#\" /etc/httpd/conf.d/ssl.conf\n        sed -i \"s#SSLCertificateKeyFile .*#SSLCertificateKeyFile /etc/httpd/ssl/mydomain.key#\" /etc/httpd/conf.d/ssl.conf\n    else\n        echo \"SSLCertificateFile /etc/httpd/ssl/mydomain.crt\" &gt;&gt; /etc/httpd/conf.d/ssl.conf\n        echo \"SSLCertificateKeyFile /etc/httpd/ssl/mydomain.key\" &gt;&gt; /etc/httpd/conf.d/ssl.conf\n    fi\n\n    # Update NGINX conf\n    if grep -q \"ssl_certificate\" /etc/nginx/nginx.conf; then\n        sed -i \"s#ssl_certificate .*#ssl_certificate /etc/nginx/ssl/mydomain.crt;#\" /etc/nginx/nginx.conf\n        sed -i \"s#ssl_certificate_key .*#ssl_certificate_key /etc/nginx/ssl/mydomain.key;#\" /etc/nginx/nginx.conf\n    else\n        echo \"ssl_certificate /etc/nginx/ssl/mydomain.crt;\" &gt;&gt; /etc/nginx/nginx.conf\n        echo \"ssl_certificate_key /etc/nginx/ssl/mydomain.key;\" &gt;&gt; /etc/nginx/nginx.conf\n    fi\n\n    systemctl restart httpd nginx\n\n    if [ $? -ne 0 ]; then\n        echo \"Error restarting services. Checking logs...\"\n        if [ \"$OS\" == \"RHEL\" ]; then\n            cat /var/log/httpd/error_log /var/log/nginx/error.log\n        else\n            cat /var/log/apache2/error.log /var/log/nginx/error.log\n        fi\n        exit 1\n    fi\n\n    echo \"SSL Certificate installed and services restarted successfully.\"\n}\n\n# Function to renew SSL certificate\nrenew_certificate() {\n    echo \"Renewing SSL certificate...\"\n    certbot renew\n    if [ $? -ne 0 ]; then\n        echo \"Error renewing SSL certificate.\"\n        exit 1\n    fi\n    install_certificate\n}\n\n# Function to validate HTTPS connection\nvalidate_https() {\n    echo \"Validating HTTPS connection on port 443...\"\n    curl -Is https://$common_name:443 | head -n 1\n    if [ $? -ne 0 ]; then\n        echo \"Error: Unable to connect to $common_name on port 443.\"\n        exit 1\n    fi\n    echo \"HTTPS validation successful.\"\n}\n\n# Menu-driven interface\nwhile true; do\n    echo \"SSL Certificate Management Script\"\n    echo \"1. Check Existing SSL Certificate\"\n    echo \"2. Create Key and CSR\"\n    echo \"3. Generate SSL Certificate (Let's Encrypt)\"\n    echo \"4. Install Certificate\"\n    echo \"5. Renew SSL Certificate\"\n    echo \"6. Validate HTTPS Connection\"\n    echo \"7. Exit\"\n\n    read -p \"Enter your choice: \" choice\n\n    case $choice in\n        1) check_existing_ssl ;;\n        2) create_key_and_csr ;;\n        3) create_ssl_certificate ;;\n        4) install_certificate ;;\n        5) renew_certificate ;;\n        6) validate_https ;;\n        7) echo \"Exiting...\"; exit ;;\n        *) echo \"Invalid choice. Please select a valid option.\" ;;\n    esac\ndone\n</code></pre> <p>Explanation</p> <p>This multi-function script automates SSL certificate management with Certbot and OpenSSL. Follow the prompts to manage certificates.</p>"},{"location":"shell-scripts/scripts/#monitoring-automation-scripts","title":"Monitoring &amp; Automation Scripts","text":"<p>Below are 50+ scripts designed to monitor system resources, services, and perform various automated checks. Each script includes a code snippet and an explanation using MkDocs admonitions.</p>"},{"location":"shell-scripts/scripts/#1-http-service-health-check","title":"1. HTTP Service Health Check","text":"<pre><code>#!/bin/bash\nSERVICE_URL=\"http://localhost:8080/health\"\nTIMEOUT=5\nLOG_FILE=\"/var/log/health_check.log\"\n\nresponse=$(curl -s --max-time $TIMEOUT -o /dev/null -w \"%{http_code}\" \"$SERVICE_URL\")\nif [ \"$response\" -eq 200 ]; then\n  echo \"$(date): Service is up.\" &gt;&gt; \"$LOG_FILE\"\nelse\n  echo \"$(date): Service is down! HTTP code: $response\" &gt;&gt; \"$LOG_FILE\"\nfi\n</code></pre> <p>Explanation</p> <p>Checks an HTTP endpoint and logs the health status of the service.</p>"},{"location":"shell-scripts/scripts/#2-disk-usage-monitor_1","title":"2. Disk Usage Monitor","text":"<p>(See the Practical Example above.)</p>"},{"location":"shell-scripts/scripts/#3-memory-usage-monitor_1","title":"3. Memory Usage Monitor","text":"<p>(See the Practical Example above.)</p>"},{"location":"shell-scripts/scripts/#4-cpu-load-monitor","title":"4. CPU Load Monitor","text":"<pre><code>#!/bin/bash\nTHRESHOLD=2.0\nLOG_FILE=\"/var/log/cpu_load.log\"\n\nload_avg=$(uptime | awk -F'load average:' '{ print $2 }' | cut -d',' -f1 | tr -d ' ')\nif (( $(echo \"$load_avg &gt; $THRESHOLD\" | bc -l) )); then\n  echo \"$(date): High load average detected: $load_avg\" &gt;&gt; \"$LOG_FILE\"\nfi\n</code></pre> <p>Explanation</p> <p>Monitors the 1-minute load average and logs if it exceeds a threshold.</p>"},{"location":"shell-scripts/scripts/#5-process-monitor","title":"5. Process Monitor","text":"<pre><code>#!/bin/bash\nPROCESS_NAME=\"nginx\"\nLOG_FILE=\"/var/log/process_monitor.log\"\n\nif ! pgrep -x \"$PROCESS_NAME\" &gt; /dev/null; then\n  echo \"$(date): Process $PROCESS_NAME not found.\" &gt;&gt; \"$LOG_FILE\"\nfi\n</code></pre> <p>Explanation</p> <p>Checks if a specific process is running and logs if not.</p>"},{"location":"shell-scripts/scripts/#6-service-auto-restart","title":"6. Service Auto-Restart","text":"<p>(See the Practical Example above.)</p>"},{"location":"shell-scripts/scripts/#7-log-file-error-monitor","title":"7. Log File Error Monitor","text":"<pre><code>#!/bin/bash\nLOG_FILE=\"/var/log/application.log\"\nOUTPUT_LOG=\"/var/log/error_monitor.log\"\nPATTERN=\"ERROR\"\n\nif grep -q \"$PATTERN\" \"$LOG_FILE\"; then\n  echo \"$(date): Errors found in $LOG_FILE:\" &gt;&gt; \"$OUTPUT_LOG\"\n  grep \"$PATTERN\" \"$LOG_FILE\" &gt;&gt; \"$OUTPUT_LOG\"\nfi\n</code></pre> <p>Explanation</p> <p>Scans a log file for error patterns and logs any errors found.</p>"},{"location":"shell-scripts/scripts/#8-ssl-certificate-expiry-checker","title":"8. SSL Certificate Expiry Checker","text":"<p>(See the Practical Example above.)</p>"},{"location":"shell-scripts/scripts/#9-configuration-backup-with-rotation","title":"9. Configuration Backup with Rotation","text":"<pre><code>#!/bin/bash\nBACKUP_DIR=\"/backups/config\"\nSOURCE_DIR=\"/etc\"\nDATE=$(date +%F)\nBACKUP_FILE=\"${BACKUP_DIR}/config_backup_${DATE}.tar.gz\"\n\nmkdir -p \"$BACKUP_DIR\"\ntar -czf \"$BACKUP_FILE\" \"$SOURCE_DIR\"\nfind \"$BACKUP_DIR\" -type f -mtime +7 -exec rm {} \\;\necho \"$(date): Backup created: $BACKUP_FILE\"\n</code></pre> <p>Explanation</p> <p>Creates a compressed backup of the <code>/etc</code> directory and rotates backups older than 7 days.</p>"},{"location":"shell-scripts/scripts/#10-docker-container-monitor","title":"10. Docker Container Monitor","text":"<p>(See the Practical Example above.)</p>"},{"location":"shell-scripts/scripts/#11-system-update-automation","title":"11. System Update Automation","text":"<pre><code>#!/bin/bash\nLOG_FILE=\"/var/log/system_update.log\"\n{\n  echo \"----- $(date) -----\"\n  apt update &amp;&amp; apt upgrade -y\n  echo \"Update complete.\"\n} &gt;&gt; \"$LOG_FILE\" 2&gt;&amp;1\n</code></pre> <p>Explanation</p> <p>Automates system package updates on Debian/Ubuntu and logs the output.</p>"},{"location":"shell-scripts/scripts/#12-network-connectivity-checker","title":"12. Network Connectivity Checker","text":"<p>(See script 5 above for network checks.)</p>"},{"location":"shell-scripts/scripts/#13-temporary-file-cleaner","title":"13. Temporary File Cleaner","text":"<pre><code>#!/bin/bash\nfind /tmp -type f -mtime +7 -exec rm -f {} \\;\necho \"$(date): Cleaned temporary files older than 7 days.\"\n</code></pre> <p>Explanation</p> <p>Removes files in <code>/tmp</code> that are older than 7 days.</p>"},{"location":"shell-scripts/scripts/#14-open-file-descriptor-monitor","title":"14. Open File Descriptor Monitor","text":"<pre><code>#!/bin/bash\nTHRESHOLD=1000\nLOG_FILE=\"/var/log/fd_monitor.log\"\nfd_count=$(lsof | wc -l)\n\nif [ \"$fd_count\" -gt \"$THRESHOLD\" ]; then\n  echo \"$(date): High file descriptor count: $fd_count\" &gt;&gt; \"$LOG_FILE\"\nfi\n</code></pre> <p>Explanation</p> <p>Monitors the number of open file descriptors and logs if above threshold.</p>"},{"location":"shell-scripts/scripts/#15-combined-system-resource-monitor","title":"15. Combined System Resource Monitor","text":"<pre><code>#!/bin/bash\nCPU_THRESHOLD=80\nMEM_THRESHOLD=80\nDISK_THRESHOLD=80\nLOG_FILE=\"/var/log/resource_monitor.log\"\n\ncpu_usage=$(top -bn1 | grep \"Cpu(s)\" | sed \"s/.*, *\\([0-9.]*\\)%* id.*/\\1/\" | awk '{print 100 - $1}')\nmem_usage=$(free | grep Mem | awk '{printf \"%.0f\", $3/$2 * 100}')\ndisk_usage=$(df -h / | awk 'NR==2 {print $5}' | tr -d '%')\n\nalert=0\n(( $(echo \"$cpu_usage &gt; $CPU_THRESHOLD\" | bc -l) )) &amp;&amp; alert=1\n(( $(echo \"$mem_usage &gt; $MEM_THRESHOLD\" | bc -l) )) &amp;&amp; alert=1\n[ \"$disk_usage\" -gt \"$DISK_THRESHOLD\" ] &amp;&amp; alert=1\n\nif [ $alert -eq 1 ]; then\n  echo \"$(date): Resource Alert - CPU: ${cpu_usage}%, Memory: ${mem_usage}%, Disk: ${disk_usage}%\" &gt;&gt; \"$LOG_FILE\"\nfi\n</code></pre> <p>Explanation</p> <p>Checks CPU, memory, and disk usage; logs an alert if any exceed the thresholds.</p>"},{"location":"shell-scripts/scripts/#16-active-ssh-sessions-monitor","title":"16. Active SSH Sessions Monitor","text":"<pre><code>#!/bin/bash\nTHRESHOLD=5\nLOG_FILE=\"/var/log/ssh_sessions.log\"\nsession_count=$(who | grep -c 'pts/')\n\nif [ \"$session_count\" -gt \"$THRESHOLD\" ]; then\n  echo \"$(date): High number of SSH sessions: $session_count\" &gt;&gt; \"$LOG_FILE\"\nfi\n</code></pre> <p>Explanation</p> <p>Monitors the count of active SSH sessions and logs if they exceed the defined threshold.</p>"},{"location":"shell-scripts/scripts/#17-system-uptime-logger","title":"17. System Uptime Logger","text":"<pre><code>#!/bin/bash\nLOG_FILE=\"/var/log/uptime.log\"\necho \"$(date): Uptime - $(uptime)\" &gt;&gt; \"$LOG_FILE\"\n</code></pre> <p>Explanation</p> <p>Logs the current system uptime.</p>"},{"location":"shell-scripts/scripts/#18-system-temperature-monitor","title":"18. System Temperature Monitor","text":"<pre><code>#!/bin/bash\nTHRESHOLD=70\nLOG_FILE=\"/var/log/temperature.log\"\n\nif command -v sensors &amp;&gt;/dev/null; then\n  temp=$(sensors | awk '/^Package id 0:/ {print $4}' | tr -d '+\u00b0C')\n  if (( $(echo \"$temp &gt; $THRESHOLD\" | bc -l) )); then\n    echo \"$(date): High CPU temperature detected: $temp\u00b0C\" &gt;&gt; \"$LOG_FILE\"\n  fi\nelse\n  echo \"sensors command not found.\" &gt;&gt; \"$LOG_FILE\"\nfi\n</code></pre> <p>Explanation</p> <p>Uses the <code>sensors</code> command to check CPU temperature and logs if it\u2019s too high.</p>"},{"location":"shell-scripts/scripts/#19-log-rotation-and-archiving","title":"19. Log Rotation and Archiving","text":"<pre><code>#!/bin/bash\nLOG_FILE=\"/var/log/myapp.log\"\nARCHIVE_DIR=\"/var/log/archive\"\nmkdir -p \"$ARCHIVE_DIR\"\nDATE=$(date +%F)\nmv \"$LOG_FILE\" \"$ARCHIVE_DIR/myapp.log.$DATE\"\ngzip \"$ARCHIVE_DIR/myapp.log.$DATE\"\ntouch \"$LOG_FILE\"\necho \"$(date): Rotated log file.\"\n</code></pre> <p>Explanation</p> <p>Rotates a log file by compressing the old log and starting a new one.</p>"},{"location":"shell-scripts/scripts/#20-database-connection-monitor","title":"20. Database Connection Monitor","text":"<pre><code>#!/bin/bash\nDB_HOST=\"localhost\"\nDB_USER=\"root\"\nDB_PASS=\"password\"\nLOG_FILE=\"/var/log/db_connection.log\"\n\nif ! mysqladmin ping -h \"$DB_HOST\" -u \"$DB_USER\" -p\"$DB_PASS\" --silent; then\n  echo \"$(date): Cannot reach MySQL on $DB_HOST.\" &gt;&gt; \"$LOG_FILE\"\nfi\n</code></pre> <p>Explanation</p> <p>Checks if a MySQL database is reachable and logs if not.</p>"},{"location":"shell-scripts/scripts/#21-database-query-health-check","title":"21. Database Query Health Check","text":"<pre><code>#!/bin/bash\nDB_HOST=\"localhost\"\nDB_USER=\"root\"\nDB_PASS=\"password\"\nDB_NAME=\"testdb\"\nLOG_FILE=\"/var/log/db_query.log\"\n\nresult=$(mysql -h \"$DB_HOST\" -u \"$DB_USER\" -p\"$DB_PASS\" \"$DB_NAME\" -e \"SELECT 1;\" 2&gt;&amp;1)\necho \"$(date): Query result: $result\" &gt;&gt; \"$LOG_FILE\"\n</code></pre> <p>Explanation</p> <p>Runs a simple query against a MySQL database and logs the output.</p>"},{"location":"shell-scripts/scripts/#22-kafka-topic-monitor","title":"22. Kafka Topic Monitor","text":"<pre><code>#!/bin/bash\nTOPIC=\"important_topic\"\nKAFKA_BIN=\"/usr/bin/kafka-topics.sh\"\nLOG_FILE=\"/var/log/kafka_monitor.log\"\n\nif ! \"$KAFKA_BIN\" --list --zookeeper localhost:2181 | grep -qw \"$TOPIC\"; then\n  echo \"$(date): Kafka topic $TOPIC not found.\" &gt;&gt; \"$LOG_FILE\"\nfi\n</code></pre> <p>Explanation</p> <p>Checks if a Kafka topic exists and logs if missing.</p>"},{"location":"shell-scripts/scripts/#23-network-port-usage-monitor","title":"23. Network Port Usage Monitor","text":"<pre><code>#!/bin/bash\nCRITICAL_PORT=80\nLOG_FILE=\"/var/log/port_usage.log\"\n\nif ! netstat -tuln | grep -q \":$CRITICAL_PORT \"; then\n  echo \"$(date): Critical port $CRITICAL_PORT is not in use.\" &gt;&gt; \"$LOG_FILE\"\nfi\n</code></pre> <p>Explanation</p> <p>Verifies if a critical network port is in use.</p>"},{"location":"shell-scripts/scripts/#24-firewall-status-checker","title":"24. Firewall Status Checker","text":"<pre><code>#!/bin/bash\nLOG_FILE=\"/var/log/firewall_status.log\"\n\nif command -v ufw &amp;&gt;/dev/null; then\n  status=$(ufw status | head -n 1)\n  if [[ \"$status\" != *\"active\"* ]]; then\n    echo \"$(date): UFW firewall is not active.\" &gt;&gt; \"$LOG_FILE\"\n  fi\nelse\n  echo \"ufw not installed.\" &gt;&gt; \"$LOG_FILE\"\nfi\n</code></pre> <p>Explanation</p> <p>Checks if the UFW firewall is active and logs if not.</p>"},{"location":"shell-scripts/scripts/#25-disk-iops-and-latency-monitor","title":"25. Disk IOPS and Latency Monitor","text":"<pre><code>#!/bin/bash\nTHRESHOLD_IOPS=100\nLOG_FILE=\"/var/log/disk_iops.log\"\n\nif command -v iostat &amp;&gt;/dev/null; then\n  iops=$(iostat -dx 1 2 | awk 'NR==7 {print $4}')\n  if (( $(echo \"$iops &gt; $THRESHOLD_IOPS\" | bc -l) )); then\n    echo \"$(date): High IOPS detected: $iops\" &gt;&gt; \"$LOG_FILE\"\n  fi\nelse\n  echo \"iostat not found.\" &gt;&gt; \"$LOG_FILE\"\nfi\n</code></pre> <p>Explanation</p> <p>Monitors disk IOPS using <code>iostat</code> and logs when thresholds are exceeded.</p>"},{"location":"shell-scripts/scripts/#26-prometheus-metrics-pusher","title":"26. Prometheus Metrics Pusher","text":"<pre><code>#!/bin/bash\nPUSHGATEWAY=\"http://localhost:9091\"\nJOB_NAME=\"sre_metrics\"\nMETRIC_VALUE=$(uptime | awk -F'load average:' '{ print $2 }' | cut -d',' -f1 | tr -d ' ')\necho \"load_average $METRIC_VALUE\" | curl --data-binary @- \"$PUSHGATEWAY/metrics/job/$JOB_NAME\"\n</code></pre> <p>Explanation</p> <p>Pushes a custom load average metric to a Prometheus pushgateway.</p>"},{"location":"shell-scripts/scripts/#27-ssl-certificate-chain-checker","title":"27. SSL Certificate Chain Checker","text":"<pre><code>#!/bin/bash\nDOMAIN=\"example.com\"\nLOG_FILE=\"/var/log/ssl_chain.log\"\nchain=$(echo | openssl s_client -showcerts -servername \"$DOMAIN\" -connect \"$DOMAIN\":443 2&gt;/dev/null)\nif [[ -z \"$chain\" ]]; then\n  echo \"$(date): Failed to retrieve certificate chain for $DOMAIN.\" &gt;&gt; \"$LOG_FILE\"\nfi\n</code></pre> <p>Explanation</p> <p>Validates the SSL certificate chain for a given domain.</p>"},{"location":"shell-scripts/scripts/#28-filesystem-mount-checker","title":"28. Filesystem Mount Checker","text":"<pre><code>#!/bin/bash\nFS=\"/mnt/data\"\nLOG_FILE=\"/var/log/mount_check.log\"\n\nif ! mount | grep -q \"$FS\"; then\n  echo \"$(date): Filesystem $FS is not mounted.\" &gt;&gt; \"$LOG_FILE\"\nfi\n</code></pre> <p>Explanation</p> <p>Verifies that a specific filesystem is mounted and logs if it isn\u2019t.</p>"},{"location":"shell-scripts/scripts/#29-raid-status-monitor","title":"29. RAID Status Monitor","text":"<pre><code>#!/bin/bash\nLOG_FILE=\"/var/log/raid_status.log\"\nif command -v mdadm &amp;&gt;/dev/null; then\n  status=$(mdadm --detail /dev/md0 | grep 'State :' | awk '{print $3}')\n  if [[ \"$status\" != \"clean\" &amp;&amp; \"$status\" != \"active\" ]]; then\n    echo \"$(date): RAID status is $status.\" &gt;&gt; \"$LOG_FILE\"\n  fi\nelse\n  echo \"mdadm not installed.\" &gt;&gt; \"$LOG_FILE\"\nfi\n</code></pre> <p>Explanation</p> <p>Checks the health of a RAID array and logs if it isn\u2019t healthy.</p>"},{"location":"shell-scripts/scripts/#30-swap-usage-monitor","title":"30. Swap Usage Monitor","text":"<pre><code>#!/bin/bash\nTHRESHOLD=50  # percentage\nLOG_FILE=\"/var/log/swap_usage.log\"\nswap_usage=$(free | awk '/Swap/ {print $3/$2 * 100.0}')\nif (( $(echo \"$swap_usage &gt; $THRESHOLD\" | bc -l) )); then\n  echo \"$(date): High swap usage: $swap_usage%\" &gt;&gt; \"$LOG_FILE\"\nfi\n</code></pre> <p>Explanation</p> <p>Monitors swap usage and logs if it exceeds a threshold percentage.</p>"},{"location":"shell-scripts/scripts/#31-boot-time-logger","title":"31. Boot Time Logger","text":"<pre><code>#!/bin/bash\nLOG_FILE=\"/var/log/boot_time.log\"\nboot_time=$(who -b | awk '{print $3, $4}')\necho \"$(date): Boot time - $boot_time\" &gt;&gt; \"$LOG_FILE\"\n</code></pre> <p>Explanation</p> <p>Logs the system boot time to a file.</p>"},{"location":"shell-scripts/scripts/#32-process-count-monitor","title":"32. Process Count Monitor","text":"<pre><code>#!/bin/bash\nTHRESHOLD=300\nLOG_FILE=\"/var/log/process_count.log\"\nprocess_count=$(ps aux | wc -l)\nif [ \"$process_count\" -gt \"$THRESHOLD\" ]; then\n  echo \"$(date): High process count: $process_count\" &gt;&gt; \"$LOG_FILE\"\nfi\n</code></pre> <p>Explanation</p> <p>Monitors the total number of processes and logs if the count is too high.</p>"},{"location":"shell-scripts/scripts/#33-load-average-alert","title":"33. Load Average Alert","text":"<pre><code>#!/bin/bash\nTHRESHOLD=3.0\nLOG_FILE=\"/var/log/load_average.log\"\nload_avg=$(uptime | awk -F'load average:' '{ print $2 }' | cut -d',' -f1 | tr -d ' ')\nif (( $(echo \"$load_avg &gt; $THRESHOLD\" | bc -l) )); then\n  echo \"$(date): Load average high: $load_avg\" &gt;&gt; \"$LOG_FILE\"\nfi\n</code></pre> <p>Explanation</p> <p>Alerts if the 1-minute load average is above a specified threshold.</p>"},{"location":"shell-scripts/scripts/#34-dns-resolution-tester","title":"34. DNS Resolution Tester","text":"<pre><code>#!/bin/bash\nDOMAINS=(\"example.com\" \"google.com\")\nLOG_FILE=\"/var/log/dns_resolution.log\"\n\nfor domain in \"${DOMAINS[@]}\"; do\n  if ! nslookup \"$domain\" &amp;&gt;/dev/null; then\n    echo \"$(date): DNS resolution failed for $domain\" &gt;&gt; \"$LOG_FILE\"\n  fi\ndone\n</code></pre> <p>Explanation</p> <p>Tests DNS resolution for a list of domains and logs any failures.</p>"},{"location":"shell-scripts/scripts/#35-active-network-connections-monitor","title":"35. Active Network Connections Monitor","text":"<pre><code>#!/bin/bash\nTHRESHOLD=100\nLOG_FILE=\"/var/log/network_connections.log\"\nconn_count=$(netstat -an | grep ESTABLISHED | wc -l)\nif [ \"$conn_count\" -gt \"$THRESHOLD\" ]; then\n  echo \"$(date): High number of active connections: $conn_count\" &gt;&gt; \"$LOG_FILE\"\nfi\n</code></pre> <p>Explanation</p> <p>Monitors established network connections and logs if the count is high.</p>"},{"location":"shell-scripts/scripts/#36-package-update-checker-debian","title":"36. Package Update Checker (Debian)","text":"<pre><code>#!/bin/bash\nLOG_FILE=\"/var/log/package_update.log\"\napt update &gt; /dev/null 2&gt;&amp;1\nupdates=$(apt list --upgradable 2&gt;/dev/null | grep -v \"Listing...\")\necho \"$(date): Available updates:\" &gt;&gt; \"$LOG_FILE\"\necho \"$updates\" &gt;&gt; \"$LOG_FILE\"\n</code></pre> <p>Explanation</p> <p>Checks for available package updates on Debian/Ubuntu systems and logs them.</p>"},{"location":"shell-scripts/scripts/#37-ntp-synchronization-checker","title":"37. NTP Synchronization Checker","text":"<pre><code>#!/bin/bash\nLOG_FILE=\"/var/log/ntp_sync.log\"\nif command -v timedatectl &amp;&gt;/dev/null; then\n  sync_status=$(timedatectl show -p NTPSynchronized --value)\n  if [ \"$sync_status\" != \"yes\" ]; then\n    echo \"$(date): NTP is not synchronized.\" &gt;&gt; \"$LOG_FILE\"\n  fi\nelse\n  echo \"timedatectl not found.\" &gt;&gt; \"$LOG_FILE\"\nfi\n</code></pre> <p>Explanation</p> <p>Verifies whether NTP is synchronized and logs if it isn\u2019t.</p>"},{"location":"shell-scripts/scripts/#38-virtualization-host-metrics","title":"38. Virtualization Host Metrics","text":"<pre><code>#!/bin/bash\nLOG_FILE=\"/var/log/virt_metrics.log\"\nif grep -qi hypervisor /proc/cpuinfo; then\n  echo \"$(date): Running on a virtualization host.\" &gt;&gt; \"$LOG_FILE\"\n  top -b -n1 | head -n 10 &gt;&gt; \"$LOG_FILE\"\nfi\n</code></pre> <p>Explanation</p> <p>Logs basic metrics if the system is a virtualization host.</p>"},{"location":"shell-scripts/scripts/#39-hardware-error-monitor","title":"39. Hardware Error Monitor","text":"<pre><code>#!/bin/bash\nLOG_FILE=\"/var/log/hardware_errors.log\"\nif dmesg | grep -i -E \"error|fail\" &amp;&gt;/dev/null; then\n  echo \"$(date): Hardware errors detected.\" &gt;&gt; \"$LOG_FILE\"\nfi\n</code></pre> <p>Explanation</p> <p>Monitors <code>dmesg</code> for hardware errors and logs if any are detected.</p>"},{"location":"shell-scripts/scripts/#40-synthetic-transaction-script","title":"40. Synthetic Transaction Script","text":"<pre><code>#!/bin/bash\nURL=\"http://localhost:8080/api/transaction\"\nLOG_FILE=\"/var/log/synthetic_transaction.log\"\n\nresponse=$(curl -s -o /dev/null -w \"%{http_code}\" \"$URL\")\nif [ \"$response\" -ne 200 ]; then\n  echo \"$(date): Synthetic transaction failed with code $response\" &gt;&gt; \"$LOG_FILE\"\nelse\n  echo \"$(date): Synthetic transaction succeeded.\" &gt;&gt; \"$LOG_FILE\"\nfi\n</code></pre> <p>Explanation</p> <p>Simulates a user transaction by performing an HTTP request and logging the response.</p>"},{"location":"shell-scripts/scripts/#41-system-metrics-collector","title":"41. System Metrics Collector","text":"<pre><code>#!/bin/bash\nLOG_FILE=\"/var/log/vmstat.log\"\nvmstat 1 5 &gt;&gt; \"$LOG_FILE\"\n</code></pre> <p>Explanation</p> <p>Collects system metrics using <code>vmstat</code> and logs the output.</p>"},{"location":"shell-scripts/scripts/#42-swap-usage-over-time-monitor","title":"42. Swap Usage Over Time Monitor","text":"<pre><code>#!/bin/bash\nLOG_FILE=\"/var/log/swap_usage_trend.log\"\nswap_used=$(free -m | awk '/Swap/ {print $3}')\necho \"$(date): Swap used: ${swap_used}MB\" &gt;&gt; \"$LOG_FILE\"\n</code></pre> <p>Explanation</p> <p>Logs the current swap usage for trend analysis.</p>"},{"location":"shell-scripts/scripts/#43-io-wait-monitor","title":"43. I/O Wait Monitor","text":"<pre><code>#!/bin/bash\nTHRESHOLD=20\nLOG_FILE=\"/var/log/iowait.log\"\niowait=$(vmstat 1 2 | tail -1 | awk '{print $15}')\nif [ \"$iowait\" -gt \"$THRESHOLD\" ]; then\n  echo \"$(date): High I/O wait: ${iowait}%\" &gt;&gt; \"$LOG_FILE\"\nfi\n</code></pre> <p>Explanation</p> <p>Checks I/O wait time using <code>vmstat</code> and logs if above the threshold.</p>"},{"location":"shell-scripts/scripts/#44-controlled-failover-test","title":"44. Controlled Failover Test","text":"<pre><code>#!/bin/bash\nSERVICE=\"dummy_service\"\nLOG_FILE=\"/var/log/failover_test.log\"\nsystemctl stop \"$SERVICE\"\nsleep 5\nsystemctl start \"$SERVICE\"\nif systemctl is-active --quiet \"$SERVICE\"; then\n  echo \"$(date): Failover test successful for $SERVICE.\" &gt;&gt; \"$LOG_FILE\"\nelse\n  echo \"$(date): Failover test failed for $SERVICE.\" &gt;&gt; \"$LOG_FILE\"\nfi\n</code></pre> <p>Explanation</p> <p>Simulates a service failure by stopping and restarting a dummy service, then logs the outcome.</p>"},{"location":"shell-scripts/scripts/#45-backup-cleanup-script","title":"45. Backup Cleanup Script","text":"<pre><code>#!/bin/bash\nBACKUP_DIR=\"/backups\"\nfind \"$BACKUP_DIR\" -type f -mtime +30 -exec rm {} \\;\necho \"$(date): Cleaned up backups older than 30 days.\"\n</code></pre> <p>Explanation</p> <p>Removes backup files older than 30 days from the specified directory.</p>"},{"location":"shell-scripts/scripts/#46-network-latency-and-jitter-monitor","title":"46. Network Latency and Jitter Monitor","text":"<pre><code>#!/bin/bash\nENDPOINTS=(\"8.8.8.8\" \"1.1.1.1\")\nLOG_FILE=\"/var/log/network_latency.log\"\nfor host in \"${ENDPOINTS[@]}\"; do\n  result=$(ping -c 5 \"$host\" | tail -1 | awk -F'/' '{print \"Latency: \"$5\", Jitter: \"$6}')\n  echo \"$(date): $host - $result\" &gt;&gt; \"$LOG_FILE\"\ndone\n</code></pre> <p>Explanation</p> <p>Measures ping latency and jitter for multiple endpoints and logs the results.</p>"},{"location":"shell-scripts/scripts/#47-cron-job-health-monitor","title":"47. Cron Job Health Monitor","text":"<pre><code>#!/bin/bash\nCRON_LOG=\"/var/log/cron.log\"\nKEYWORD=\"daily_backup\"\nLOG_FILE=\"/var/log/cron_monitor.log\"\nif ! grep -q \"$KEYWORD\" \"$CRON_LOG\"; then\n  echo \"$(date): Cron job $KEYWORD did not run.\" &gt;&gt; \"$LOG_FILE\"\nfi\n</code></pre> <p>Explanation</p> <p>Checks if critical cron jobs have run by searching for log entries.</p>"},{"location":"shell-scripts/scripts/#48-service-dependency-checker","title":"48. Service Dependency Checker","text":"<pre><code>#!/bin/bash\nSERVICE=\"myapp\"\nDEPENDENCY_PORT=6379  # Example: Redis\nLOG_FILE=\"/var/log/dependency_check.log\"\n\nif ! netstat -tuln | grep -q \":$DEPENDENCY_PORT \"; then\n  echo \"$(date): Dependency on port $DEPENDENCY_PORT not met for $SERVICE.\" &gt;&gt; \"$LOG_FILE\"\nfi\n</code></pre> <p>Explanation</p> <p>Verifies that required dependencies (e.g., a Redis port) are available for a service.</p>"},{"location":"shell-scripts/scripts/#49-environment-variable-checker","title":"49. Environment Variable Checker","text":"<pre><code>#!/bin/bash\nREQUIRED_VARS=(\"APP_ENV\" \"DB_HOST\" \"DB_USER\")\nLOG_FILE=\"/var/log/env_check.log\"\nmissing=0\n\nfor var in \"${REQUIRED_VARS[@]}\"; do\n  if [ -z \"${!var}\" ]; then\n    echo \"Environment variable $var is not set.\" &gt;&gt; \"$LOG_FILE\"\n    missing=1\n  fi\ndone\n\nif [ $missing -eq 1 ]; then\n  echo \"$(date): One or more required environment variables are missing.\" &gt;&gt; \"$LOG_FILE\"\nfi\n</code></pre> <p>Explanation</p> <p>Checks that essential environment variables are set and logs if any are missing.</p>"},{"location":"shell-scripts/scripts/#50-custom-alerting-aggregator","title":"50. Custom Alerting Aggregator","text":"<pre><code>#!/bin/bash\nLOG_FILE=\"/var/log/alert_summary.log\"\n: &gt; \"$LOG_FILE\"\n\n# Check nginx service\nif ! systemctl is-active --quiet nginx; then\n  echo \"nginx service down\" &gt;&gt; \"$LOG_FILE\"\nfi\n\n# Check disk usage\ndisk_usage=$(df -h / | awk 'NR==2 {print $5}' | tr -d '%')\nif [ \"$disk_usage\" -gt 80 ]; then\n  echo \"Disk usage high: ${disk_usage}%\" &gt;&gt; \"$LOG_FILE\"\nfi\n\n# Additional checks can be added here...\n\nif [ -s \"$LOG_FILE\" ]; then\n  echo \"Alert summary generated at $(date):\" &gt;&gt; \"$LOG_FILE\"\nfi\n</code></pre> <p>Explanation</p> <p>Aggregates multiple health checks and logs a summary if issues are detected.</p>"},{"location":"sonarqube/sonarqube/","title":"SonarQube Installation, Configuration, and Integration with Jenkins","text":"Mastering SonarQube: Installation, Configuration, and Jenkins Integration <p>SonarQube is an open-source platform that provides continuous inspection of code quality. It performs automatic reviews to detect bugs, vulnerabilities, and code smells in your codebase. This guide will walk you through installing SonarQube on Ubuntu, configuring it, and integrating it with Jenkins to enhance your continuous integration and continuous delivery (CI/CD) pipeline.</p>"},{"location":"sonarqube/sonarqube/#what-is-sonarqube","title":"What is SonarQube?","text":"<p>SonarQube is a powerful tool for continuous code quality inspection. It helps developers identify and fix issues early in the development lifecycle, ensuring that their code is secure, maintainable, and efficient. SonarQube can analyze code in various programming languages, providing feedback on code quality metrics such as bugs, vulnerabilities, code smells, duplications, and test coverage.</p>"},{"location":"sonarqube/sonarqube/#step-1-installing-sonarqube-on-ubuntu","title":"Step 1: Installing SonarQube on Ubuntu","text":""},{"location":"sonarqube/sonarqube/#11-prerequisites","title":"1.1 Prerequisites","text":"<p>Before installing SonarQube, ensure that you have Java (JDK 11 or newer) installed. If not, install it using the following command:</p> <pre><code>sudo apt update\nsudo apt install openjdk-11-jdk -y\n</code></pre> <p>Verify the Java installation:</p> <pre><code>java -version\n</code></pre>"},{"location":"sonarqube/sonarqube/#12-install-dependencies","title":"1.2 Install Dependencies","text":"<p>SonarQube requires PostgreSQL as its database backend. To install PostgreSQL:</p> <pre><code>sudo apt install postgresql postgresql-contrib -y\n</code></pre> <p>After installation, create a new database and user for SonarQube:</p> <pre><code>sudo -u postgres psql\nCREATE USER sonar WITH PASSWORD 'sonar';\nCREATE DATABASE sonar;\nGRANT ALL PRIVILEGES ON DATABASE sonar TO sonar;\n\\q\n</code></pre>"},{"location":"sonarqube/sonarqube/#13-download-and-install-sonarqube","title":"1.3 Download and Install SonarQube","text":"<p>Now, download the latest version of SonarQube from the official website:</p> <pre><code>wget https://binaries.sonarsource.com/CommercialEdition/sonarqube-9.3.0.51899.zip\n</code></pre> <p>Unzip the downloaded file:</p> <pre><code>unzip sonarqube-9.3.0.51899.zip\nsudo mv sonarqube-9.3.0.51899 /opt/sonarqube\n</code></pre>"},{"location":"sonarqube/sonarqube/#14-configure-sonarqube","title":"1.4 Configure SonarQube","text":"<p>Navigate to the SonarQube configuration file:</p> <pre><code>cd /opt/sonarqube/conf\nsudo nano sonar.properties\n</code></pre> <p>Edit the following properties to set up your PostgreSQL database:</p> <pre><code>sonar.jdbc.url=jdbc:postgresql://localhost/sonar\nsonar.jdbc.username=sonar\nsonar.jdbc.password=sonar\n</code></pre>"},{"location":"sonarqube/sonarqube/#15-start-sonarqube","title":"1.5 Start SonarQube","text":"<p>SonarQube is bundled with a script to start the application. You can start SonarQube using the following commands:</p> <pre><code>cd /opt/sonarqube/bin/linux-x86-64\n./sonar.sh start\n</code></pre> <p>You can verify that SonarQube is running by visiting:</p> <pre><code>http://localhost:9000\n</code></pre> <p>The default login credentials are: - Username: admin - Password: admin</p>"},{"location":"sonarqube/sonarqube/#step-2-installing-the-sonarqube-plugin-for-jenkins","title":"Step 2: Installing the SonarQube Plugin for Jenkins","text":"<p>To integrate SonarQube with Jenkins, you need to install the SonarQube Scanner for Jenkins plugin.</p>"},{"location":"sonarqube/sonarqube/#21-install-the-plugin","title":"2.1 Install the Plugin","text":"<ol> <li>Open Jenkins in your browser (<code>http://localhost:8080</code>).</li> <li>Navigate to Manage Jenkins &gt; Manage Plugins.</li> <li>Search for SonarQube Scanner in the Available tab and install it.</li> </ol>"},{"location":"sonarqube/sonarqube/#step-3-configuring-sonarqube-in-jenkins","title":"Step 3: Configuring SonarQube in Jenkins","text":""},{"location":"sonarqube/sonarqube/#31-configure-sonarqube-server-in-jenkins","title":"3.1 Configure SonarQube Server in Jenkins","text":"<ol> <li>Go to Manage Jenkins &gt; Configure System.</li> <li>Scroll down to the SonarQube Servers section.</li> <li>Click Add SonarQube and enter the following details:</li> <li>Name: SonarQube (or any name you prefer)</li> <li>Server URL: <code>http://localhost:9000</code></li> <li>Authentication Token: To generate an authentication token, log in to SonarQube and go to My Account &gt; Security &gt; Generate Tokens.</li> </ol>"},{"location":"sonarqube/sonarqube/#32-install-the-sonarqube-scanner-in-jenkins","title":"3.2 Install the SonarQube Scanner in Jenkins","text":"<ol> <li>In the SonarQube Scanner section, click Add SonarQube Scanner.</li> <li>Enter the Installation Name (e.g., SonarQube Scanner) and set the Version.</li> </ol> <p>Jenkins will automatically detect the SonarQube Scanner when the plugin is installed.</p>"},{"location":"sonarqube/sonarqube/#step-4-create-a-jenkins-pipeline-to-run-sonarqube-analysis","title":"Step 4: Create a Jenkins Pipeline to Run SonarQube Analysis","text":""},{"location":"sonarqube/sonarqube/#41-create-a-new-jenkins-pipeline","title":"4.1 Create a New Jenkins Pipeline","text":"<ol> <li>From the Jenkins dashboard, click on New Item.</li> <li>Choose Pipeline and give it a name (e.g., SonarQube-Pipeline).</li> <li>Click OK.</li> </ol>"},{"location":"sonarqube/sonarqube/#42-define-the-pipeline-script","title":"4.2 Define the Pipeline Script","text":"<p>Add the following script in the Pipeline section. This pipeline will perform a SonarQube analysis on your project:</p> <pre><code>pipeline {\n    agent any\n    environment {\n        SONARQUBE = 'SonarQube'  // The name of the SonarQube server configured in Jenkins\n    }\n    stages {\n        stage('Checkout') {\n            steps {\n                git 'https://github.com/your-repository-url.git'\n            }\n        }\n        stage('SonarQube Analysis') {\n            steps {\n                script {\n                    // Run the SonarQube scanner\n                    sh \"sonar-scanner\"\n                }\n            }\n        }\n        stage('Build') {\n            steps {\n                // Build your project (e.g., using Maven, Gradle, etc.)\n                sh 'mvn clean install'\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"sonarqube/sonarqube/#43-run-the-pipeline","title":"4.3 Run the Pipeline","text":"<p>Save the pipeline and click Build Now. Jenkins will execute the pipeline, run the SonarQube analysis, and publish the results to SonarQube.</p>"},{"location":"sonarqube/sonarqube/#step-5-view-sonarqube-analysis-results","title":"Step 5: View SonarQube Analysis Results","text":"<p>After the pipeline runs, you can view the results by logging into the SonarQube dashboard at:</p> <pre><code>http://localhost:9000\n</code></pre> <p>Here, you\u2019ll see detailed code quality metrics, including:</p> <ul> <li>Code coverage</li> <li>Code duplication</li> <li>Code smells</li> <li>Vulnerabilities</li> <li>Bugs</li> </ul> <p>By following these steps, you\u2019ve successfully installed SonarQube, configured it, and integrated it with Jenkins to perform continuous code quality analysis. With SonarQube integrated into your Jenkins pipeline, you can automatically monitor code quality, identify issues early, and maintain high-quality code throughout your development lifecycle.</p> <p>?? info \"Tip\"     For better code quality enforcement, integrate SonarQube with Jenkins' automated build and testing process to prevent merging code that does not meet your quality standards.</p> <p>```</p>"},{"location":"sonarqube/sonarqube/#key-features","title":"Key Features:","text":"<ol> <li>SonarQube Setup: The guide walks users through installing and configuring SonarQube on Ubuntu with PostgreSQL as the database backend.</li> <li>Jenkins Integration: Instructions on integrating SonarQube with Jenkins via the SonarQube Scanner plugin.</li> <li>Pipeline Example: A Jenkins pipeline example that runs SonarQube analysis as part of the build process.</li> <li>User-Friendly Navigation: This guide is broken down into easy-to-follow steps with detailed explanations for each phase.</li> </ol>"}]}